{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-1-How-to-Write-a-New-Network.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0IerUqWPN2YL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How to Write a New Network\n",
        "\n",
        "## Precondition\n",
        "\n",
        "Install Chainer and Cupy."
      ]
    },
    {
      "metadata": {
        "id": "kizqNcZNNuYL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 21
            },
            {
              "item_id": 45
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "de43cbeb-c415-4748-8dad-3f653a67f431",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520062255406,
          "user_tz": -540,
          "elapsed": 40721,
          "user": {
            "displayName": "Yasuki IKEUCHI",
            "photoUrl": "//lh6.googleusercontent.com/-QrqZkF-x2us/AAAAAAAAAAI/AAAAAAAAAAA/PzOMhw6mH3o/s50-c-k-no/photo.jpg",
            "userId": "110487681503898314699"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install Chainer and CuPy!\n",
        "\n",
        "!curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 28.9 MB of archives.\n",
            "After this operation, 71.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libcusparse8.0 amd64 8.0.61-1 [22.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvrtc8.0 amd64 8.0.61-1 [6,225 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvtoolsext1 amd64 8.0.61-1 [32.2 kB]\n",
            "Fetched 28.9 MB in 1s (16.2 MB/s)\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libcusparse8.0:amd64.\n",
            "(Reading database ... 16669 files and directories currently installed.)\n",
            "Preparing to unpack .../libcusparse8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libnvrtc8.0:amd64.\n",
            "Preparing to unpack .../libnvrtc8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../libnvtoolsext1_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JCollecting cupy-cuda80==4.0.0b3 from https://github.com/kmaehashi/chainer-colab/releases/download/2018-02-06/cupy_cuda80-4.0.0b3-cp36-cp36m-linux_x86_64.whl\n",
            "  Downloading https://github.com/kmaehashi/chainer-colab/releases/download/2018-02-06/cupy_cuda80-4.0.0b3-cp36-cp36m-linux_x86_64.whl (205.2MB)\n",
            "\u001b[K    13% |████▍                           | 28.3MB 36.2MB/s eta 0:00:05"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 205.2MB 7.1kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80==4.0.0b3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80==4.0.0b3)\n",
            "Collecting fastrlock>=0.3 (from cupy-cuda80==4.0.0b3)\n",
            "  Downloading fastrlock-0.3-cp36-cp36m-manylinux1_x86_64.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: fastrlock, cupy-cuda80\n",
            "Successfully installed cupy-cuda80-4.0.0b3 fastrlock-0.3\n",
            "Collecting chainer==4.0.0b3\n",
            "  Downloading chainer-4.0.0b3.tar.gz (366kB)\n",
            "\u001b[K    100% |████████████████████████████████| 368kB 2.3MB/s \n",
            "\u001b[?25hCollecting filelock (from chainer==4.0.0b3)\n",
            "  Downloading filelock-3.0.4.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer==4.0.0b3)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer==4.0.0b3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from chainer==4.0.0b3)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.0.0->chainer==4.0.0b3)\n",
            "Building wheels for collected packages: chainer, filelock\n",
            "  Running setup.py bdist_wheel for chainer ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ce/20/9f/4f7d6978c1a5f88bf2bb18d429998b85cf75cfe96315c7631b\n",
            "  Running setup.py bdist_wheel for filelock ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/5f/5e/8a/9f1eb481ffbfff95d5f550570c1dbeff3c1785c8383c12c62b\n",
            "Successfully built chainer filelock\n",
            "Installing collected packages: filelock, chainer\n",
            "Successfully installed chainer-4.0.0b3 filelock-3.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NgmVF6QhOLSK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3decdb4c-9734-43cd-8194-84432f907e40",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520062260652,
          "user_tz": -540,
          "elapsed": 5228,
          "user": {
            "displayName": "Yasuki IKEUCHI",
            "photoUrl": "//lh6.googleusercontent.com/-QrqZkF-x2us/AAAAAAAAAAI/AAAAAAAAAAA/PzOMhw6mH3o/s50-c-k-no/photo.jpg",
            "userId": "110487681503898314699"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import chainer\n",
        "from chainer import cuda, Function, gradient_check, report, training, utils, Variable\n",
        "from chainer import datasets, iterators, optimizers, serializers\n",
        "from chainer import Link, Chain, ChainList\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer.training import extensions"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/cupy/core/fusion.py:659: FutureWarning: cupy.core.fusion is experimental. The interface can change in the future.\n",
            "  util.experimental('cupy.core.fusion')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hYjsrZEJFpKE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional Network for Visual Recognition Tasks\n",
        "\n",
        "In this section, you will learn how to write\n",
        "\n",
        "* A small convolutional network with a model class that is inherited from :class:`chainer.Chain`,\n",
        "* A large convolutional network that has several building block networks with :class:`chainer.ChainList`.\n",
        "\n",
        "After reading this section, you will be able to:\n",
        "\n",
        "* Write your own original convolutional network in Chainer\n",
        "\n",
        "A convolutional network (ConvNet) is mainly comprised of convolutional layers.\n",
        "This type of network is commonly used for various visual recognition tasks,\n",
        "e.g., classifying hand-written digits or natural images into given object\n",
        "classes, detecting objects from an image, and labeling all pixels of an image\n",
        "with the object classes (semantic segmentation), and so on.\n",
        "\n",
        "In such tasks, a typical ConvNet takes a set of images whose shape is\n",
        ":math:`(N, C, H, W)`, where\n",
        "\n",
        "- `N` denotes the number of images in a mini-batch,\n",
        "- `C` denotes the number of channels of those images,\n",
        "- `H` and `W` denote the height and width of those images,\n",
        "\n",
        "respectively. Then, it typically outputs a fixed-sized vector as membership\n",
        "probabilities over the target object classes. It also can output a set of\n",
        "feature maps that have the corresponding size to the input image for a pixel\n",
        "labeling task, etc.\n"
      ]
    },
    {
      "metadata": {
        "id": "ltUfGA-JPGZg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note\n",
        "\n",
        "> The below example code assumes that some packages are already imported.\n",
        "> Please see the details here: `basic`."
      ]
    },
    {
      "metadata": {
        "id": "KSgZ2XqzPRll",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LeNet5\n",
        "\n",
        "Here, let's start by defining LeNet5 [LeCun98] in Chainer.\n",
        "This is a ConvNet model that has 5 layers comprised of 3 convolutional layers\n",
        "and 2 fully-connected layers. This was proposed to classify hand-written\n",
        "digit images in 1998. In Chainer, the model can be written as follows:"
      ]
    },
    {
      "metadata": {
        "id": "04ZUqAdNFq9M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LeNet5(Chain):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution2D(\n",
        "                in_channels=1, out_channels=6, ksize=5, stride=1)\n",
        "            self.conv2 = L.Convolution2D(\n",
        "                in_channels=6, out_channels=16, ksize=5, stride=1)\n",
        "            self.conv3 = L.Convolution2D(\n",
        "                in_channels=16, out_channels=120, ksize=4, stride=1)\n",
        "            self.fc4 = L.Linear(None, 84)\n",
        "            self.fc5 = L.Linear(84, 10)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = F.sigmoid(self.conv1(x))\n",
        "        h = F.max_pooling_2d(h, 2, 2)\n",
        "        h = F.sigmoid(self.conv2(h))\n",
        "        h = F.max_pooling_2d(h, 2, 2)\n",
        "        h = F.sigmoid(self.conv3(h))\n",
        "        h = F.sigmoid(self.fc4(h))\n",
        "        if chainer.config.train:\n",
        "            return self.fc5(h)\n",
        "        return F.softmax(self.fc5(h))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zcZTI-7Ob8T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A typical way to write your network is creating a new class inherited from\n",
        "`chainer.Chain` class. When defining your model in this way, typically,\n",
        "all the layers which have trainable parameters are registered to the model\n",
        "by assigning the objects of `chainer.Link` as an attribute.\n",
        "\n",
        "The model class is instantiated before the forward and backward computations.\n",
        "To give input images and label vectors simply by calling the model object\n",
        "like a function, `__call__` is usually defined in the model class.\n",
        "\n",
        "This method performs the forward computation of the model. Chainer uses\n",
        "the powerful autograd system for any computational graphs written with\n",
        "`chainer.FunctionNode`\\ s and `chainer.Link`\\ s (actually a\n",
        "`chainer.Link` calls a corresponding `chainer.FunctionNode`\n",
        "inside of it), so that you don't need to explicitly write the code for backward\n",
        "computations in the model. Just prepare the data, then give it to the model.\n",
        "\n",
        "The way this works is the resulting output `chainer.Variable` from the\n",
        "forward computation has a `chainer.Variable.backward` method to perform\n",
        "autograd. In the above model, `__call__` has a ``if`` statement at the\n",
        "end to switch its behavior by the Chainer's running mode, i.e., training mode or\n",
        "not. Chainer presents the running mode as a global variable ``chainer.config.train``.\n",
        "\n",
        "When it's in training mode, `__call__` returns the output value of the\n",
        "last layer as is to compute the loss later on, otherwise it returns a\n",
        "prediction result by calculating `chainer.functions.softmax`."
      ]
    },
    {
      "metadata": {
        "id": "zaRmPHh_PaJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "> In Chainer v1, if a function or link behaved differently in\n",
        "> training and other modes, it was common that it held an attribute\n",
        "> that represented its running mode or was provided with the mode\n",
        "> from outside as an argument. In Chainer v2, it is recommended to use\n",
        "> the global configuration `chainer.config.train` to switch the running mode.\n"
      ]
    },
    {
      "metadata": {
        "id": "20wNAeRUPoQi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you don't want to write ``conv1`` and the other layers more than once, you\n",
        "can also write the model like in this way:"
      ]
    },
    {
      "metadata": {
        "id": "uIBiqO44PfMW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "    class LeNet5(Chain):\n",
        "        def __init__(self):\n",
        "            super(LeNet5, self).__init__()\n",
        "            net = [('conv1', L.Convolution2D(1, 6, 5, 1))]\n",
        "            net += [('_sigm1', F.Sigmoid())]\n",
        "            net += [('_mpool1', F.MaxPooling2D(2, 2))]\n",
        "            net += [('conv2', L.Convolution2D(6, 16, 5, 1))]\n",
        "            net += [('_sigm2', F.Sigmoid())]\n",
        "            net += [('_mpool2', F.MaxPooling2D(2, 2))]\n",
        "            net += [('conv3', L.Convolution2D(16, 120, 4, 1))]\n",
        "            net += [('_sigm3', F.Sigmoid())]\n",
        "            net += [('_mpool3', F.MaxPooling2D(2, 2))]\n",
        "            net += [('fc4', L.Linear(None, 84))]\n",
        "            net += [('_sigm4', F.Sigmoid())]\n",
        "            net += [('fc5', L.Linear(84, 10))]\n",
        "            net += [('_sigm5', F.Sigmoid())]\n",
        "            with self.init_scope():\n",
        "                for n in net:\n",
        "                    if not n[0].startswith('_'):\n",
        "                        setattr(self, n[0], n[1])\n",
        "            self.forward = net\n",
        "\n",
        "        def __call__(self, x):\n",
        "            for n, f in self.forward:\n",
        "                if not n.startswith('_'):\n",
        "                    x = getattr(self, n)(x)\n",
        "                else:\n",
        "                    x = f(x)\n",
        "            if chainer.config.train:\n",
        "                return x\n",
        "            return F.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "coslyHOtQGxm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code creates a list of all `chainer.Link`\\ s and\n",
        "`chainer.FunctionNode`\\ s after calling its superclass's constructor.\n",
        "\n",
        "Then the elements of the list are registered to this model as\n",
        "trainable layers when the name of an element doesn't start with ``_``\n",
        "character. This operation can be freely replaced with many other ways because\n",
        "those names are just designed to select `chainer.Link`\\ s only from the\n",
        "list ``net`` easily. `chainer.FunctionNode` doesn't have any trainable\n",
        "parameters, so that we can't register it to the model, but we want to use\n",
        "`chainer.FunctionNode`\\ s for constructing a forward path. The list\n",
        "``net`` is stored as an attribute `forward` to refer it in\n",
        "`__call__`. In `__call__`, it retrieves all layers in the network\n",
        "from `self.forward` sequentially regardless of what types of object (\n",
        "`chainer.Link` or `chainer.FunctionNode`) it is, and gives the\n",
        "input variable or the intermediate output from the previous layer to the\n",
        "current layer. The last part of the `__call__` to switch its behavior\n",
        "by the training/inference mode is the same as the former way."
      ]
    },
    {
      "metadata": {
        "id": "3OtpUJ_cQP1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Ways to calculate loss\n",
        "\n",
        "When you train the model with label vector ``t``, the loss should be calculated\n",
        "using the output from the model. There also are several ways to calculate the\n",
        "loss:"
      ]
    },
    {
      "metadata": {
        "id": "CCziSWwtQLY7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def do():\n",
        "    model = LeNet5()\n",
        "\n",
        "    # Input data and label\n",
        "    x = np.random.rand(32, 1, 28, 28).astype(np.float32)\n",
        "    t = np.random.randint(0, 10, size=(32,)).astype(np.int32)\n",
        "\n",
        "    # Forward computation\n",
        "    y = model(x)\n",
        "\n",
        "    # Loss calculation\n",
        "    loss = F.softmax_cross_entropy(y, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAuoNPUjbmFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a primitive way to calculate a loss value from the output of the model.\n",
        "\n",
        "On the other hand, the loss computation can be included in the model itself by\n",
        "wrapping the model object (`chainer.Chain` or\n",
        "`chainer.ChainList` object) with a class inherited from\n",
        "`chainer.Chain`. The outer `chainer.Chain` should take the\n",
        "model defined above and register it with `chainer.Chain.init_scope`.\n",
        "\n",
        "`chainer.Chain` is actually\n",
        "inherited from `chainer.Link`, so that `chainer.Chain` itself\n",
        "can also be registered as a trainable `chainer.Link` to another\n",
        "`chainer.Chain`. Actually, `chainer.links.Classifier` class to\n",
        "wrap the model and add the loss computation to the model already exists.\n",
        "\n",
        "Actually, there is already a `chainer.links.Classifier` class that can\n",
        "be used to wrap the model and include the loss computation as well.\n",
        "\n",
        "It can be used like this\n"
      ]
    },
    {
      "metadata": {
        "id": "5AMnm8vJQVgy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def do():\n",
        "    model = L.Classifier(LeNet5())\n",
        "\n",
        "    # Foward & Loss calculation\n",
        "    loss = model(x, t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7_DiDKzb9PH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class takes a model object as an input argument and registers it to\n",
        "a ``predictor`` property as a trained parameter. As shown above, the returned\n",
        "object can then be called like a function in which we pass ``x`` and ``t`` as\n",
        "the input arguments and the resulting loss value (which we recall is a\n",
        "`chainer.Variable`) is returned.\n",
        "\n",
        "See the detailed implementation of `chainer.links.Classifier` from\n",
        "hereclass`chainer.links.Classifier` and check the implementation by looking\n",
        "at the source.\n",
        "\n",
        "From the above examples, we can see that Chainer provides the flexibility to\n",
        "write our original network in many different ways. Such flexibility intends to\n",
        "make it intuitive for users to design new and complex models.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WOurFBsBcAWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### VGG16\n",
        "\n",
        "Next, let's write some larger models in Chainer. When you write a large network\n",
        "consisting of several building block networks, :class:`chainer.ChainList` is\n",
        "useful. First, let's see how to write a VGG16 [Simonyan14] model.\n"
      ]
    },
    {
      "metadata": {
        "id": "T_D2bGV9byTj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class VGG16(chainer.ChainList):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__(\n",
        "            VGGBlock(64),\n",
        "            VGGBlock(128),\n",
        "            VGGBlock(256, 3),\n",
        "            VGGBlock(512, 3),\n",
        "            VGGBlock(512, 3, True))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for f in self.children():\n",
        "            x = f(x)\n",
        "        if chainer.config.train:\n",
        "            return x\n",
        "        return F.softmax(x)\n",
        "\n",
        "\n",
        "class VGGBlock(chainer.Chain):\n",
        "    def __init__(self, n_channels, n_convs=2, fc=False):\n",
        "        w = chainer.initializers.HeNormal()\n",
        "        super(VGGBlock, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution2D(None, n_channels, 3, 1, 1, initialW=w)\n",
        "            self.conv2 = L.Convolution2D(\n",
        "                n_channels, n_channels, 3, 1, 1, initialW=w)\n",
        "            if n_convs == 3:\n",
        "                self.conv3 = L.Convolution2D(\n",
        "                    n_channels, n_channels, 3, 1, 1, initialW=w)\n",
        "            if fc:\n",
        "                self.fc4 = L.Linear(None, 4096, initialW=w)\n",
        "                self.fc5 = L.Linear(4096, 4096, initialW=w)\n",
        "                self.fc6 = L.Linear(4096, 1000, initialW=w)\n",
        "\n",
        "        self.n_convs = n_convs\n",
        "        self.fc = fc\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = F.relu(self.conv1(x))\n",
        "        h = F.relu(self.conv2(h))\n",
        "        if self.n_convs == 3:\n",
        "            h = F.relu(self.conv3(h))\n",
        "        h = F.max_pooling_2d(h, 2, 2)\n",
        "        if self.fc:\n",
        "            h = F.dropout(F.relu(self.fc4(h)))\n",
        "            h = F.dropout(F.relu(self.fc5(h)))\n",
        "            h = self.fc6(h)\n",
        "        return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fCP9FOHQdAV6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's it. VGG16 is a model which won the 1st place in\n",
        "[classification + localization task at ILSVRC 2014](http://www.image-net.org/challenges/LSVRC/2014/results#clsloc,\n",
        "and since then, has become one of the standard models for many different tasks\n",
        "as a pre-trained model.\n",
        "\n",
        "This has 16-layers, so it's called \"VGG-16\", but we can\n",
        "write this model without writing all layers independently. Since this model\n",
        "consists of several building blocks that have the same architecture, we can\n",
        "build the whole network by re-using the building block definition.\n",
        "\n",
        "Each part of the network is consisted of 2 or 3 convolutional layers and activation\n",
        "function (`chainer.functions.relu`) following them, and\n",
        "`chainer.functions.max_pooling_2d` operations. This block is written as\n",
        "`VGGBlock` in the above example code. And the whole network just calls\n",
        "this block one by one in sequential manner.\n"
      ]
    },
    {
      "metadata": {
        "id": "oOyKzj-7c7zD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuwSx9lXdh0P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ResNet152\n",
        "\n",
        "\n",
        "How about ResNet? ResNet [He16] came in the following year's ILSVRC. It is a\n",
        "much deeper model than VGG16, having up to 152 layers. This sounds super\n",
        "laborious to build, but it can be implemented in almost same manner as VGG16.\n",
        "\n",
        "In the other words, it's easy. One possible way to write ResNet-152 is:\n"
      ]
    },
    {
      "metadata": {
        "id": "QbPfVL_ydqA4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class ResNet152(chainer.Chain):\n",
        "    def __init__(self, n_blocks=[3, 8, 36, 3]):\n",
        "        w = chainer.initializers.HeNormal()\n",
        "        super(ResNet152, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1 = L.Convolution2D(None, 64, 7, 2, 3, initialW=w, nobias=True)\n",
        "            self.bn1 = L.BatchNormalization(64)\n",
        "            self.res2 = ResBlock(n_blocks[0], 64, 64, 256, 1)\n",
        "            self.res3 = ResBlock(n_blocks[1], 256, 128, 512)\n",
        "            self.res4 = ResBlock(n_blocks[2], 512, 256, 1024)\n",
        "            self.res5 = ResBlock(n_blocks[3], 1024, 512, 2048)\n",
        "            self.fc6 = L.Linear(2048, 1000)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self.bn1(self.conv1(x))\n",
        "        h = F.max_pooling_2d(F.relu(h), 2, 2)\n",
        "        h = self.res2(h)\n",
        "        h = self.res3(h)\n",
        "        h = self.res4(h)\n",
        "        h = self.res5(h)\n",
        "        h = F.average_pooling_2d(h, h.shape[2:], stride=1)\n",
        "        h = self.fc6(h)\n",
        "        if chainer.config.train:\n",
        "            return h\n",
        "        return F.softmax(h)\n",
        "\n",
        "\n",
        "class ResBlock(chainer.ChainList):\n",
        "    def __init__(self, n_layers, n_in, n_mid, n_out, stride=2):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.add_link(BottleNeck(n_in, n_mid, n_out, stride, True))\n",
        "        for _ in range(n_layers - 1):\n",
        "            self.add_link(BottleNeck(n_out, n_mid, n_out))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for f in self.children():\n",
        "            x = f(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeck(chainer.Chain):\n",
        "    def __init__(self, n_in, n_mid, n_out, stride=1, proj=False):\n",
        "        w = chainer.initializers.HeNormal()\n",
        "        super(BottleNeck, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.conv1x1a = L.Convolution2D(\n",
        "                n_in, n_mid, 1, stride, 0, initialW=w, nobias=True)\n",
        "            self.conv3x3b = L.Convolution2D(\n",
        "                n_mid, n_mid, 3, 1, 1, initialW=w, nobias=True)\n",
        "            self.conv1x1c = L.Convolution2D(\n",
        "                n_mid, n_out, 1, 1, 0, initialW=w, nobias=True)\n",
        "            self.bn_a = L.BatchNormalization(n_mid)\n",
        "            self.bn_b = L.BatchNormalization(n_mid)\n",
        "            self.bn_c = L.BatchNormalization(n_out)\n",
        "            if proj:\n",
        "                self.conv1x1r = L.Convolution2D(\n",
        "                    n_in, n_out, 1, stride, 0, initialW=w, nobias=True)\n",
        "                self.bn_r = L.BatchNormalization(n_out)\n",
        "        self.proj = proj\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = F.relu(self.bn_a(self.conv1x1a(x)))\n",
        "        h = F.relu(self.bn_b(self.conv3x3b(h)))\n",
        "        h = self.bn_c(self.conv1x1c(h))\n",
        "        if self.proj:\n",
        "            x = self.bn_r(self.conv1x1r(x))\n",
        "        return F.relu(h + x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "isqRzDSedxsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the `BottleNeck` class, depending on the value of the proj argument\n",
        "supplied to the initializer, it will conditionally compute a convolutional\n",
        "layer ``conv1x1r`` which will extend the number of channels of the input ``x``\n",
        "to be equal to the number of channels of the output of ``conv1x1c``, and\n",
        "followed by a batch normalization layer before the final ReLU layer.\n",
        "\n",
        "Writing the building block in this way improves the re-usability of a class.\n",
        "It switches not only the behavior in `__class__` by flags but also the\n",
        "parameter registration. In this case, when `proj` is ``False``, the\n",
        "`BottleNeck` doesn't have `conv1x1r` and `bn_r` layers, so the memory\n",
        "usage would be efficient compared to the case when it registers both anyway and\n",
        "just ignore them if `proj` is ``False``.\n",
        "\n",
        "Using nested `chainer.Chain` s and `chainer.ChainList` for\n",
        "sequential part enables us to write complex and very deep models easily.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-l9qikeyd6Dm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Use Pre-trained Models\n",
        "\n",
        "Various ways to write your models were described above. It turns out that\n",
        "VGG16 and ResNet are very useful as general feature extractors for many kinds\n",
        "of tasks, including but not limited to image classification. So, Chainer\n",
        "provides you with the pre-trained VGG16 and ResNet-50/101/152 models with a\n",
        "simple API. You can use these models as follows:\n"
      ]
    },
    {
      "metadata": {
        "id": "qNy9pa3PduRO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf39d310-0376-4b66-af09-81e79dcfe39f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520062360752,
          "user_tz": -540,
          "elapsed": 90387,
          "user": {
            "displayName": "Yasuki IKEUCHI",
            "photoUrl": "//lh6.googleusercontent.com/-QrqZkF-x2us/AAAAAAAAAAI/AAAAAAAAAAA/PzOMhw6mH3o/s50-c-k-no/photo.jpg",
            "userId": "110487681503898314699"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from chainer.links import VGG16Layers\n",
        "model = VGG16Layers()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://www.robots.ox.ac.uk/%7Evgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Now loading caffemodel (usually it may take few minutes)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mgkF7PjbeNg0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When `chainer.links.VGG16Layers` is instantiated, the pre-trained\n",
        "parameters are automatically downloaded from the author's server. So you can\n",
        "immediately start to use VGG16 with pre-trained weight as a good image feature\n",
        "extractor. See the details of this model here:\n",
        "`chainer.links.VGG16Layers`.\n",
        "\n",
        "In the case of ResNet models, there are three variations differing in the number\n",
        "of layers. We have `chainer.links.ResNet50Layers`,\n",
        "`chainer.links.ResNet101Layers`, and `chainer.links.ResNet152Layers` models\n",
        "with easy parameter loading feature. ResNet's pre-trained parameters are not\n",
        "available for direct downloading, so you need to download the weight from the\n",
        "author's web page first, and then place it into the dir\n",
        "``$CHAINER_DATSET_ROOT/pfnet/chainer/models`` or your favorite place. Once\n",
        "the preparation is finished, the usage is the same as VGG16:\n"
      ]
    },
    {
      "metadata": {
        "id": "XX-M8jirrjv6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**To use on Colaboratory, you need to upload ResNet's pre-trained parameters to your Google drive.**\n",
        "\n",
        "1. Please download pre-trained model name `ResNet-152-model.caffemodel` to your local. You can find the link from https://github.com/KaimingHe/deep-residual-networks#models\n",
        "2. Please upload `ResNet-152-model.caffemodel` to **root of your Google drive**.\n",
        "3. Please run below code\n",
        "4. Please input verification code (It is required at first time)\n",
        "\n",
        "The below code shows 1) authentication, 2) list your root of Google drive and find `ResNet-152-model.caffemodel`, 3) download. For more information, please refer to [PyDrive documentation](https://googledrive.github.io/PyDrive/docs/build/html/index.html)"
      ]
    },
    {
      "metadata": {
        "id": "pDAiXqZremwV",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "import os\n",
        "\n",
        "PATH= os.path.expanduser('.chainer/dataset/pfnet/chainer/models')\n",
        "FILE = 'ResNet-152-model.caffemodel'\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n",
        "\n",
        "\n",
        "file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "id = None\n",
        "for file in file_list:\n",
        "  if file['title'] == FILE:\n",
        "    id = file['id']\n",
        "    break\n",
        "\n",
        "if id is None:\n",
        "  print(\"{} is not Found\".format(FILE))\n",
        "else:\n",
        "  file = drive.CreateFile({'id': id})\n",
        "  file.GetContentFile(os.path.join(PATH, FILE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxCr-8t1eBfm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed4b55af-f44c-4c6f-a4d8-68b2c9515de3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1520062453955,
          "user_tz": -540,
          "elapsed": 11191,
          "user": {
            "displayName": "Yasuki IKEUCHI",
            "photoUrl": "//lh6.googleusercontent.com/-QrqZkF-x2us/AAAAAAAAAAI/AAAAAAAAAAA/PzOMhw6mH3o/s50-c-k-no/photo.jpg",
            "userId": "110487681503898314699"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from chainer.links import ResNet152Layers\n",
        "\n",
        "model = ResNet152Layers()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now loading caffemodel (usually it may take few minutes)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oZ1ugwsueR-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Please see the details of usage and how to prepare the pre-trained weights for\n",
        "ResNet here: `chainer.links.ResNet50Layers`\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "* [LeCun98] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.\n",
        "    Gradient-based learning applied to document recognition. Proceedings of the\n",
        "    IEEE, 86(11), 2278–2324, 1998.\n",
        "* [Simonyan14] Simonyan, K. and Zisserman, A., Very Deep Convolutional\n",
        "    Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556,\n",
        "    2014.\n",
        "* [He16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Deep Residual\n",
        "    Learning for Image Recognition. The IEEE Conference on Computer Vision and\n",
        "    Pattern Recognition (CVPR), pp. 770-778, 2016.\n",
        " "
      ]
    }
  ]
}
