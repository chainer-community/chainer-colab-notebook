{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_en.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JR5-Ja21Ojj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec: Obtain word embeddings\n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "**Word2vec** is the tool for generating the distributed representation of words, which is proposed by Mikolov et al[[1]](#1). When the tool assigns a real-valued vector to each word, the closer the meanings of the words, the greater similarity the vectors will indicate.\n",
        "\n",
        "**Distributed representation** means assigning a real-valued vector for each word and representing the word by the vector. When representing a word by distributed representation, we call the vector **word embeddings**. In this notebook, we aim at explaining how to get the word embeddings from Penn Tree Bank dataset.\n",
        "\n",
        "Let's think about what the meaning of word is. Since we are human, so we can understand that the words \"animal\" and \"dog\" are deeply related each other. But what information will Word2vec use to learn the vectors for words? The words \"animal\" and \"dog\" should have similar vectors, but the words \"food\" and \"dog\" should be far from each other. How to know the features of those words automatically?"
      ]
    },
    {
      "metadata": {
        "id": "8n5aX8oAOjj_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Basic Idea\n",
        "\n",
        "Word2vec learns the similarity of word meanings from simple information. It learns the representation of words from sentences. The core idea is based on the assumption that the meaning of a word is affected by the words around it. This idea follows **distributional hypothesis**[[2]](#2).\n",
        "\n",
        "The word we focus on to learn its representation is called **\"center word\"**, and the words around it are called **\"context words\"**. Depending on the window size `C` determines the number of context words which is considered.\n",
        "\n",
        "Here, let's see the algorithm by using an example sentence: \"**The cute cat jumps over the lazy dog.**\"\n",
        "\n",
        "- All of the following figures consider \"cat\" as the center word.\n",
        "- According to the window size `C`, you can see that the number of context words is changed.\n",
        "\n",
        "![center_context_word.png](https://docs.chainer.org/en/stable/_images/center_context_word.png)"
      ]
    },
    {
      "metadata": {
        "id": "CUHlpGTJOjj_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Main Algorithm\n",
        "\n",
        "Word2vec, the tool for creating the word embeddings, is actually built with two models, which are called **Skip-gram** and **CBoW**.\n",
        "\n",
        "To explain the models with the figures below, we will use the following symbols.\n",
        "\n",
        "| Symbol    | Definition                                               |\n",
        "| --------: | :------------------------------------------------------- |\n",
        "| $|\\mathcal{V}|$       | The size of vocabulary                                   |\n",
        "| $D$       | The size of embedding vector                             |\n",
        "| ${\\bf v}_t$     | A one-hot center word vector                             |\n",
        "| $V_{\\pm C}$ | A set of $C$ context vectors around ${\\bf v}_t$, namely, $\\{{\\bf v}_{t+c}\\}_{c=-C}^C \\backslash {\\bf v}_t$        |\n",
        "| ${\\bf l}_H$     | An embedding vector of an input word vector              |\n",
        "| ${\\bf l}_O$     | An output vector of the network                          |\n",
        "| ${\\bf W}_H$     | The embedding matrix for inputs                          |\n",
        "| ${\\bf W}_O$     | The embedding matrix for outputs                         |\n",
        "\n",
        "**Note**\n",
        "\n",
        "Using **negative sampling** or **hierarchical softmax** for the loss function is very common, however, in this notebook, we will use the **softmax over all words** and skip the other variants for the sake of simplicity."
      ]
    },
    {
      "metadata": {
        "id": "AFx1gZ8KOjkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Skip-gram\n",
        "\n",
        "This model learns to predict context words $V_{t \\pm C}$ when a center word ${\\bf v}_t$ is given. In the model, each row of the embedding matrix for input $W_H$ becomes a word embedding of each word.\n",
        "\n",
        "When you input a center word ${\\bf v}_t$ into the network, you can predict one of context words $\\hat{\\bf v}_{t+i} \\in V_{t \\pm C}$ as follows:\n",
        "\n",
        "1. Calculate an embedding vector of the input center word vector: ${\\bf l}_H = {\\bf W}_H {\\bf v}_t$\n",
        "2. Calculate an output vector of the embedding vector: ${\\bf l}_O = {\\bf W}_O {\\bf l}_H$\n",
        "3. Calculate a probability vector of a context word: $\\hat{\\bf v}_{t+i} = \\text{softmax}({\\bf l}_O)$\n",
        "\n",
        "Each element of the $|\\mathcal{V}|$-dimensional vector $\\hat{\\bf v}_{t+i}$ is a probability that a word in the vocabulary turns out to be a context word at position $i$. So, the probability $p({\\bf v}_{t+i} \\mid {\\bf v}_t)$ can be estimated by a dot product of the one-hot vector ${\\bf v}_{t+i}$ which represents the actual word at the position $i$ and the output vector $\\hat{\\bf v}_{t+i}$.\n",
        "\n",
        "$p({\\bf v}_{t+i} \\mid {\\bf v}_t) = {\\bf v}_{t+i}^T \\hat{\\bf v}_{t+i}$\n",
        "\n",
        "The loss function for all the context words $V_{t \\pm C}$ given a center word ${\\bf v}_t$ is defined as following:\n",
        "\n",
        "$\n",
        "\\begin{eqnarray}\n",
        "L(V_{t \\pm C} | {\\bf v}_t; {\\bf W}_H, {\\bf W}_O)\n",
        "&=& \\sum_{V_{t \\pm C}} -\\log\\left(p({\\bf v}_{t+i} \\mid {\\bf v}_t)\\right) \\\\\n",
        "&=& \\sum_{V_{t \\pm C}} -\\log({\\bf v}_{t+i}^T \\hat{\\bf v}_{t+i})\n",
        "\\end{eqnarray}\n",
        "$"
      ]
    },
    {
      "metadata": {
        "id": "TAmvso8yOjkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Continuous Bag of Words (CBoW)\n",
        "\n",
        "This model learns to predict the center word ${\\bf v}_t$ when context words $V_{t \\pm C}$ is given.\n",
        "\n",
        "When you give a set of context words $V_{t \\pm C}$ to the network, you can estimate the probability of the center word $\\hat{v}_t$ as follows:\n",
        "\n",
        "1. Calculate a mean embedding vector over all context words: ${\\bf l}_H = \\frac{1}{2C} \\sum_{V_{t \\pm C}} {\\bf W}_H {\\bf v}_{t+i}$\n",
        "2. Calculate an output vector: ${\\bf l}_O = {\\bf W}_O {\\bf l}_H$\n",
        "3. Calculate an probability vector: $\\hat{\\bf v}_t = \\text{softmax}({\\bf l}_O)$\n",
        "\n",
        "Each element of $\\hat{\\bf v}_t$ is a probability that a word in the vocabulary is considered as the center word. So, the prediction $p({\\bf v}_t \\mid V_{t \\pm C})$ can be calculated by ${\\bf v}_t^T \\hat{\\bf v}_t$, where ${\\bf v}_t$ denots the one-hot vector of the actual center word vector in the sentence from the dataset.\n",
        "\n",
        "The loss function for the center word prediction is defined as follows:\n",
        "\n",
        "$\n",
        "\\begin{eqnarray}\n",
        "L({\\bf v}_t|V_{t \\pm C}; W_H, W_O)\n",
        "&=& -\\log(p({\\bf v}_t|V_{t \\pm C})) \\\\\n",
        "&=& -\\log({\\bf v}_t^T \\hat{\\bf v}_t)\n",
        "\\end{eqnarray}\n",
        "$"
      ]
    },
    {
      "metadata": {
        "id": "YZKrh07hOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Details of skip-gram\n",
        "\n",
        "In this notebook, we mainly explain skip-gram model because\n",
        "\n",
        "1. It is easier to understand the algorithm than CBoW.\n",
        "2. Even if the number of words increases, the accuracy is largely maintained. So, it is more scalable."
      ]
    },
    {
      "metadata": {
        "id": "QMspiJZzOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, let's think about a concrete example of calculating skip-gram under this setup:\n",
        "\n",
        "* The size of vocabulary $|\\mathcal{V}|$ is 10.\n",
        "* The size of embedding vector $D$ is 2.\n",
        "* Center word is \"dog\".\n",
        "* Context word is \"animal\".\n",
        "\n",
        "Since there should be more than one context words, repeat the following process for each context word.\n",
        "\n",
        "1. The one-hot vector of \"dog\" is `[0 0 1 0 0 0 0 0 0 0]` and you input it as the center word.\n",
        "2. The third row of embedding matrix ${\\bf W}_H$ is used for the word embedding of \"dog\" ${\\bf l}_H$.\n",
        "3. Then multiply ${\\bf W}_O$ with ${\\bf l}_H$ to obtain the output vector ${\\bf l}_O$\n",
        "4. Give ${\\bf l}_O$ to the softmax function to make it a predicted probability vector $\\hat{\\bf v}_{t+c}$ for a context word at the position $c$.\n",
        "5. Calculate the error between $\\hat{\\bf v}_{t+c}$ and the one-hot vector of \"animal\"; `[1 0 0 0 0 0 0 0 0 0 0]`.\n",
        "6. Propagate the error back to the network to update the parameters.\n",
        "\n",
        "![skipgram_detail.png](https://docs.chainer.org/en/stable/_images/skipgram_detail.png)"
      ]
    },
    {
      "metadata": {
        "id": "IQ19qx2UOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Implementation of skip-gram in Chainer\n",
        "\n",
        "There is an example of Word2vec in the official repository of Chainer, so we will explain how to implement skip-gram based on this: [chainer/examples/word2vec](https://github.com/chainer/chainer/tree/master/examples/word2vec)"
      ]
    },
    {
      "metadata": {
        "id": "-fxZ_DM3O7Ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, we execute the following cell and install “Chainer” and its GPU back end “CuPy”. If the “runtime type” of Colaboratory is GPU, you can run Chainer with GPU as a backend."
      ]
    },
    {
      "metadata": {
        "id": "9xOsXDFRO54W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "ae9cad40-8a15-4b24-917c-5d4b239c1b6a"
      },
      "cell_type": "code",
      "source": [
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install -q cupy-cuda80 chainer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcusparse8.0 is already the newest version (8.0.61-1).\n",
            "libnvrtc8.0 is already the newest version (8.0.61-1).\n",
            "libnvtoolsext1 is already the newest version (8.0.61-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2O-8RkiOjkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 Preparation\n",
        "\n",
        "First, let's import necessary packages:"
      ]
    },
    {
      "metadata": {
        "id": "OvL9Gk08OjkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import six\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.initializers as I\n",
        "import chainer.links as L\n",
        "import chainer.optimizers as O\n",
        "from chainer import reporter\n",
        "from chainer import training\n",
        "from chainer.training import extensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXuiONJbOjkG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 Define a skip-gram model\n",
        "\n",
        "Next, let's define a network for skip-gram."
      ]
    },
    {
      "metadata": {
        "id": "jLNEBp7gOjkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGram(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_vocab, n_units):\n",
        "        super().__init__()\n",
        "        with self.init_scope():\n",
        "            self.embed = L.EmbedID(\n",
        "                n_vocab, n_units, initialW=I.Uniform(1. / n_units))\n",
        "            self.out = L.Linear(n_units, n_vocab, initialW=0)\n",
        "\n",
        "    def __call__(self, x, context):\n",
        "        e = self.embed(context)\n",
        "        shape = e.shape\n",
        "        x = F.broadcast_to(x[:, None], (shape[0], shape[1]))\n",
        "        e = F.reshape(e, (shape[0] * shape[1], shape[2]))\n",
        "        x = F.reshape(x, (shape[0] * shape[1],))\n",
        "        center_predictions = self.out(e)\n",
        "        loss = F.softmax_cross_entropy(center_predictions, x)\n",
        "        reporter.report({'loss': loss}, self)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xL8OWlpZOjkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "- The weight matrix `self.embed.W` is the embbeding matrix for input vector `x`.\n",
        "- `__call__` takes the word ID of a center word `x` and word IDs of context words `contexts` as inputs, and outputs the error calculated by the loss function `softmax_cross_entropy`.\n",
        "- Note that the initial shape of `x` and `contexts` are `(batch_size,)` and `(batch_size, n_context)`, respectively.\n",
        "- The `batch_size` means the size of mini-batch, and `n_context` means the number of context words.\n",
        "\n",
        "First, we obtain the embedding vectors of `contexts` by `e = self.embed(contexts)`. \n",
        "\n",
        "Then `F.broadcast_to(x[:, None], (shape[0], shape[1]))` performs broadcasting of `x` (`(batch_size,)`) to `(batch_size, n_context)` by copying the same value `n_context` time to fill the second axis, and then the broadcasted `x` is reshaped into 1-D vector `(batchsize * n_context,)` while `e` is reshaped to `(batch_size * n_context, n_units)`.\n",
        "\n",
        "In skip-gram model, predicting a context word from the center word is the same as predicting the center word from a context word because the center word is always a context word when considering the context word as a center word. So, we create `batch_size * n_context` center word predictions by applying `self.out` linear layer to the embedding vectors of context words. Then, calculate softmax cross entropy between the broadcasted center word ID `x` and the predictions."
      ]
    },
    {
      "metadata": {
        "id": "dFAB4sObOjkJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3 Prepare dataset and iterator\n",
        "\n",
        "Let's retrieve the Penn Tree Bank (PTB) dataset by using Chainer's dataset utility `get_ptb_words()` method."
      ]
    },
    {
      "metadata": {
        "id": "pqUBJ1r2OjkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, val, _ = chainer.datasets.get_ptb_words()\n",
        "n_vocab = max(train) + 1  # The minimum word ID is 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yf2sqARoOjkL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then define an iterator to make mini-batches that contain a set of center words with their context words."
      ]
    },
    {
      "metadata": {
        "id": "jwYDyNyXOjkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WindowIterator(chainer.dataset.Iterator):\n",
        "\n",
        "    def __init__(self, dataset, window, batch_size, repeat=True):\n",
        "        self.dataset = np.array(dataset, np.int32)\n",
        "        self.window = window\n",
        "        self.batch_size = batch_size\n",
        "        self._repeat = repeat\n",
        "\n",
        "        self.order = np.random.permutation(\n",
        "            len(dataset) - window * 2).astype(np.int32)\n",
        "        self.order += window\n",
        "        self.current_position = 0\n",
        "        self.epoch = 0\n",
        "        self.is_new_epoch = False\n",
        "\n",
        "    def __next__(self):\n",
        "        if not self._repeat and self.epoch > 0:\n",
        "            raise StopIteration\n",
        "\n",
        "        i = self.current_position\n",
        "        i_end = i + self.batch_size\n",
        "        position = self.order[i: i_end]\n",
        "        w = np.random.randint(self.window - 1) + 1\n",
        "        offset = np.concatenate([np.arange(-w, 0), np.arange(1, w + 1)])\n",
        "        pos = position[:, None] + offset[None, :]\n",
        "        context = self.dataset.take(pos)\n",
        "        center = self.dataset.take(position)\n",
        "\n",
        "        if i_end >= len(self.order):\n",
        "            np.random.shuffle(self.order)\n",
        "            self.epoch += 1\n",
        "            self.is_new_epoch = True\n",
        "            self.current_position = 0\n",
        "        else:\n",
        "            self.is_new_epoch = False\n",
        "            self.current_position = i_end\n",
        "\n",
        "        return center, context\n",
        "\n",
        "    @property\n",
        "    def epoch_detail(self):\n",
        "        return self.epoch + float(self.current_position) / len(self.order)\n",
        "\n",
        "    def serialize(self, serializer):\n",
        "        self.current_position = serializer('current_position',\n",
        "                                           self.current_position)\n",
        "        self.epoch = serializer('epoch', self.epoch)\n",
        "        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)\n",
        "        if self._order is not None:\n",
        "            serializer('_order', self._order)\n",
        "\n",
        "def convert(batch, device):\n",
        "    center, context = batch\n",
        "    if device >= 0:\n",
        "        center = cuda.to_gpu(center)\n",
        "        context = cuda.to_gpu(context)\n",
        "    return center, context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Zq2jxKgOjkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- In the constructor, we create an array `self.order` which denotes shuffled indices of `[window, window + 1, ..., len(dataset) - window - 1]` in order to choose a center word randomly from `dataset` in a mini-batch.\n",
        "- The iterator definition `__next__` returns `batch_size` sets of  center word and context words.\n",
        "- The code `self.order[i:i_end]` returns the indices for a set of center words from the random-ordered array `self.order`. The center word IDs `center` at the random indices are retrieved by `self.dataset.take`.\n",
        "- `np.concatenate([np.arange(-w, 0), np.arange(1, w + 1)])` creates a set of offsets to retrieve context words from the dataset.\n",
        "- The code `position[:, None] + offset[None, :]` generates the indices of context words for each center word index in `position`. The context word IDs `context` are retrieved by `self.dataset.take`."
      ]
    },
    {
      "metadata": {
        "id": "IlIoeqeNOjkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4 Prepare model, optimizer, and updater"
      ]
    },
    {
      "metadata": {
        "id": "OgPecreHOjkP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unit = 100  # number of hidden units\n",
        "window = 5\n",
        "batchsize = 1000\n",
        "gpu = 0\n",
        "\n",
        "# Instantiate model\n",
        "model = SkipGram(n_vocab, unit)\n",
        "\n",
        "if gpu >= 0:\n",
        "    model.to_gpu(gpu)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = O.Adam()\n",
        "optimizer.setup(model)\n",
        "\n",
        "# Create iterators for both train and val datasets\n",
        "train_iter = WindowIterator(train, window, batchsize)\n",
        "val_iter = WindowIterator(val, window, batchsize, repeat=False)\n",
        "\n",
        "# Create updater\n",
        "updater = training.StandardUpdater(\n",
        "    train_iter, optimizer, converter=convert, device=gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccVACTY-OjkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.5 Start training"
      ]
    },
    {
      "metadata": {
        "id": "OVPlZWePOjkS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1700
        },
        "outputId": "feb79699-0038-4df4-8e99-f480df3caace"
      },
      "cell_type": "code",
      "source": [
        "epoch = 100\n",
        "\n",
        "trainer = training.Trainer(updater, (epoch, 'epoch'), out='word2vec_result')\n",
        "trainer.extend(extensions.Evaluator(val_iter, model, converter=convert, device=gpu))\n",
        "trainer.extend(extensions.LogReport())\n",
        "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'elapsed_time']))\n",
        "trainer.run()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  elapsed_time\n",
            "\u001b[J1           6.87314     6.48688               54.154        \n",
            "\u001b[J2           6.44018     6.40645               107.352       \n",
            "\u001b[J3           6.35021     6.3558                159.544       \n",
            "\u001b[J4           6.28615     6.31679               212.612       \n",
            "\u001b[J5           6.23762     6.28779               266.059       \n",
            "\u001b[J6           6.19942     6.22658               319.874       \n",
            "\u001b[J7           6.15986     6.20715               372.798       \n",
            "\u001b[J8           6.13787     6.21461               426.456       \n",
            "\u001b[J9           6.10637     6.24927               479.725       \n",
            "\u001b[J10          6.08759     6.23192               532.966       \n",
            "\u001b[J11          6.06768     6.19332               586.339       \n",
            "\u001b[J12          6.04607     6.17291               639.295       \n",
            "\u001b[J13          6.0321      6.21226               692.67        \n",
            "\u001b[J14          6.02178     6.18489               746.599       \n",
            "\u001b[J15          6.00098     6.17341               799.408       \n",
            "\u001b[J16          5.99099     6.19581               852.966       \n",
            "\u001b[J17          5.97425     6.22275               905.819       \n",
            "\u001b[J18          5.95974     6.20495               958.404       \n",
            "\u001b[J19          5.96579     6.16532               1012.49       \n",
            "\u001b[J20          5.95292     6.21457               1066.24       \n",
            "\u001b[J21          5.93696     6.18441               1119.45       \n",
            "\u001b[J22          5.91804     6.20695               1171.98       \n",
            "\u001b[J23          5.93265     6.15757               1225.99       \n",
            "\u001b[J24          5.92238     6.17064               1279.85       \n",
            "\u001b[J25          5.9154      6.21545               1334.01       \n",
            "\u001b[J26          5.90538     6.1812                1387.68       \n",
            "\u001b[J27          5.8807      6.18523               1439.72       \n",
            "\u001b[J28          5.89009     6.19992               1492.67       \n",
            "\u001b[J29          5.8773      6.24146               1545.48       \n",
            "\u001b[J30          5.89217     6.21846               1599.79       \n",
            "\u001b[J31          5.88493     6.21654               1653.95       \n",
            "\u001b[J32          5.87784     6.18502               1707.45       \n",
            "\u001b[J33          5.88031     6.14161               1761.75       \n",
            "\u001b[J34          5.86278     6.22893               1815.29       \n",
            "\u001b[J35          5.83335     6.18966               1866.56       \n",
            "\u001b[J36          5.85978     6.24276               1920.18       \n",
            "\u001b[J37          5.85921     6.23888               1974.2        \n",
            "\u001b[J38          5.85195     6.19231               2027.92       \n",
            "\u001b[J39          5.8396      6.20542               2080.78       \n",
            "\u001b[J40          5.83745     6.27583               2133.37       \n",
            "\u001b[J41          5.85996     6.23596               2188          \n",
            "\u001b[J42          5.85743     6.17438               2242.4        \n",
            "\u001b[J43          5.84051     6.25449               2295.84       \n",
            "\u001b[J44          5.83023     6.30226               2348.84       \n",
            "\u001b[J45          5.84677     6.23473               2403.11       \n",
            "\u001b[J46          5.82406     6.27398               2456.11       \n",
            "\u001b[J47          5.82827     6.21509               2509.17       \n",
            "\u001b[J48          5.8253      6.23009               2562.15       \n",
            "\u001b[J49          5.83697     6.2564                2616.35       \n",
            "\u001b[J50          5.81998     6.29104               2669.38       \n",
            "\u001b[J51          5.82926     6.26068               2723.47       \n",
            "\u001b[J52          5.81457     6.30152               2776.36       \n",
            "\u001b[J53          5.82587     6.29581               2830.24       \n",
            "\u001b[J54          5.80614     6.30994               2882.85       \n",
            "\u001b[J55          5.8161      6.23224               2935.73       \n",
            "\u001b[J56          5.80867     6.26867               2988.48       \n",
            "\u001b[J57          5.79467     6.24508               3040.2        \n",
            "\u001b[J58          5.81687     6.24676               3093.57       \n",
            "\u001b[J59          5.82064     6.30236               3147.68       \n",
            "\u001b[J60          5.80855     6.30184               3200.75       \n",
            "\u001b[J61          5.81298     6.25173               3254.06       \n",
            "\u001b[J62          5.80753     6.32951               3307.42       \n",
            "\u001b[J63          5.82505     6.2472                3361.68       \n",
            "\u001b[J64          5.78396     6.28168               3413.14       \n",
            "\u001b[J65          5.80209     6.24962               3465.96       \n",
            "\u001b[J66          5.80107     6.326                 3518.83       \n",
            "\u001b[J67          5.83765     6.28848               3574.57       \n",
            "\u001b[J68          5.7864      6.3506                3626.88       \n",
            "\u001b[J69          5.80329     6.30671               3679.82       \n",
            "\u001b[J70          5.80032     6.29277               3732.69       \n",
            "\u001b[J71          5.80647     6.30722               3786.21       \n",
            "\u001b[J72          5.8176      6.30046               3840.51       \n",
            "\u001b[J73          5.79912     6.35945               3893.81       \n",
            "\u001b[J74          5.80484     6.32439               3947.35       \n",
            "\u001b[J75          5.82065     6.29674               4002.03       \n",
            "\u001b[J76          5.80872     6.27921               4056.05       \n",
            "\u001b[J77          5.80891     6.28952               4110.1        \n",
            "\u001b[J78          5.79121     6.35363               4163.39       \n",
            "\u001b[J79          5.79161     6.32894               4216.34       \n",
            "\u001b[J80          5.78601     6.3255                4268.95       \n",
            "\u001b[J81          5.79062     6.29608               4321.73       \n",
            "\u001b[J82          5.7959      6.37235               4375.25       \n",
            "\u001b[J83          5.77828     6.31001               4427.44       \n",
            "\u001b[J84          5.7879      6.25628               4480.09       \n",
            "\u001b[J85          5.79297     6.29321               4533.27       \n",
            "\u001b[J86          5.79286     6.2725                4586.44       \n",
            "\u001b[J87          5.79388     6.36764               4639.82       \n",
            "\u001b[J88          5.79062     6.33841               4692.89       \n",
            "\u001b[J89          5.7879      6.31828               4745.68       \n",
            "\u001b[J90          5.81015     6.33247               4800.19       \n",
            "\u001b[J91          5.78858     6.37569               4853.31       \n",
            "\u001b[J92          5.7966      6.35733               4907.27       \n",
            "\u001b[J93          5.79814     6.34506               4961.09       \n",
            "\u001b[J94          5.81956     6.322                 5016.65       \n",
            "\u001b[J95          5.81565     6.35974               5071.69       \n",
            "\u001b[J96          5.78953     6.37451               5125.02       \n",
            "\u001b[J97          5.7993      6.42065               5179.34       \n",
            "\u001b[J98          5.79129     6.37995               5232.89       \n",
            "\u001b[J99          5.76834     6.36254               5284.7        \n",
            "\u001b[J100         5.79829     6.3785                5338.93       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqGB9iD2Tmng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = chainer.datasets.get_ptb_words_vocabulary()\n",
        "index2word = {wid: word for word, wid in six.iteritems(vocab)}\n",
        "\n",
        "# Save the word2vec model\n",
        "with open('word2vec.model', 'w') as f:\n",
        "    f.write('%d %d\\n' % (len(index2word), unit))\n",
        "    w = cuda.to_cpu(model.embed.W.data)\n",
        "    for i, wi in enumerate(w):\n",
        "        v = ' '.join(map(str, wi))\n",
        "        f.write('%s %s\\n' % (index2word[i], v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HgBVYr_b8dS8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.6 Search the similar words"
      ]
    },
    {
      "metadata": {
        "id": "7QDwFawQ8daT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import six\n",
        "\n",
        "n_result = 5  # number of search result to show\n",
        "\n",
        "\n",
        "with open('word2vec.model', 'r') as f:\n",
        "    ss = f.readline().split()\n",
        "    n_vocab, n_units = int(ss[0]), int(ss[1])\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    w = numpy.empty((n_vocab, n_units), dtype=numpy.float32)\n",
        "    for i, line in enumerate(f):\n",
        "        ss = line.split()\n",
        "        assert len(ss) == n_units + 1\n",
        "        word = ss[0]\n",
        "        word2index[word] = i\n",
        "        index2word[i] = word\n",
        "        w[i] = numpy.array([float(s) for s in ss[1:]], dtype=numpy.float32)\n",
        "\n",
        "\n",
        "s = numpy.sqrt((w * w).sum(1))\n",
        "w /= s.reshape((s.shape[0], 1))  # normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFel0uXmUfJl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "  if query not in word2index:\n",
        "    print('\"{0}\" is not found'.format(query))\n",
        "    return\n",
        "\n",
        "  v = w[word2index[query]]\n",
        "  similarity = w.dot(v)\n",
        "  print('query: {}'.format(query))\n",
        "\n",
        "  count = 0\n",
        "  for i in (-similarity).argsort():\n",
        "      if numpy.isnan(similarity[i]):\n",
        "          continue\n",
        "      if index2word[i] == query:\n",
        "          continue\n",
        "      print('{0}: {1}'.format(index2word[i], similarity[i]))\n",
        "      count += 1\n",
        "      if count == n_result:\n",
        "          return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3PrgDLi9pqf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Search by \"apple\" word."
      ]
    },
    {
      "metadata": {
        "id": "_JerH5KJ9NFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "84e097d5-80e8-4a5f-c790-5bbe104d7f2c"
      },
      "cell_type": "code",
      "source": [
        "query = \"apple\"\n",
        "search(query)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: apple\n",
            "computer: 0.5457335710525513\n",
            "compaq: 0.5068206191062927\n",
            "microsoft: 0.4654524028301239\n",
            "network: 0.42985647916793823\n",
            "trotter: 0.42716777324676514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JVXz7sbc8diq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Reference\n",
        "\n",
        "* [1] [Mikolov, Tomas; et al. “Efficient Estimation of Word Representations in Vector Space”. arXiv:1301.3781](https://arxiv.org/abs/1301.3781)\n",
        "* [2] [Distributional Hypothesis](https://aclweb.org/aclwiki/Distributional_Hypothesis)\n"
      ]
    },
    {
      "metadata": {
        "id": "HhBJdMTi8jxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}