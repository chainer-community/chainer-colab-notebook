{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec_ja.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "JR5-Ja21Ojj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Word2Vec: Obtain word embeddings\n",
        "\n",
        "## 0. Introduction\n",
        "\n",
        "**word2vec**は単語の分散表現を生成するツールで、Mikolov et al[1]によって提案されました。単語の意味が近いほど類似度が大きくなるように、word2vecは各単語に実ベクトルを割り当てます。\n",
        "\n",
        "ここで、**分散表現**とは各単語に対して実ベクトルを割り当て、そのベクトルで単語を表現することです。分散表現で単語を表現する場合、そのベクトルを**word embeddings(単語埋め込み)** と呼びます。このNotebookでは、Penn Tree Bankのデータセットからword embeddingsを獲得する方法を説明します。\n",
        "\n",
        "さて、そもそも単語の意味とはなんでしょうか。人であれば「動物」と「犬」という単語が似ているというのはなんとなく分かります。しかし、word2vecは何の情報を元に、「動物」と「犬」は似ているとか、「食べ物」と「犬」は似ていないといった意味の類似度を学習すれば良いのでしょうか。"
      ]
    },
    {
      "metadata": {
        "id": "8n5aX8oAOjj_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. 基本的なアイデア\n",
        "\n",
        "word2vecは単語の意味の類似度を単純な情報から学習します。それは文章における単語の並びです、つまりある単語の意味は、その単語の周囲の単語で決まるというアイデアです。 このアイデアは**distributional hypothesis(分布仮設)**[2]に基づいています。\n",
        "\n",
        "学習対象の単語を**center word**、その周囲の単語を**context words**と呼びます。ウィンドウサイズ`C`に応じてcontex wordの数は変わります。\n",
        "\n",
        "例として、**The cute cat jumps over the lazy dog.**という文で説明を行います。\n",
        "以下の図は全てcenter wordをcatとした場合のものです。\n",
        "ウィンドウサイズ`C`に応じて、catを学習する際に使用するcontex wordが変わることがわかると思います。\n",
        "\n",
        "![center_context_word.png](https://docs.chainer.org/en/stable/_images/center_context_word.png)"
      ]
    },
    {
      "metadata": {
        "id": "CUHlpGTJOjj_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. 主なアルゴリズム\n",
        "\n",
        "word2vecと呼ばれる手法は実は**Skip-gram**と**CBoW**という2つの手法の総称です。\n",
        "\n",
        "To explain the models with the figures below, we will use the following symbols.\n",
        "\n",
        "* $|\\mathcal{V}|$ : ボキャブラリ数\n",
        "* $D$              :  埋め込みベクトルのサイズ\n",
        "* ${\\bf v}_t$     : center wordのone-hotベクトル\n",
        "* $V_{\\pm C}$     : ${\\bf v}_t$の周囲のcontext wordのone-hotベクトルの集合、つまり$\\{{\\bf v}_{t+c}\\}_{c=-C}^C \\backslash {\\bf v}_t$\n",
        "* ${\\bf l}_H$     : 入力単語に対する埋め込みベクトル\n",
        "* ${\\bf l}_O$     : ネットワークの出力ベクトル\n",
        "* ${\\bf W}_H$     : 入力に対する埋め込み行列\n",
        "* ${\\bf W}_O$     : 出力に対する埋め込み行列\n",
        "\n",
        "**Note**\n",
        "\n",
        "**negative sampling**や**hierarchical softmax**をロス関数に使うことが一般的だが、**すべての単語に対するsoftmax関数**を使い、説明を簡略化するため他の説明は省略します。"
      ]
    },
    {
      "metadata": {
        "id": "AFx1gZ8KOjkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Skip-gram\n",
        "\n",
        "このモデルは、 center wordが与えられたときにその周囲のcontext words $V_{t \\pm C}$を予測するように学習します。この時、入力に対する埋め込み行列$W_H$の各行が各単語の分散表現になります。\n",
        "\n",
        "center word ${\\bf v}_t$をネットワークに入力したとき、以下のようにしてcontext words $\\hat{\\bf v}_{t+i} \\in V_{t \\pm C}$を予測することができます\n",
        "\n",
        "1. 入力されたcenter wordに対する埋め込みベクトルを計算する: ${\\bf l}_H = {\\bf W}_H {\\bf v}_t$\n",
        "2. 埋め込みベクトルを使って出力ベクトルを計算する: ${\\bf l}_O = {\\bf W}_O {\\bf l}_H$\n",
        "3. context wordの確率ベクトルを計算する: $\\hat{\\bf v}_{t+i} = \\text{softmax}({\\bf l}_O)$\n",
        "\n",
        "$|\\mathcal{V}|$次元のベクトル$\\hat{\\bf v}_{t+i}$の各要素は、各単語がcontext wordである確率です。そのため、確率$p({\\bf v}_{t+i} \\mid {\\bf v}_t)$は、context wordのone-hotベクトル${\\bf v}_{t+i}$と確率ベクトル$\\hat{\\bf v}_{t+i}$の内積で計算することができます。\n",
        "\n",
        "\\begin{eqnarray}\n",
        "p({\\bf v}_{t+i} \\mid {\\bf v}_t) = {\\bf v}_{t+i}^T \\hat{\\bf v}_{t+i}\n",
        "\\end{eqnarray}\n",
        "\n",
        "そして、center word ${\\bf v}_t$に対するすべてのcontext word$V_{t \\pm C}$のロス関数は以下で計算することができます。\n",
        "\n",
        "\n",
        "\\begin{eqnarray}\n",
        "L(V_{t \\pm C} | {\\bf v}_t; {\\bf W}_H, {\\bf W}_O)\n",
        "&=& \\sum_{V_{t \\pm C}} -\\log\\left(p({\\bf v}_{t+i} \\mid {\\bf v}_t)\\right) \\\\\n",
        "&=& \\sum_{V_{t \\pm C}} -\\log({\\bf v}_{t+i}^T \\hat{\\bf v}_{t+i})\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "TAmvso8yOjkA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Continuous Bag of Words (CBoW)\n",
        "\n",
        "このモデルは、context word $V_{t \\pm C}$ が与えられたときにcenter word ${\\bf v}_t$を予測するように学習します。\n",
        "\n",
        "context words $V_{t \\pm C}$をネットワークに与えたとき、以下のようにcenter word $\\hat{v}_t$の確率を計算することができます。\n",
        "\n",
        "1. すべてのcontext wordに対する埋め込みベクトルの平均を計算します: ${\\bf l}_H = \\frac{1}{2C} \\sum_{V_{t \\pm C}} {\\bf W}_H {\\bf v}_{t+i}$\n",
        "2. 埋め込みベクトルを使って出力ベクトルを計算します: ${\\bf l}_O = {\\bf W}_O {\\bf l}_H$\n",
        "3. center wordの確率ベクトルを計算する: $\\hat{\\bf v}_t = \\text{softmax}({\\bf l}_O)$\n",
        "\n",
        "$|\\mathcal{V}|$次元のベクトル$\\hat{\\bf v}_t$の各要素は、各単語がcenter wordである確率です。そのため、確率$p({\\bf v}_t \\mid V_{t \\pm C})$は、center wordのone-hotベクトル${\\bf v}_{t}$と確率ベクトル$\\hat{\\bf v}_{t}$の内積で計算することができます。\n",
        "\n",
        "\\begin{eqnarray}\n",
        "p({\\bf v}_{t} \\mid V_{t \\pm C}) = {\\bf v}_{t}^T \\hat{\\bf v}_{t}\n",
        "\\end{eqnarray}\n",
        "\n",
        "The loss function for the center word prediction is defined as follows:\n",
        "\n",
        "そして、context word$V_{t \\pm C}$対するcenter word ${\\bf v}_t$のロス関数は以下で計算することができます。\n",
        "\n",
        "\n",
        "\\begin{eqnarray}\n",
        "L({\\bf v}_t|V_{t \\pm C}; W_H, W_O)\n",
        "&=& -\\log(p({\\bf v}_t|V_{t \\pm C})) \\\\\n",
        "&=& -\\log({\\bf v}_t^T \\hat{\\bf v}_t)\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "YZKrh07hOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Skip-gramの詳細\n",
        "\n",
        "本チュートリアルでは、以下の観点からSkip-gramをメインで扱います。\n",
        "\n",
        "1. 学習アルゴリズムがCBoWに比べて理解しやすい\n",
        "2. 単語数が増えても精度が落ちにくく、スケールしやすい"
      ]
    },
    {
      "metadata": {
        "id": "QMspiJZzOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "skip-gramのアルゴリズムを理解するために、以下の設定で具体的な例から考えてみましょう:\n",
        "\n",
        "* ボキャブラリ数 $|\\mathcal{V}|$ は10。\n",
        "* 埋め込みベクトルのサイズ$D$は2。\n",
        "* Center wordはdog。\n",
        "* Context wordはanimal。\n",
        "\n",
        "そして、以下の工程をcontext word数回繰り返します。\n",
        "\n",
        "1. dogのone-hotベクトルは`[0 0 1 0 0 0 0 0 0 0]`で、 これをモデルに入力する。\n",
        "2. このとき、埋め込み行列${\\bf W}_H$の３番目の行${\\bf l}_H$がdogの埋め込みベクトルとなる。\n",
        "3. そして、出力ベクトル${\\bf l}_O$を計算するため、${\\bf W}_O$と${\\bf l}_H$の積を計算する。\n",
        "4. $c$の位置にあるcontext wordの確率ベクトル$\\hat{\\bf v}_{t+c}$を予測するため${\\bf l}_O$をsoftmax関数に入力する。\n",
        "5. $\\hat{\\bf v}_{t+c}$と animalのone-hotベクトル`[1 0 0 0 0 0 0 0 0 0 0]`の誤差を計算する。\n",
        "6. 誤差を伝播させてネットワークのパラメータを更新する。\n",
        "\n",
        "![skipgram_detail.png](https://docs.chainer.org/en/stable/_images/skipgram_detail.png)"
      ]
    },
    {
      "metadata": {
        "id": "IQ19qx2UOjkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Chainerによるskip-gram実装方法\n",
        "\n",
        "GitHubレポジトリ上のexamples内にword2vecに関するコードがあるので、それに基づいて説明をしていきます。[chainer/examples/word2vec](https://github.com/chainer/chainer/tree/master/examples/word2vec)"
      ]
    },
    {
      "metadata": {
        "id": "-fxZ_DM3O7Ea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "まずは、以下のセルを実行して、ChainerとそのGPUバックエンドであるCuPyをインストールします。Colaboratoryの「ランタイムのタイプ」がGPUであれば、GPUをバックエンドとしてChainerを動かすことができます。"
      ]
    },
    {
      "metadata": {
        "id": "9xOsXDFRO54W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "ae9cad40-8a15-4b24-917c-5d4b239c1b6a"
      },
      "cell_type": "code",
      "source": [
        "!curl https://colab.chainer.org/install | sh -"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcusparse8.0 is already the newest version (8.0.61-1).\n",
            "libnvrtc8.0 is already the newest version (8.0.61-1).\n",
            "libnvtoolsext1 is already the newest version (8.0.61-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G2O-8RkiOjkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 準備\n",
        "\n",
        "必要なパッケージを`import`しましょう。"
      ]
    },
    {
      "metadata": {
        "id": "OvL9Gk08OjkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import six\n",
        "\n",
        "import chainer\n",
        "from chainer import cuda\n",
        "import chainer.functions as F\n",
        "import chainer.initializers as I\n",
        "import chainer.links as L\n",
        "import chainer.optimizers as O\n",
        "from chainer import reporter\n",
        "from chainer import training\n",
        "from chainer.training import extensions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AXuiONJbOjkG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 skip-gramモデルの定義\n",
        "\n",
        "次にskip-gramのネットワーク構造を定義しましょう。"
      ]
    },
    {
      "metadata": {
        "id": "jLNEBp7gOjkG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SkipGram(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_vocab, n_units):\n",
        "        super().__init__()\n",
        "        with self.init_scope():\n",
        "            self.embed = L.EmbedID(\n",
        "                n_vocab, n_units, initialW=I.Uniform(1. / n_units))\n",
        "            self.out = L.Linear(n_units, n_vocab, initialW=0)\n",
        "\n",
        "    def __call__(self, x, context):\n",
        "        e = self.embed(context)\n",
        "        shape = e.shape\n",
        "        x = F.broadcast_to(x[:, None], (shape[0], shape[1]))\n",
        "        e = F.reshape(e, (shape[0] * shape[1], shape[2]))\n",
        "        x = F.reshape(x, (shape[0] * shape[1],))\n",
        "        center_predictions = self.out(e)\n",
        "        loss = F.softmax_cross_entropy(center_predictions, x)\n",
        "        reporter.report({'loss': loss}, self)\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xL8OWlpZOjkI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**\n",
        "\n",
        "- 重み行列`self.embed.W`は入力`x`に対する埋め込み行列です。\n",
        "- `__call__`は center wordの単語ID `x`とcontext wordの単語ID `contexts`を入力として取ります。そして、ロス関数`softmax_cross_entropy`で計算された誤差を出力します。\n",
        "- 注意してもらいたいのが、 `x`と`contexts`の形がそれぞれ`(batch_size,)`と`(batch_size, n_context)`になっていることです。\n",
        "- `batch_size`はミニバッチサイズを意味し、 `n_context`はcontext word数を意味します。\n",
        "\n",
        "まず、`e = self.embed(contexts)`で`contexts`に対応する分散表現を取得しています。\n",
        "\n",
        "そして、 `F.broadcast_to(x[:, None], (shape[0], shape[1]))`とすることで、`x`(`(batch_size,)`) を`(batch_size, n_context)`の形にブロードキャストします。このとき、 列方向に`n_context`回だけ同じ値がコピーされます。そして、ブロードキャストされた`x`は１次元ベクトルにreshapeされ、`(batchsize * n_context,)`になります。一方で、`e`は`(batch_size * n_context, n_units)`の形にreshapeされます。\n",
        "\n",
        "注意してもらいたいのが、skip-gramの場合、center wordとcontext wordは1対1で対応するため、center wordとcontext wordを入れ替えてモデル化しても問題がないです。そのため、上記ではcenter wordとcontext wordを入れ替えて学習させているように見えますが、問題はありません。なぜこのようなことをするかと言うと、CBoWモデルとコードの整合性が取りやすいからです。"
      ]
    },
    {
      "metadata": {
        "id": "dFAB4sObOjkJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.3 datasetとiteratorの準備\n",
        "\n",
        "Chainer'が用意するユーティリティ関数`get_ptb_words()`を使って、Penn Tree Bank (PTB)のデータセットをダウンロードしましょう。"
      ]
    },
    {
      "metadata": {
        "id": "pqUBJ1r2OjkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, val, _ = chainer.datasets.get_ptb_words()\n",
        "n_vocab = max(train) + 1  # The minimum word ID is 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yf2sqARoOjkL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "center wordと、そのcontext wordを含むミニバッチを生成するIteratorを定義しましょう。"
      ]
    },
    {
      "metadata": {
        "id": "jwYDyNyXOjkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WindowIterator(chainer.dataset.Iterator):\n",
        "\n",
        "    def __init__(self, dataset, window, batch_size, repeat=True):\n",
        "        self.dataset = np.array(dataset, np.int32)\n",
        "        self.window = window\n",
        "        self.batch_size = batch_size\n",
        "        self._repeat = repeat\n",
        "\n",
        "        self.order = np.random.permutation(\n",
        "            len(dataset) - window * 2).astype(np.int32)\n",
        "        self.order += window\n",
        "        self.current_position = 0\n",
        "        self.epoch = 0\n",
        "        self.is_new_epoch = False\n",
        "\n",
        "    def __next__(self):\n",
        "        if not self._repeat and self.epoch > 0:\n",
        "            raise StopIteration\n",
        "\n",
        "        i = self.current_position\n",
        "        i_end = i + self.batch_size\n",
        "        position = self.order[i: i_end]\n",
        "        w = np.random.randint(self.window - 1) + 1\n",
        "        offset = np.concatenate([np.arange(-w, 0), np.arange(1, w + 1)])\n",
        "        pos = position[:, None] + offset[None, :]\n",
        "        context = self.dataset.take(pos)\n",
        "        center = self.dataset.take(position)\n",
        "\n",
        "        if i_end >= len(self.order):\n",
        "            np.random.shuffle(self.order)\n",
        "            self.epoch += 1\n",
        "            self.is_new_epoch = True\n",
        "            self.current_position = 0\n",
        "        else:\n",
        "            self.is_new_epoch = False\n",
        "            self.current_position = i_end\n",
        "\n",
        "        return center, context\n",
        "\n",
        "    @property\n",
        "    def epoch_detail(self):\n",
        "        return self.epoch + float(self.current_position) / len(self.order)\n",
        "\n",
        "    def serialize(self, serializer):\n",
        "        self.current_position = serializer('current_position',\n",
        "                                           self.current_position)\n",
        "        self.epoch = serializer('epoch', self.epoch)\n",
        "        self.is_new_epoch = serializer('is_new_epoch', self.is_new_epoch)\n",
        "        if self._order is not None:\n",
        "            serializer('_order', self._order)\n",
        "\n",
        "def convert(batch, device):\n",
        "    center, context = batch\n",
        "    if device >= 0:\n",
        "        center = cuda.to_gpu(center)\n",
        "        context = cuda.to_gpu(context)\n",
        "    return center, context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Zq2jxKgOjkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- コンストラクタの中で、文書中の単語の位置をシャッフルしたリスト`self.order`を作成しています。文書からランダムに単語を選択し学習するようにするためです。ウィンドウサイズ分だけ最初と最後を切り取った単語の位置がシャッフルされて入っています。\n",
        "- イテレータの定義`__next__`は、コンストラクタのパラメータに従ってミニバッチサイズ個のcenter word `center`とcontext word `context`を返します。\n",
        "- `self.order[i:i_end]`で、単語の位置をシャッフルしたリスト`self.order`から`batch_size`分のcenter wordのインデックス`position`を生成します。(`position`は後で`self.dataset.take`によってcenter word `center`に変換されます。)\n",
        "- `np.concatenate([np.arange(-w, 0), np.arange(1, w + 1)])`で、ウインドウを表現するオフセット`offset`を作成しています。\n",
        "- `position[:, None] + offset[None, :]`によって、それぞれのcenter wordに対するcontext word のインデックス`pos`を生成します。`pos`は後で`self.dataset.take`によってcontext word  `context`に変換されます。"
      ]
    },
    {
      "metadata": {
        "id": "IlIoeqeNOjkO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.4 model, optimizer, updaterの準備"
      ]
    },
    {
      "metadata": {
        "id": "OgPecreHOjkP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unit = 100  # number of hidden units\n",
        "window = 5\n",
        "batchsize = 1000\n",
        "gpu = 0\n",
        "\n",
        "# Instantiate model\n",
        "model = SkipGram(n_vocab, unit)\n",
        "\n",
        "if gpu >= 0:\n",
        "    model.to_gpu(gpu)\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = O.Adam()\n",
        "optimizer.setup(model)\n",
        "\n",
        "# Create iterators for both train and val datasets\n",
        "train_iter = WindowIterator(train, window, batchsize)\n",
        "val_iter = WindowIterator(val, window, batchsize, repeat=False)\n",
        "\n",
        "# Create updater\n",
        "updater = training.StandardUpdater(\n",
        "    train_iter, optimizer, converter=convert, device=gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccVACTY-OjkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.5 trainingの開始"
      ]
    },
    {
      "metadata": {
        "id": "OVPlZWePOjkS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1700
        },
        "outputId": "feb79699-0038-4df4-8e99-f480df3caace"
      },
      "cell_type": "code",
      "source": [
        "epoch = 100\n",
        "\n",
        "trainer = training.Trainer(updater, (epoch, 'epoch'), out='word2vec_result')\n",
        "trainer.extend(extensions.Evaluator(val_iter, model, converter=convert, device=gpu))\n",
        "trainer.extend(extensions.LogReport())\n",
        "trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'validation/main/loss', 'elapsed_time']))\n",
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  elapsed_time\n",
            "\u001b[J1           6.87314     6.48688               54.154        \n",
            "\u001b[J2           6.44018     6.40645               107.352       \n",
            "\u001b[J3           6.35021     6.3558                159.544       \n",
            "\u001b[J4           6.28615     6.31679               212.612       \n",
            "\u001b[J5           6.23762     6.28779               266.059       \n",
            "\u001b[J6           6.19942     6.22658               319.874       \n",
            "\u001b[J7           6.15986     6.20715               372.798       \n",
            "\u001b[J8           6.13787     6.21461               426.456       \n",
            "\u001b[J9           6.10637     6.24927               479.725       \n",
            "\u001b[J10          6.08759     6.23192               532.966       \n",
            "\u001b[J11          6.06768     6.19332               586.339       \n",
            "\u001b[J12          6.04607     6.17291               639.295       \n",
            "\u001b[J13          6.0321      6.21226               692.67        \n",
            "\u001b[J14          6.02178     6.18489               746.599       \n",
            "\u001b[J15          6.00098     6.17341               799.408       \n",
            "\u001b[J16          5.99099     6.19581               852.966       \n",
            "\u001b[J17          5.97425     6.22275               905.819       \n",
            "\u001b[J18          5.95974     6.20495               958.404       \n",
            "\u001b[J19          5.96579     6.16532               1012.49       \n",
            "\u001b[J20          5.95292     6.21457               1066.24       \n",
            "\u001b[J21          5.93696     6.18441               1119.45       \n",
            "\u001b[J22          5.91804     6.20695               1171.98       \n",
            "\u001b[J23          5.93265     6.15757               1225.99       \n",
            "\u001b[J24          5.92238     6.17064               1279.85       \n",
            "\u001b[J25          5.9154      6.21545               1334.01       \n",
            "\u001b[J26          5.90538     6.1812                1387.68       \n",
            "\u001b[J27          5.8807      6.18523               1439.72       \n",
            "\u001b[J28          5.89009     6.19992               1492.67       \n",
            "\u001b[J29          5.8773      6.24146               1545.48       \n",
            "\u001b[J30          5.89217     6.21846               1599.79       \n",
            "\u001b[J31          5.88493     6.21654               1653.95       \n",
            "\u001b[J32          5.87784     6.18502               1707.45       \n",
            "\u001b[J33          5.88031     6.14161               1761.75       \n",
            "\u001b[J34          5.86278     6.22893               1815.29       \n",
            "\u001b[J35          5.83335     6.18966               1866.56       \n",
            "\u001b[J36          5.85978     6.24276               1920.18       \n",
            "\u001b[J37          5.85921     6.23888               1974.2        \n",
            "\u001b[J38          5.85195     6.19231               2027.92       \n",
            "\u001b[J39          5.8396      6.20542               2080.78       \n",
            "\u001b[J40          5.83745     6.27583               2133.37       \n",
            "\u001b[J41          5.85996     6.23596               2188          \n",
            "\u001b[J42          5.85743     6.17438               2242.4        \n",
            "\u001b[J43          5.84051     6.25449               2295.84       \n",
            "\u001b[J44          5.83023     6.30226               2348.84       \n",
            "\u001b[J45          5.84677     6.23473               2403.11       \n",
            "\u001b[J46          5.82406     6.27398               2456.11       \n",
            "\u001b[J47          5.82827     6.21509               2509.17       \n",
            "\u001b[J48          5.8253      6.23009               2562.15       \n",
            "\u001b[J49          5.83697     6.2564                2616.35       \n",
            "\u001b[J50          5.81998     6.29104               2669.38       \n",
            "\u001b[J51          5.82926     6.26068               2723.47       \n",
            "\u001b[J52          5.81457     6.30152               2776.36       \n",
            "\u001b[J53          5.82587     6.29581               2830.24       \n",
            "\u001b[J54          5.80614     6.30994               2882.85       \n",
            "\u001b[J55          5.8161      6.23224               2935.73       \n",
            "\u001b[J56          5.80867     6.26867               2988.48       \n",
            "\u001b[J57          5.79467     6.24508               3040.2        \n",
            "\u001b[J58          5.81687     6.24676               3093.57       \n",
            "\u001b[J59          5.82064     6.30236               3147.68       \n",
            "\u001b[J60          5.80855     6.30184               3200.75       \n",
            "\u001b[J61          5.81298     6.25173               3254.06       \n",
            "\u001b[J62          5.80753     6.32951               3307.42       \n",
            "\u001b[J63          5.82505     6.2472                3361.68       \n",
            "\u001b[J64          5.78396     6.28168               3413.14       \n",
            "\u001b[J65          5.80209     6.24962               3465.96       \n",
            "\u001b[J66          5.80107     6.326                 3518.83       \n",
            "\u001b[J67          5.83765     6.28848               3574.57       \n",
            "\u001b[J68          5.7864      6.3506                3626.88       \n",
            "\u001b[J69          5.80329     6.30671               3679.82       \n",
            "\u001b[J70          5.80032     6.29277               3732.69       \n",
            "\u001b[J71          5.80647     6.30722               3786.21       \n",
            "\u001b[J72          5.8176      6.30046               3840.51       \n",
            "\u001b[J73          5.79912     6.35945               3893.81       \n",
            "\u001b[J74          5.80484     6.32439               3947.35       \n",
            "\u001b[J75          5.82065     6.29674               4002.03       \n",
            "\u001b[J76          5.80872     6.27921               4056.05       \n",
            "\u001b[J77          5.80891     6.28952               4110.1        \n",
            "\u001b[J78          5.79121     6.35363               4163.39       \n",
            "\u001b[J79          5.79161     6.32894               4216.34       \n",
            "\u001b[J80          5.78601     6.3255                4268.95       \n",
            "\u001b[J81          5.79062     6.29608               4321.73       \n",
            "\u001b[J82          5.7959      6.37235               4375.25       \n",
            "\u001b[J83          5.77828     6.31001               4427.44       \n",
            "\u001b[J84          5.7879      6.25628               4480.09       \n",
            "\u001b[J85          5.79297     6.29321               4533.27       \n",
            "\u001b[J86          5.79286     6.2725                4586.44       \n",
            "\u001b[J87          5.79388     6.36764               4639.82       \n",
            "\u001b[J88          5.79062     6.33841               4692.89       \n",
            "\u001b[J89          5.7879      6.31828               4745.68       \n",
            "\u001b[J90          5.81015     6.33247               4800.19       \n",
            "\u001b[J91          5.78858     6.37569               4853.31       \n",
            "\u001b[J92          5.7966      6.35733               4907.27       \n",
            "\u001b[J93          5.79814     6.34506               4961.09       \n",
            "\u001b[J94          5.81956     6.322                 5016.65       \n",
            "\u001b[J95          5.81565     6.35974               5071.69       \n",
            "\u001b[J96          5.78953     6.37451               5125.02       \n",
            "\u001b[J97          5.7993      6.42065               5179.34       \n",
            "\u001b[J98          5.79129     6.37995               5232.89       \n",
            "\u001b[J99          5.76834     6.36254               5284.7        \n",
            "\u001b[J100         5.79829     6.3785                5338.93       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YqGB9iD2Tmng",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab = chainer.datasets.get_ptb_words_vocabulary()\n",
        "index2word = {wid: word for word, wid in six.iteritems(vocab)}\n",
        "\n",
        "# Save the word2vec model\n",
        "with open('word2vec.model', 'w') as f:\n",
        "    f.write('%d %d\\n' % (len(index2word), unit))\n",
        "    w = cuda.to_cpu(model.embed.W.data)\n",
        "    for i, wi in enumerate(w):\n",
        "        v = ' '.join(map(str, wi))\n",
        "        f.write('%s %s\\n' % (index2word[i], v))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HgBVYr_b8dS8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.6 似た単語の検索"
      ]
    },
    {
      "metadata": {
        "id": "7QDwFawQ8daT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import six\n",
        "\n",
        "n_result = 5  # number of search result to show\n",
        "\n",
        "\n",
        "with open('word2vec.model', 'r') as f:\n",
        "    ss = f.readline().split()\n",
        "    n_vocab, n_units = int(ss[0]), int(ss[1])\n",
        "    word2index = {}\n",
        "    index2word = {}\n",
        "    w = numpy.empty((n_vocab, n_units), dtype=numpy.float32)\n",
        "    for i, line in enumerate(f):\n",
        "        ss = line.split()\n",
        "        assert len(ss) == n_units + 1\n",
        "        word = ss[0]\n",
        "        word2index[word] = i\n",
        "        index2word[i] = word\n",
        "        w[i] = numpy.array([float(s) for s in ss[1:]], dtype=numpy.float32)\n",
        "\n",
        "\n",
        "s = numpy.sqrt((w * w).sum(1))\n",
        "w /= s.reshape((s.shape[0], 1))  # normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFel0uXmUfJl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def search(query):\n",
        "  if query not in word2index:\n",
        "    print('\"{0}\" is not found'.format(query))\n",
        "    return\n",
        "\n",
        "  v = w[word2index[query]]\n",
        "  similarity = w.dot(v)\n",
        "  print('query: {}'.format(query))\n",
        "\n",
        "  count = 0\n",
        "  for i in (-similarity).argsort():\n",
        "      if numpy.isnan(similarity[i]):\n",
        "          continue\n",
        "      if index2word[i] == query:\n",
        "          continue\n",
        "      print('{0}: {1}'.format(index2word[i], similarity[i]))\n",
        "      count += 1\n",
        "      if count == n_result:\n",
        "          return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3PrgDLi9pqf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "appleで検索してみましょう。"
      ]
    },
    {
      "metadata": {
        "id": "_JerH5KJ9NFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "84e097d5-80e8-4a5f-c790-5bbe104d7f2c"
      },
      "cell_type": "code",
      "source": [
        "query = \"apple\"\n",
        "search(query)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "query: apple\n",
            "computer: 0.5457335710525513\n",
            "compaq: 0.5068206191062927\n",
            "microsoft: 0.4654524028301239\n",
            "network: 0.42985647916793823\n",
            "trotter: 0.42716777324676514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JVXz7sbc8diq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Reference\n",
        "\n",
        "* [1] [Mikolov, Tomas; et al. “Efficient Estimation of Word Representations in Vector Space”. arXiv:1301.3781](https://arxiv.org/abs/1301.3781)\n",
        "* [2] [Distributional Hypothesis](https://aclweb.org/aclwiki/Distributional_Hypothesis)\n"
      ]
    },
    {
      "metadata": {
        "id": "HhBJdMTi8jxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
