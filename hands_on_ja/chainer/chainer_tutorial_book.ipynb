{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chainer-tutorial-book.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "IOi_pqQaRoAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainer Tutorial Bookへようこそ!\n",
        "\n",
        "Chainer Tutorial Book（CTB）では次のことをオンラインで学べます\n",
        "\n",
        "* ディープラーニングの基礎知識\n",
        "* Chainerを使ったディープラーニングの開発\n",
        "\n",
        "CTBではユーザーは実際にコードを書いて実行しながらディープラーニングを学べます。\n",
        "\n",
        "CTBは複数の章から構成されています。\n",
        "前から順番に読むことをお勧めしますが一部分だけ知りたい場合は先に読み進めても大丈夫です。\n",
        "\n",
        "それではさっそくはじめていきましょう。\n",
        "\n",
        "## ヒント\n",
        "\n",
        "数学の専門知識が必要な話題についてはセクション名の後に*がついています。\n",
        "このセクションは読み飛ばしても大丈夫ですが，読むことで理解が深まります。"
      ]
    },
    {
      "metadata": {
        "id": "vGnhKuraUYSv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "まずは下のブロックを実行してChainerとCupyをインストールしましょう。"
      ]
    },
    {
      "metadata": {
        "id": "HUEj1sckTD90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "0512da9f-86a1-41b0-dd24-86294aa7b00e"
      },
      "cell_type": "code",
      "source": [
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install -q cupy-cuda80 chainer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 28.9 MB of archives.\n",
            "After this operation, 71.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libcusparse8.0 amd64 8.0.61-1 [22.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvrtc8.0 amd64 8.0.61-1 [6,225 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvtoolsext1 amd64 8.0.61-1 [32.2 kB]\n",
            "Fetched 28.9 MB in 1s (16.3 MB/s)\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libcusparse8.0:amd64.\n",
            "(Reading database ... 18396 files and directories currently installed.)\n",
            "Preparing to unpack .../libcusparse8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libnvrtc8.0:amd64.\n",
            "Preparing to unpack .../libnvrtc8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../libnvtoolsext1_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KExGij1GRoQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainer Tutorial Bookについて\n",
        "\n",
        "はじめに，Chainer Tutorial Book (CTB）の仕組みについて簡単に紹介します。\n",
        "\n",
        "CTBは，Goggleが提供しているColaboratoryというウェブサービスを利用しています。\n",
        "\n",
        "このウェブサービスはJupyter Notebookをウェブサービス化したものであり、環境構築などをせず直ぐにNotebookを実行することができます。\n",
        "\n",
        "さっそく下のコードを実行してみましょう。\n",
        "下のブロックをクリックしたあと、Shift+Enterで実行することができます。\n",
        "プログラムは自由に編集可能です。プログラムを編集した後に再度実行すると編集後のプログラムを実行できます。"
      ]
    },
    {
      "metadata": {
        "id": "M3i5Xg1WUhZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "1c333828-001d-46c2-9755-45f54a658bf9"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import numpy as np\n",
        "\n",
        "print(\"Hello World!\")\n",
        "print(chainer.__version__)\n",
        "print(355.0 / 113.0)\n",
        "print(np.eye(5))\n",
        "\n",
        "total = 0\n",
        "for i in range(10):\n",
        "    total += i\n",
        "\n",
        "print(total)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello World!\n",
            "4.2.0\n",
            "3.1415929203539825\n",
            "[[1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1.]]\n",
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pFQVRBU6R6nX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 機械学習\n",
        "\n",
        "ニューラルネットワークを含む多くの機械学習における学習タスクは最適なパラメータを探す問題です。\n",
        "最適なパラメータは目的関数の最小化（最大化）問題を解くことで自動的に得られます。\n",
        "\n",
        "一般に何かを学習させたいという場合は次のステップからなります。\n",
        "\n",
        "1. 学習対象のモデルを定義する\n",
        "2. 目的関数を定義する\n",
        "3. 目的関数を最適化することで，モデルを学習する\n",
        "\n",
        "これらを順番にみていきましょう。"
      ]
    },
    {
      "metadata": {
        "id": "so_tGLp7R6qG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. 学習対象のモデルを定義する (1)\n",
        "\n",
        "はじめに学習対象のモデルを定義します。\n",
        "\n",
        "ここでは，学習対象のモデルはいくつかのパラメータを使った関数だとします。\n",
        "Pythonプログラムでいうと，学習対象のモデルはクラスのメソッドであり，パラメータはメンバ変数（インスタンス）のようなものです。\n",
        "\n",
        "例えば次のクラスFはパラメータ $a$ と $b$ を持ち，関数としては $ax + b$を表します。\n",
        "この関数の挙動はパラメータ $a$ と $b$ を変えることで変わります。\n",
        "\n",
        "```\n",
        "class F(object):\n",
        "    def __init__(self, a, b):\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.a * x + self.b\n",
        "\n",
        "f = F(2.0, -1.0)\n",
        "print(f(1.0)) # 1.0\n",
        "print(f(2.0)) # 3.0\n",
        "```\n",
        "\n",
        "\n",
        "同様に，学習対象のモデルも複数のパラメータを持ち，それらパラメータを調整することで望むような挙動をするようにさせます。\n",
        "\n",
        "## パラメトリックモデル(*)\n",
        "\n",
        "このようなパラメータで特徴づけられたモデルをパラメトリックモデルとよびます。\n",
        "例えば，パラメータ $\\theta$ で特徴付けられた関数は $y=f(x; \\theta)$ です。\n",
        "この関数の挙動がパラメータ $\\theta$ で変わることを示すために，引数とは違って $;\\theta$ と表します。\n",
        "\n",
        "## 課題\n",
        "\n",
        "上記例を $f(1) = 5$、$f(2) = 8$となるようにaとbを調整し，\n",
        "\n",
        "```\n",
        "f = F(2.0, -1.0)\n",
        "```\n",
        "\n",
        "を書き換えてください。\n",
        "（つまり，あなた自身でモデルを学習させてください）"
      ]
    },
    {
      "metadata": {
        "id": "cg4IDGlcVW1l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fca4c148-8cb0-443e-f457-327d8fd54a54"
      },
      "cell_type": "code",
      "source": [
        "class F(object):\n",
        "    def __init__(self, a, b):\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.a * x + self.b\n",
        "\n",
        "\n",
        "f = F(2.0, -1.0)\n",
        "print(f(1.0))  # 1.0\n",
        "print(f(2.0))  # 3.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OS1w693hR6sh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. 学習対象のモデルを定義する (2)\n",
        "\n",
        "Chainerでは学習可能なモデルをLinkとよびます。\n",
        "\n",
        "ディープラーニングで利用される代表的なLinkは `chainer.links` でサポートされています。\n",
        "また，自分で新しいLinkを作ることもできます。\n",
        "\n",
        "以降では，この `chainer.links` を `L` として使えるようにします。\n",
        "\n",
        "\n",
        "```\n",
        "from chainer import links as L\n",
        "```\n",
        "\n",
        "もっと基本的なLinkはLinearとよばれるLinkです。\n",
        "Linearは全ての入力と出力がつながっているようなニューラルネットワークを表します。\n",
        "Linearはニューラルネットワークの文脈では全結合層，数学の用語では線形変換，アフィン変換とよびます。\n",
        "たとえば，次の例では5個のユニットから，2個のユニットへの変換を表します。\n",
        "\n",
        "```\n",
        "lin = L.Linear(5, 2)\n",
        "```\n",
        "\n",
        "この `lin` はLinkオブジェクトですが，次のように関数呼び出しをすることができます。\n",
        "（この関数呼び出しは `__call__` で定義されており，演算子オーバーロードで実現されています。）\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "from chainer import Variable\n",
        "\n",
        "lin = L.Linear(5, 2)\n",
        "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
        "y1 = lin(x)\n",
        "```\n",
        "\n",
        "この `numpy` ， `Variable` については後で詳しく説明します。\n",
        "この例である`np.ones((3, 5), dtype=np.float32)` は3行5列で全ての値が1であるような行列を作ります。\n",
        "`Variable` はその値に加えて学習に必要な情報が埋め込まれているオブジェクトとだけ覚えてください。\n",
        "つまりここでは3個の5次元のベクトルを用意し，それをVariableというオブジェクトにセットし，\n",
        "それを `lin` の引数として与えて，出力を `y` として計算しています。\n",
        "`lin`は5次元の入力を2次元の出力へ変換する関数なので，`y`は3個の2次元のベクトルになります。\n",
        "\n",
        "\n",
        "## 線形変換，アフィン変換 (*)\n",
        "\n",
        "線形変換（アフィン変換）は次のように表される変換です。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "f(x; θ) &= Wx + b \\\\\n",
        "θ &= (W, b)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "例えば上記例のLinearは，5次元のベクトルから2次元のベクトルへの線形変換を表します。\n",
        "\n",
        "# 課題\n",
        "\n",
        "上記の例で，出力を4次元にし，それを出力してください。\n"
      ]
    },
    {
      "metadata": {
        "id": "l1D7OJNnWqrV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "15394d0c-c138-424d-acad-93c139ca008e"
      },
      "cell_type": "code",
      "source": [
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "lin = L.Linear(5, 2)\n",
        "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
        "y1 = lin(x)\n",
        "\n",
        "print(x.data)\n",
        "print(y1.data)\n",
        "\n",
        "y2 = F.relu(x)\n",
        "y3 = F.relu(lin(x))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]\n",
            " [1. 1. 1. 1. 1.]]\n",
            "[[-0.12078336 -2.4637902 ]\n",
            " [-0.12078336 -2.4637902 ]\n",
            " [-0.12078336 -2.4637902 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3fl67pPKR6u9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1. 学習対象のモデルを定義する (3)\n",
        "\n",
        "Chainerでもう一つ重要なオブジェクトとしてFunctionがあります。\n",
        "FuncitonはLinkとは違って，学習可能なパラメータを持ちません。\n",
        "つまり，学習によって挙動を変えません。\n",
        "\n",
        "ディープラーニングで利用されている代表的な関数は `chainer.functions` で定義されています。\n",
        "また，自分で新しいFunctionを作ることもできます。\n",
        "\n",
        "以降では，この  `chainer.functions` をFとして使えるようにします。\n",
        "\n",
        "```\n",
        "from chainer import functions as F\n",
        "```\n",
        "\n",
        "例えば，ディープラーニングでよく使われるReLUとよばれる非線形関数 $f_{relu}$ は\n",
        "\n",
        "$$f_{relu}(x)=max(x,0)$$\n",
        "\n",
        "で定義されます。\n",
        "つまり，もし $x$ が $0$ よりも大きければ $x$ をそのまま返し，もし小さければ $0$ を返すような関数です。\n",
        "\n",
        "Chainerでは次のように記述できます。\n",
        "\n",
        "```\n",
        "from chainer import functions as F\n",
        "...\n",
        "y2 = F.relu(x)\n",
        "```\n",
        "\n",
        "これらのLinkとFunctionを組みわせて複雑な関数を作ることができます。\n",
        "例えば，前回の例のLinearを適用した後にReLUを適用した結果は次のように計算されます。\n",
        "\n",
        "```\n",
        "y3 = F.relu(lin(x))\n",
        "```\n",
        "\n",
        "## 課題\n",
        "\n",
        "ReLUと並んで重要な非線形関数として，sigmoidがあります。\n",
        "例をsigmoidに変えて，その結果を表示してください。\n",
        "\n",
        "[関数一覧](http://docs.chainer.org/en/stable/reference/functions.html)"
      ]
    },
    {
      "metadata": {
        "id": "gvoWi07DRX9J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1e03babc-cf9e-40de-e3ba-d84e1460e035"
      },
      "cell_type": "code",
      "source": [
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "lin = L.Linear(5, 2)\n",
        "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
        "y1 = lin(x)\n",
        "\n",
        "y2 = F.relu(lin(x))\n",
        "print(y2.data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q6Y7cBh7XDXV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. 学習対象のモデルを定義する (4)\n",
        "\n",
        "これまで扱ったLinkとFunctionを組み合わせて，学習対象のモデルを実際に作ってみましょう。\n",
        "\n",
        "以下に三層からなるニューラルネットワークの例をあげます。\n",
        "\n",
        "```\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            # the size of the inputs to each layer will be inferred\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "```\n",
        "\n",
        "各機能は今後詳細に説明されますので，ここでは概要だけ説明します。\n",
        "詳細は理解できなくてもそのまま飛ばして問題ありません。\n",
        "\n",
        "このMLPは，三つのLinear（l1, l2, l3）を学習可能なパラメータとして持ち，`__call__`でそれらのパラメータを利用して結果を計算します。\n",
        "\n",
        "なお，`L.Linear` の第一引数には `None` を指定することで実際の入力からユニット数を自動で設定してくれます。\n",
        "\n",
        "`__call__` では先ほど定義した層に入力`x`を与えて計算（順計算）を行います。\n",
        "まず `l1` に大元の入力 `x` を与え，それをLinearで変換したものにReLUを適用します。\n",
        "その計算結果 `h1` を次の層 `l2` に与え同様の計算を行います。\n",
        "`l3` に関しても同様に前層の結果を元に計算を行います。\n",
        "最終的に `l3` の結果を返すことで計算が完了します。\n",
        "\n",
        "このように，(1)Linkを使って学習対象のパラメータを定義し，(2)次にそれらを使って順計算を定義することで学習対象のモデルを定義できます。"
      ]
    },
    {
      "metadata": {
        "id": "Uu6iO9nxXMA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "699cf561-e3cd-46e8-f2bd-da7ddd3ac672"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            # the size of the inputs to each layer will be inferred\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "model = MLP(5, 2)\n",
        "\n",
        "x = Variable(np.ones((3, 5), dtype=np.float32))\n",
        "y = model(x)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0nJzZmZXFWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. 目的関数を定義する (1)\n",
        "\n",
        "次に目的関数を定義します。\n",
        "\n",
        "目的関数は何を学習させたいのかを表す関数です。\n",
        "目的関数は一つの値を出力し，値が小さければ望ましい状態を表すような関数です。\n",
        "\n",
        "例えば，訓練データに対し学習対象モデルが予測をし，間違えた回数を $L$ とします。\n",
        "\n",
        "この間違えた回数 $L$ を小さくするということは，学習対象モデルが訓練データをたくさん当てられるようにすることを意味します。\n",
        "\n",
        "この場合，目的関数は学習対象モデルを引数として受取り，間違えた数を返すような関数です。\n",
        "\n",
        "次から，代表的な学習問題である分類教師あり学習の目的関数を例に考えていきます。"
      ]
    },
    {
      "metadata": {
        "id": "Kfc6RwhlXq3P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. 目的関数を定義する (2)\n",
        "\n",
        "分類とは入力 $x$ から出力 $y \\in \\{ 1, ..., k \\}$ を推定する問題です。出力はラベルと呼ばれます。\n",
        "例えば，画像からそこに写っている動物を推定したい場合では，入力 $x$ が画像であり，出力 $y$ がそのラベル（猫，犬など）となります。"
      ]
    },
    {
      "metadata": {
        "id": "XZPIfXTaXsxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. 目的関数を定義する (3)\n",
        "\n",
        "今回，学習したいモデルは入力 $x$ が与えられた時に出力 $y$ の条件付き確率 $p(y \\mid x)$ を出力してくれるようなモデルです。\n",
        "\n",
        "例えば，入力$x$に犬が写っていれば $p(犬 \\mid x) = 0.99, p(猫 \\mid x) = 0.01$ となるようなモデルです。\n",
        "\n",
        "このようなカテゴリ値（離散値）に対する確率分布をモデル化するにはSoftmaxを利用します。\n",
        "\n",
        "Chainerでは `chainer.functions` で `softmax` 関数が定義されているのでそれを使いましょう。\n",
        "\n",
        "例えば，入力`x`を何らかのモデルで変換しカテゴリ種類数と同じ次元数を持つベクトル`t`を作ります。\n",
        "次に（必ずしも確率分布となっていない）ベクトル`t`を`softmax`を使って確率分布に変換します。\n",
        "\n",
        "```\n",
        "t = model(x)\n",
        "y = F.softmax(t)\n",
        "```\n",
        "\n",
        "## Softmax　(*)\n",
        "\n",
        "Softmax（または多クラスロジスティックスモデル）とは $d$ 次元の実数値ベクトルから，\n",
        "$d$ 次元の確率分布を作る方法の一つです。\n",
        "\n",
        "softmaxは，$x$ が $d$ 次元であり，各次元の値が $x[0], x[1], ..., x[d-1]$ の時，\n",
        "\n",
        "$$y[i]=\\frac{\\exp(x[i])}{\\sum_j \\exp(x[j])}$$\n",
        "\n",
        "と表されます。\n",
        "\n",
        "あるベクトルvが確率分布となる条件として，\n",
        "\n",
        "1. 各次元の値が非負\n",
        "2. 合計値が1\n",
        "\n",
        "という条件があります。\n",
        "Softmaxは， $\\exp$ が非負であることから1の条件を満たし，各次元の値を足した値で割っていることから2の条件を満たします。"
      ]
    },
    {
      "metadata": {
        "id": "aKaC03tcX-Me",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. 目的関数を定義する (4)\n",
        "\n",
        "学習を行うために，入力と正解の出力のペアからなる $n$ 個の学習データ\n",
        "\n",
        "$$D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$$\n",
        "\n",
        "を用意します。\n",
        "\n",
        "この学習データと，学習対象のモデルが一致するように，つまり学習データそれぞれ $(x_i, y_i)$ に対し， $p(y_i \\mid x_i)$ が大きい時に，値が小さくなるような目的関数を用意します。\n",
        "\n",
        "ここでは代表的な関数である`softmax_cross_entropy`とよばれる関数を使います。\n",
        "\n",
        "```\n",
        "# x は入力, tは正解の出力\n",
        "h = MLP(x)\n",
        "loss = F.softmax_cross_entropy(h, t)\n",
        "```\n",
        "\n",
        "ChainerではSoftmaxを適用した後に，クロスエントロピー損失関数を適用した目的関数が用意されています。\n",
        "これは，二つまとめて処理をしないと，数値誤差の問題があるためです。\n",
        "\n",
        "\n",
        "## クロスエントロピー損失関数 (*)\n",
        "\n",
        "学習の目標は学習データ$D$の確率分布 $p(y|x)$ と，学習対象のモデルによる確率分布 $q(y \\mid x; \\theta)$ が一致するようにすることです。\n",
        "確率分布間がどれだけ離れているかを表す指標としてKLダイバージェンスが知られています。\n",
        "KLダイバージェンスは二つの確率分布 $P$ と $Q$ の遠さを次のように定義します。\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "KL(P \\mid\\mid Q) &= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} \\\\\n",
        "                 &= \\sum_x P(x) \\log P(x) - \\sum_x P(x) \\log Q(x)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "もし， $P$ と $Q$ が同じならば，全ての $x$ について $\\frac{P(x)}{Q(x)}=1$ となるので $KL(P \\mid\\mid Q)=0$ となります。\n",
        "この二つの分布が違うと， $KL(P \\mid\\mid Q)>0$ となり，近ければ近いほど0に近づくような指標です。\n",
        "\n",
        "学習データによって定義される確率分布は訓練分布と呼ばれ，それに基づく条件付き確率は\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "P(y \\mid x) &= \\frac{P(x, y)}{P(x)} \\\\\n",
        "            &= \\frac{\\sum_{i}I(x=x_i, y=y_i)}{\\sum_{i}I(x=x_i)}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "と表されます。\n",
        "但し， $I$ はデルタ関数とよばれ， $I(c)$ は $c$ が真である時は $1$，それ以外は $0$ であるような関数です。\n",
        "\n",
        "\n",
        "訓練分布に基づく条件づき確率 $P(y \\mid x)$ と，モデルによる条件づき確率 $Q(y \\mid x)$ のKLダイバージェンスは\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "KL(P \\mid\\mid Q) &= \\sum_{x} \\sum_y P(y \\mid x) \\log \\frac{P(y \\mid x)}{Q(y \\mid x)} \\\\\n",
        "                 &= \\sum_{x, y} P(y \\mid x) \\log P(y \\mid x) - \\sum_{x, y} P(y \\mid x) \\log Q(y \\mid x)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "となります。\n",
        "この最適化において， $Q$ に依存する項は第二項のみであり， $(x, y) \\in D$ の時 $P(y \\mid x)=1$ ，それ以外 $0$ ですので\n",
        "\n",
        "$L(\\theta) = - \\sum_{i=1}^n \\log Q(y_i \\mid x_i)$\n",
        "\n",
        "となります。\n",
        "これをクロスエントロピー損失関数，または負の対数尤度ともよばれます。\n",
        "\n",
        "ここまで読んだ方で，なぜ学習データ $D$ から得られた確率分布 $P$ そのものを直接使わず，学習モデルによる確率 $Q$ を使うのかと思った方がいるかもしれません。\n",
        "それは，学習の目標は学習データだけをうまく分類することではなく未知のデータをうまく分類することだからです。\n",
        "\n",
        "$P$ は，入力が学習データと全く同じであればそれが正解となりますが，そうでない場合は確率分布が不定になりえます。\n",
        "$Q$ はありえそうなモデルの中で一番 $P$ に近い分布を探しています。\n",
        "$Q$ は $P$ とは違って全ての $x$ について確率分布を与えることができるため，学習データには含まれない未知のデータでもうまく分類できます。\n",
        "別の言い方をすると$P$を平滑化した確率分布が$Q$となります。"
      ]
    },
    {
      "metadata": {
        "id": "6XbpE7BvX-SZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. 目的関数を定義する (5)\n",
        "\n",
        "また，推定する値が連続値である回帰問題の場合は次の二乗誤差を使います\n",
        "\n",
        "$$\\|y - f(x)\\|^2$$\n",
        "\n",
        "これは，Chainerでは `mean_squared_error` として用意されています。\n",
        "\n",
        "```\n",
        "loss = F.mean_squared_error(h, t)\n",
        "```\n",
        "\n",
        "損失関数のいくつかは `chainer.functions` 内で定義されています。\n",
        "また，自分で新しい損失関数を定義することもできます。\n",
        "\n",
        "得られた目的関数 $L(\\theta)$ は学習対象のモデルのパラメータ $\\theta$\n",
        "によって値が決まる関数であることに注意してください。\n",
        "この目的関数は，学習データをうまく分類できるような確率分布に対応するパラメータであれば小さい値をとり，そうでない場合は大きな値をとるような関数です。\n",
        "\n",
        "これにより，学習という問題を目的関数を最小化する最適化問題に変換することができました。\n",
        "\n",
        "この目的関数をどのように小さくするかは次章で扱います。\n",
        "\n",
        "## 課題 (*)\n",
        "\n",
        "なぜ，確率分布の場合，二乗誤差ではなくクロスエントロピー損失関数を使うのか考えてみてください"
      ]
    },
    {
      "metadata": {
        "id": "E8V7pb--YXOu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. 目的関数を最適化することで，モデルを学習する (1)\n",
        "\n",
        "ここまで学習対象のモデルを定義し，目的関数を定義してきました。\n",
        "\n",
        "この目的関数を使って，最適なパラメータを探す作業が最適化です。\n",
        "\n",
        "最適化問題は，パターゴルフのような問題とみなすことができます。\n",
        "現在の変数 $x$ が位置に対応し，関数の値 $F(x)$ が高さに対応します．\n",
        "最適化の目標は最も低い位置を探すことです。\n",
        "\n",
        "\n",
        "## 最適化問題 (*)\n",
        "\n",
        "一般に関数 $f(x)$ について， $f(x)$ を最小化するような $x$ 求める問題を最適化問題とよびます。\n",
        "\n",
        "例えば，1次元変数$x$について，\n",
        "\n",
        "$$f(x)=x^2 + 4x - 7$$\n",
        "\n",
        "という目的関数は\n",
        "\n",
        "$$f(x)=(x+2)^2 - 11$$\n",
        "\n",
        "であることから $x=-2$ が最小値を達成する変数であり，その時の最小値は $(-2+2)^2 - 11 = -11$ となります。\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "quZm7VUQYXSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. 目的関数を最適化することで，モデルを学習する (2)\n",
        "\n",
        "この最適化問題を解くために，次のような戦略をとります。\n",
        "\n",
        "1. 現在の位置 $x_t$ から目的関数の値が最も急激に下がりそうな方向を調べる\n",
        "\n",
        "    * この方向を勾配と呼び，$-v_t$ と書きます。\n",
        "\n",
        "2. その勾配にしたがって現在の位置から少し動かす: $x_{t+1} = x_t - \\alpha_t v_t$\n",
        "\n",
        "    * この時のステップ幅 $\\alpha_t > 0$ を更新律と呼びます。\n",
        "\n",
        "3. 1. 2.を関数の値が変わらなくなるまで繰り返す\n",
        "\n",
        "このような最適化手法を勾配降下法と呼びます。\n",
        "\n",
        "勾配降下法において，最も大変なのが勾配 $v_t$ の推定です。\n",
        "実際のパターゴルフのような三次元の世界では，一番急激に下っている方向を探すのは簡単です。\n",
        "一方，ディープラーニングの場合は一番急激に下がっている方向を探すために，誤差逆伝播法（back propagation）を利用し推定します。\n",
        "誤差逆伝搬法の計算量は順計算の計算量とほぼ同じであり効率的に勾配を求めることができます。\n",
        "\n",
        "\n",
        "## 誤差逆伝播法(*)\n",
        "\n",
        "出力から入力に向かって，目的関数の出力についての勾配を伝播させていくことで効率的に勾配を求めることができます。"
      ]
    },
    {
      "metadata": {
        "id": "Vn-O5B02YXU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. 目的関数を最適化することで，モデルを学習する (3)\n",
        "\n",
        "Chainerは誤差逆伝播法を使った勾配計算を標準でサポートしており，任意の順計算について誤差逆伝搬法を使って勾配を求めることができるようになっています。\n",
        "`chainer.Variable` はこの誤差逆伝搬法を実現するために必要な情報を記録する仕掛けが入っています。\n",
        "\n",
        "ユーザーが `Variable` を変数として順計算の計算手順を書いている時，Chainerは後で誤差逆伝播法ができるように内部で計算グラフを構築しています。\n",
        "\n",
        "例えば，前回の `loss` を目的関数とした場合，途中のパラメータ，入力についての勾配は， `backward` という関数を呼び出すことで求めることができます。\n",
        "\n",
        "```\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "勾配情報はこの `loss` の計算に関わった全ての `Variable`, `Link` の `grad` 属性に格納されます。"
      ]
    },
    {
      "metadata": {
        "id": "ajGfouzDYp-b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. 目的関数を最適化することで，モデルを学習する (4)\n",
        "\n",
        "勾配情報に基づきパラメータを更新する手法が `chainer.optimizers` にサポートされています。\n",
        "\n",
        "```\n",
        "from chainer import optimizers\n",
        "```\n",
        "\n",
        "代表的な最適化手法はSGD, RMSprop, Adamなどです。\n",
        "\n",
        "```\n",
        "opt = optimizers.Adam()\n",
        "opt.setup(model)\n",
        "```\n",
        "\n",
        "最適化エンジンがどの学習可能な関数を目標とするかは `setup` で設定します。\n",
        "そして，勾配を求めて，その勾配情報を元に最適化します。\n",
        "\n",
        "```\n",
        "loss.backward()\n",
        "opt.update()\n",
        "```\n",
        "\n",
        "誤差逆伝搬法は強力で多くの関数の勾配を正確にかつ高速に求めることができます。\n",
        "そのため，非常に多くのパラメータを持つモデルの場合でも効率的に学習することができます\n"
      ]
    },
    {
      "metadata": {
        "id": "Q7Q8LRqBYqBX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. 目的関数を最適化することで，モデルを学習する (5)\n",
        "\n",
        "誤差逆伝播法で勾配が求められるので，これを使って目的関数の最適化ができますが，問題が一つあります。\n",
        "目的関数 $L(\\theta)$ の勾配 $v(\\theta)$ は，目的関数の定義からデータ全てを調べないと求められません。\n",
        "しかし，毎回勾配を求めるたびにデータを全て調べるのは計算コストが大きすぎます。\n",
        "そのため，データ全体を使わずにデータの一部だけを利用し勾配の推定値を求め，\n",
        "それを利用しパラメータを更新します。これを確率的勾配降下法（SGD）とよびます。\n",
        "\n",
        "パターゴルフで例えるなら，大体こっちの方向が下ってそうだとわかったら，方向を細かく決めずにさっさと打ってしまう方法です。一回打つまでの時間が少なくてすむため単位時間あたりにより多くの回数打つことができ，真面目に方向を定めるより効率よく下ることができます。\n",
        "\n",
        "この勾配推定に使うためにサンプリングされた学習データをミニバッチとよび，その個数をミニバッチサイズBとします。\n",
        "ミニバッチサイズは例えば32〜1024程度の値が利用されます。\n",
        "\n",
        "# 確率的勾配降下法の利点 (*)\n",
        "\n",
        "勾配降下法には次の二つの問題があります。\n",
        "一つ目は最適解ではなく局所解に収束してしまう，つまり本当に一番小さい値ではないがその周辺からみると小さい値に収束してしまう問題です。パターゴルフでいえば途中でくぼみがありそこにはまってしまうことに相当します。\n",
        "二つ目はプラトーと呼ばれる平坦な領域にはまってしまう問題です。例えば平面のような場所では勾配が非常に小さく本当は平面の先にもっと良い解があったとしても，止まってしまう問題があります。\n",
        "\n",
        "SGDは勾配の推定値を使って更新していくため，常に変数は推定によるノイズの影響である程度ランダムに動き続けます。\n",
        "このランダムに動くことによって，局所解やプラトーから抜け出すことができます。"
      ]
    },
    {
      "metadata": {
        "id": "O_cWhwslYqDr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 機械学習のまとめ\n",
        "\n",
        "これまで機械学習の手順についてみてきました。ここでおさらいしましょう。\n",
        "\n",
        "1. 学習対象のモデルを定義する\n",
        "\n",
        "    学習可能なパラメータを持つ関数をLinkとよび，関数をFunctionとよびます。\n",
        "    学習対象のモデルはLinkとFunctionを組みわせて定義します。\n",
        "\n",
        "2. 目的関数を定義する\n",
        "\n",
        "    学習する目標を目的関数で表します。\n",
        "    目的関数を最小化することで学習が実現されるように目的関数を定義します。\n",
        "    目的関数は学習対象のモデルとそれを評価する関数によって定義できます。\n",
        "    代表的な評価関数として，分類に対する `softmax_cross_entropy`，\n",
        "    回帰に対する `mean_squared_error` があります。\n",
        "\n",
        "3. 目的関数を最適化することで，モデルを学習する\n",
        "\n",
        "    勾配降下法を利用し，目的関数を最小化するようなパラメータを求め，結果として学習を実現します。\n",
        "    この勾配は誤差逆伝播法を利用し効率よく求められます。\n",
        "    誤差逆伝播法は得られた損失の値に対し `backward()` を呼びだすことで求められます。\n",
        "    求まった勾配を使って各パラメータを更新するためには，optimizersを使います。\n",
        "\n",
        "次から具体的な学習問題を通じてもう一度詳しく見ていきますが，\n",
        "その前にChainerで利用しているNumPyについて少し説明します。"
      ]
    },
    {
      "metadata": {
        "id": "7s_KwqyVZECt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NumPy, Cupy, テンソル\n",
        "\n",
        "Chainerの中で主要なオブジェクトはNumPyで定義される `ndarray` よばれる多次元配列です。\n",
        "これは数学的にはテンソルともよばれます。\n",
        "\n",
        "NumPyの使い方については素晴らしい資料やチュートリアルが存在します。例えば\n",
        "[チュートリアル](http://naoyat.hatenablog.jp/entry/2011/12/29/021414)\n",
        "を参照してください。\n",
        "\n",
        "ここでは，Chainerを扱う上で最低限必要なNumPyの `ndarray` についての知識について説明します。\n",
        "\n",
        "NumPyは慣習としてnpとして利用します。\n",
        "\n",
        "```\n",
        "import numpy as np\n",
        "```\n",
        "\n",
        "NumPyの主要なオブジェクトは `ndarray` であり，多次元配列を表します。\n",
        "`ndarray` は次元または軸（axis）を持ち，軸の数をrankとよびます。\n",
        "\n",
        "多次元配列はそれぞれ `rank==0` の時はスカラー， `rank==1` の時はベクトル， `rank==2` の時は行列， `rank>=3` の時はテンソルとよばれます。\n",
        "`ndarray` の寸法（shape）は各軸の配列長を表す整数からなるタプル（例 (3,) (2, 3, 4)）で表され， `shape` 属性として取得できます。\n",
        "`ndarray` の値は全て同じ型を持ち， `dtype` 属性で参照できます。\n",
        "ディープラーニングで扱う場合， `dtype` は殆どの場合 `np.float32` , `np.int32` です。\n",
        "\n",
        "それでは長さ3のベクトルvと2行4列からなる行列mを作ってみましょう。\n",
        "最初の引数がshapeを指定し，二つ目の引数が型を指定します。\n",
        "\n",
        "```\n",
        "v = np.zeros((3,), np.float32)\n",
        "m = np.zeros((2, 4), np.float32)\n",
        "\n",
        "print(m.shape)\n",
        "print(m.dtype)\n",
        "```\n",
        "\n",
        "格納されている値は添字を使って参照したり，また添字を使って代入することができます。\n",
        "\n",
        "```\n",
        "print(m[0, 2])\n",
        "m[0, 2] = 7\n",
        "print(m[0, 2])\n",
        "```\n",
        "\n",
        "例えば，スカラーは0次元配列，ベクトルは1次元配列，行列は2次元配列です。\n",
        "\n",
        "ChainerではNumPyと同じコードでGPU上での演算を実現するcupyとよばれるライブラリを使います。\n",
        "それによりCPUかGPUかを区別せずに同じコードを書くことができます。\n",
        "\n",
        "```\n",
        "is_gpu = True # CPUの場合はFalse\n",
        "\n",
        "xp = cupy if is_gpu else numpy\n",
        "\n",
        "m1 = xp.zeros((3, 4))\n",
        "```\n",
        "\n",
        "## 課題\n",
        "\n",
        "shapeが `(10, 5, 4)` であり， `x[i, i, i] = 1` ，つまり1軸目と2軸目と3軸目の添字が一致する場合のみ1になり，それ以外は全て0になるような `ndarray` を作り，それを表示してください。"
      ]
    },
    {
      "metadata": {
        "id": "JQ3LW2_TZOAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "6a1419e4-9647-4de1-ade3-412fcfa09843"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.empty((3, 640, 480), dtype=np.int32)\n",
        "\n",
        "print(x)  # xをランダムに初期化した時の値\n",
        "\n",
        "print(x.shape)  # xの寸法\n",
        "\n",
        "print(x.ndim)  # xの軸数\n",
        "\n",
        "print(x.size)  # xの全要素数\n",
        "\n",
        "print(x.dtype)  # xの型\n",
        "\n",
        "# x[i, i, i]=1, それ以外は0となるようなx"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n",
            "(3, 640, 480)\n",
            "3\n",
            "921600\n",
            "int32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V6ubHBMAZEQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ディープラーニングによる手書き文字認識\n",
        "\n",
        "ここまでディープラーニングとChainerの使い方の基礎を学んできました。\n",
        "ここからは，MNISTとよばれる手書き文字データセットを使って，多層パーセプトロンによる多クラス分類器の学習をしてみましょう。\n",
        "MNISTデータセットは70000枚の28*28のグレイスケール画像から構成されており，それぞれに0〜9の数字がかかれています。\n",
        "このデータセットを60000の学習データと，10000のテストデータに分けて使います。\n",
        "\n",
        "なお，以降では各画像を28*28の画素を並べた784のグレイスケール値がならんた784次元のベクトルとして扱うようにします。\n",
        "\n",
        "MNISTデータセットのダウンロードは次の `dataset.get_mnist()` を呼び出すことで実行されます。\n",
        "\n",
        "```\n",
        "from chainer import dataset\n",
        "train, test = dataset.get_mnist()\n",
        "```\n",
        "\n",
        "これらのデータは `chainer.TupleDataset` で構成されており，各サンプルが画像とそのラベル（0〜9）のタプルから構成されています。\n",
        "例えば，train[100]は100番目のデータの画像とラベルからなるタプルを返します\n",
        "\n",
        "```\n",
        "x, y = train[100]\n",
        "print x\n",
        "print y\n",
        "```\n",
        "\n",
        "xは784次元のベクトル（浮動小数点値），yがラベル（整数値）です。\n",
        "なお， `print x` はChainer Playgroundでは大きすぎてそのままでは表示できませんし，\n",
        "単純な数値として表示されるだけなのでよく分かりません。\n",
        "\n",
        "そのためChainer Playgroundでは専用の補助関数 `print_mnist` を用意しています。\n",
        "それを利用することでMNISTデータセットの画像を表示できます。\n",
        "`print_mnist` を使用するためにはまず `playground` をインポートします。\n",
        "\n",
        "```\n",
        "import playground\n",
        "```\n",
        "\n",
        "その後，\n",
        "\n",
        "```\n",
        "x, y = train[100]\n",
        "playground.print_mnist(x)\n",
        "print y\n",
        "```\n",
        "\n",
        "とすることで100番目のデータを画像として表示します。\n",
        "\n",
        "## 課題\n",
        "\n",
        "trainの各ラベル毎の画像の平均ベクトルを求め，それらを順に表示せよ。\n",
        "\n",
        "## 補足： `dataset.get_mnist()` によるデータセットの取得\n",
        "\n",
        "`dataset.get_mnist()` は一度目の呼び出し時は実際にデータセットをダウンロードするため遅いですが，\n",
        "二回目以降は既にダウンロードされているキャッシュを利用して実行されますので速く処理されます。手元で `dataset.get_mnist()` を実行する際，ダウンロード先は環境変数\n",
        "`CHAINER_DATASET_ROOT` で指定することができます。\n",
        "デフォルトは `~/.chainer/dataset` です。\n",
        "\n",
        "また，Chainer Playground内では事前にデータセットをダウンロードしキャッシュ済みの状態になっています。\n",
        "そのためChainer Playgroundでは何度 `dataset.get_mnist()` を実行してもデータセットを公開しているサーバに負担がかかることはありません。\n",
        "\n",
        "## 補足：他のデータセット\n",
        "\n",
        "MNISTは，様々な研究開発のベースラインとしてよく使用されているデータセットであり，新しい手法の性能評価，デバッグのためにも使われます。\n",
        "最近は，より現実世界の問題に近い複雑なデータセットであるCIFAR-10，CIFAR-100やSVHN（Street View House Numbers)データセットが使われる場合も多くなっています。\n",
        "http://ufldl.stanford.edu/housenumbers/"
      ]
    },
    {
      "metadata": {
        "id": "zgUBTOkwXg-V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "79949cfb-7e5a-4b29-eaee-10b58aaa6f45"
      },
      "cell_type": "code",
      "source": [
        "from chainer import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "train, test = datasets.get_mnist()\n",
        "x, y = train[99]\n",
        "\n",
        "plt.imshow(x.reshape(28,28),cmap='gray_r')\n",
        "plt.show()\n",
        "print(y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading from http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADM1JREFUeJzt3V+oXeWZx/HvMSJqaGpqrZkeFDWV\nhwwBUW9qGdt0Gj2d4JgLU3shQVSIiAmVoReW3qjgWCuSMJqp1s40JSpoEOqfirbRQS8U/2FqLfJM\nLTXCicVotJrpkEn1zMXZkXPi2evs7LP/nPh8Pzfu9b57rfO48ed613r3Xu/IxMQEkj7bjhh2AZL6\nz6BLBRh0qQCDLhVg0KUCjhzQ3/HWvtR/I+06ug56RGwEvspkiL+XmS90eyxJ/dXV0D0ivgGcnpnn\nAFcA/9bTqiT1VLfX6N8CfgmQma8BiyNiUc+qktRT3QZ9CbB7yvbuVpukeahXd93b3gSQNHzdBn0X\n08/gXwbemns5kvqh26D/GlgDEBFnAbsy88OeVSWpp0a6/fVaRPwI+DrwMXB1Zv624e3Oo0v91/YS\nuuugHyKDLvVf26D7FVipAIMuFWDQpQIMulSAQZcKMOhSAQZdKsCgSwUYdKkAgy4VYNClAgy6VIBB\nlwow6FIBBl0qwKBLBRh0qQCDLhVg0KUCDLpUgEGXCjDoUgEGXSrAoEsFGHSpAIMuFWDQpQIMulSA\nQZcKOHLYBeizadu2bW37Lr744sZ977jjjsb+devWNfaPjLRdVLSsroIeESuAbcDvW02/y8wNvSpK\nUm/N5Yz+VGau6VklkvrGa3SpgJGJiYlD3qk1dP934HXgC8D1mfmbhl0O/Y9IOlRtb050G/RR4B+A\n+4HTgP8CvpKZ/9dmF4NejDfjhqLtv3hX1+iZOQ7c19r8Y0T8GRgF/tTN8ST1V1fX6BFxSUR8v/V6\nCXAiMN7LwiT1TrdD988B9wLHAUcxeY3+aMMuDt2LOeuss9r27dixY07HfuONNxr7Tz755Dkd/zDW\n86H7h8A/d12OpIFyek0qwKBLBRh0qQCDLhVg0KUCDLpUgEGXCjDoUgEGXSrAoEsFGHSpAIMuFWDQ\npQJ83LO68uqrr07bXr58+bS28fHuH09w7LHHNvYvWLCg62NX5RldKsCgSwUYdKkAgy4VYNClAgy6\nVIBBlwpwHl1dueeee6Zt33TTTdPadu/e3fWxN23a1Ng/Ojra9bGr8owuFWDQpQIMulSAQZcKMOhS\nAQZdKsCgSwU4j66u7Nmzp6O2mSxatKixf8mSJV3VpPY6CnpELAceBDZm5u0RcRKwFVgAvAWszcx9\n/StT0lzMOnSPiIXAbcATU5pvADZn5rnA68Dl/SlPUi90co2+D1gF7JrStgJ4qPX6YWBlb8uS1Esj\nExMTHb0xIq4D3mkN3d/OzC+12pcCWzPzaw27d/ZHJM3FSLuOXtyMa3twfXZdeeWV07bvvPPOaW13\n3XVX231nuxl39913N/ZfcMEFHVSoqbqdXtsbEce0Xo8yfVgvaZ7pNujbgYtary8CHutNOZL6Ydah\ne0ScDdwKnALsj4g1wCXAloi4EtgJ/KKfRWrw9u7d29j/3HPPddQ2k1WrVjX2OzTvvVmDnpkvMXmX\n/WDn9bwaSX3hV2ClAgy6VIBBlwow6FIBBl0qwJ+pakbPPvtsY/8rr7zSUdtM/Bnq4HlGlwow6FIB\nBl0qwKBLBRh0qQCDLhVg0KUCnEcv6oMPPmjsf+211+Z0/OOPP75t34YNG+Z0bB06z+hSAQZdKsCg\nSwUYdKkAgy4VYNClAgy6VIDz6EW9/PLLjf0bN25s7J9pnnxq21VXXdV231NPPXWW6tRrntGlAgy6\nVIBBlwow6FIBBl0qwKBLBRh0qQDn0Yu68cYbG/t37tzZ2N/0e3OAtWvXHnJN6p+Ogh4Ry4EHgY2Z\neXtEbAHOBt5tveWWzPxVf0qUNFezBj0iFgK3AU8c1PWDzHykL1VJ6qlOrtH3AauAXX2uRVKfjExM\nTHT0xoi4DnhnytB9CXAU8DawPjPfadi9sz8iaS5G2nV0ezNuK/BuZu6IiGuB64D1XR5LQ3D++ec3\n9m/fvr2x/+Cbcbt37+aEE074ZPuZZ55pu+/pp5/eQYXqpa6CnplTr9cfAn7Sm3Ik9UNX8+gR8UBE\nnNbaXAG82rOKJPVcJ3fdzwZuBU4B9kfEGibvwt8XEX8F9gKX9bNIHbr33nuvsX/ZsmWN/bMN3Y88\n8tP/6UxtO+IIv4s1n8wa9Mx8icmz9sEe6Hk1kvrC/+1KBRh0qQCDLhVg0KUCDLpUgD9T/Yx68cUX\nG/sfeaT590gnnnhiY//VV1/d2LZ06dLG/TVYntGlAgy6VIBBlwow6FIBBl0qwKBLBRh0qYCOHyU1\nRz5Kqg/27NnTtm/lypWN++7YsaOx/8ILL2zs37x587Tt0dFRxsfHp21r4No+SsozulSAQZcKMOhS\nAQZdKsCgSwUYdKkAgy4V4O/R57GDH9m8ePHiaW0Hz2VPNds8+XnnndfYf/PNNzf2zzRP7tz5/OUZ\nXSrAoEsFGHSpAIMuFWDQpQIMulSAQZcKcB59Hnv++eenbY+NjU1ru/fee9vuO9tz2Tdt2tTYHxEd\nVKjDRUdBj4gfA+e23n8T8AKwFVgAvAWszcx9/SpS0tzMOnSPiG8CyzPzHODbwCbgBmBzZp4LvA5c\n3tcqJc1JJ9foTwPfab1+H1gIrAAearU9DDQ/t0jSUB3SM+MiYh2TQ/ixzPxSq20psDUzv9awq8+M\nk/qv7TPjOr4ZFxGrgSuA84E/dHJwzc3jjz8+bXtsbGxa2zXXXNN23/fff7/x2E8++WRj/7Jlyzqo\nUIeLjqbXImIM+CHwT5n5F2BvRBzT6h4FdvWpPkk9MOsZPSI+D9wCrMzMA88X3g5cBNzd+udjfauw\nsC1btkzbHhsbm9aWmW33PemkkxqPffTRR8+lNB1mOhm6fxf4InD/lLnVS4GfRcSVwE7gF/0pT1Iv\nzBr0zPwp8NMZupqfXCBp3vArsFIBBl0qwKBLBRh0qQCDLhXgz1SH6LHHmr9+MNM8+dS2M844o+2+\nTz31VOOxFy1aNEt1+izxjC4VYNClAgy6VIBBlwow6FIBBl0qwKBLBTiP3kdvvvlmY//q1asb+487\n7rhPtY2Pj3/yumke3nlyTeUZXSrAoEsFGHSpAIMuFWDQpQIMulSAQZcKcB69jz7++OPG/v379zf2\nz7S08dS2M888s7vCVI5ndKkAgy4VYNClAgy6VIBBlwow6FIBBl0qYGRiYmLWN0XEj4FzmZx3vwm4\nEDgbeLf1llsy81cNh5j9j0iaq5F2HbN+YSYivgksz8xzIuJ44GXgSeAHmflI72qU1C+dfDPuaeD5\n1uv3gYXAgr5VJKnnOhq6HxAR65gcwn8ELAGOAt4G1mfmOw27OnSX+q/t0L3jm3ERsRq4AlgPbAWu\nzcx/BHYA182xQEl91NGPWiJiDPgh8O3M/AvwxJTuh4Cf9KE2ST0y6xk9Ij4P3AJckJl7Wm0PRMRp\nrbesAF7tW4WS5qyTM/p3gS8C90fEgbafA/dFxF+BvcBl/SlPUi8c0s24OfBmnNR/c78ZJ+nwZdCl\nAgy6VIBBlwow6FIBBl0qwKBLBRh0qQCDLhVg0KUCDLpUgEGXCjDoUgEGXSpgUMsmt/35nKT+84wu\nFWDQpQIMulSAQZcKMOhSAQZdKsCgSwUMah79ExGxEfgqk4+A/l5mvjDoGmYSESuAbcDvW02/y8wN\nw6sIImI58CCwMTNvj4iTmFwOawHwFrA2M/fNk9q2cGhLafeztoOX+X6BefC59WD58a4NNOgR8Q3g\n9NYSzMuA/wTOGWQNs3gqM9cMuwiAiFgI3Mb05a9uADZn5raI+FfgcoawHFab2mAeLKXdZpnvJxjy\n5zbs5ccHPXT/FvBLgMx8DVgcEYsGXMPhYh+wCtg1pW0Fk2vdATwMrBxwTQfMVNt88TTwndbrA8t8\nr2D4n9tMdQ1s+fFBD92XAC9N2d7davtgwHW08/cR8RDwBeD6zPzNsArJzL8Bf5uyDBbAwilDzreB\nvxt4YbStDWB9RPwLnS2l3a/aPgL+p7V5BfAoMDbsz61NXR8xoM9s2Dfj5tN34P8AXA+sBi4F/iMi\njhpuSY3m02cH82wp7YOW+Z5qqJ/bsJYfH/QZfReTZ/ADvszkzZGhy8xx4L7W5h8j4s/AKPCn4VX1\nKXsj4pjM/F8ma5s3Q+fMnDdLaR+8zHdEzIvPbZjLjw/6jP5rYA1ARJwF7MrMDwdcw4wi4pKI+H7r\n9RLgRGB8uFV9ynbgotbri4DHhljLNPNlKe2ZlvlmHnxuw15+fFCrqX4iIn4EfB34GLg6M3870ALa\niIjPAfcCxwFHMXmN/ugQ6zkbuBU4BdjP5P90LgG2AEcDO4HLMnP/PKntNuBa4JOltDPz7SHUto7J\nIfB/T2m+FPgZQ/zc2tT1cyaH8H3/zAYedEmDN+ybcZIGwKBLBRh0qQCDLhVg0KUCDLpUgEGXCvh/\nWq0/GA29yHAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7df20b1550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A5-zC32qbpQM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# モデルの定義\n",
        "\n",
        "それでは次に学習対象のモデルを定義します。\n",
        "\n",
        "今回は3層からなるニューラルネットワークの例をあげます。\n",
        "\n",
        "```\n",
        "import chainer\n",
        "from chainer import links as L\n",
        "from chainer import functions as F\n",
        "\n",
        "...\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            # the size of the inputs to each layer will be inferred\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "```\n",
        "\n",
        "ニューラルネットワークのモデルを定義するオブジェクトは `chainer.Chain`（以降 `Chain` ）を継承します。\n",
        "`Chain` を継承することで，このモデルを保存したり読み込んだりすることができます。\n",
        "\n",
        "モデルでは`with self.init_scope()`のスコープ内で利用するパラメータ付き関数であるLinkを登録します。\n",
        "\n",
        "上記の例では `Linear` である `l1`, `l2`, `l3` を登録しています。\n",
        "`Linear` は線形変換であり，初期化引数として入力次元数と出力次元数をうけとります。\n",
        "`Linear` の入力次元数に `None` を指定した時は，それが最初に呼び出された時，次元数を引数から推定してくれます。\n",
        "\n",
        "初期化時に登録されたLinkはあとで `self.l1` のようにオブジェクトの属性として参照できます。\n",
        "\n",
        "次に，モデルを使って入力をどのように変換して出力を得るのかを定義します。\n",
        "学習時に出力から入力へ逆方向に勾配を伝播させる誤差逆伝搬法との比較で，この入力から出力への計算を順計算（forward-computation）とよびます。\n",
        "\n",
        "順計算は，多くの場合 `__call__` メソッドで定義します。\n",
        "さきほど登録した `l1`, `l2`, `l3` を使って入力 `x` から3回線形変換と2回ReLUを適用して結果を返す順計算を定義してみましょう。\n",
        "\n",
        "```\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "```\n",
        "\n",
        "最後， `self.l3(h2)` には `F.relu` を適用していないことに注意してください。\n",
        "softmaxを使う際によくある間違いとして，最後の出力にもReLUを適用してしまうというのがあります。\n",
        "softmaxの定義域は負を含む実数ですので，その入力を非負に制約すると，想定しない制約を課して学習することになります。\n",
        "\n",
        "順計算は `__call__` で定義する必要は必ずしもありません。\n",
        "また，順計算は複数用意してもよいですし，その場で新しく作ってもよいです。\n",
        "例えば，2層目の途中の中間結果を返すメソッドを次のように定義し使うこともできます。\n",
        "\n",
        "```\n",
        "    def forward_with_two_layers(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        return self.l2(h1)\n",
        "```\n",
        "\n",
        "順計算を `__call__` で定義したおかげで，この `MLP` は， `()` で順計算を呼び出すことができます。\n",
        "\n",
        "このように作ったMLPを分類器として使うには `L.Classifier` を使ってモデルを作ります。\n",
        "`Classifier` はデフォルトでは分類器softmax，学習時の損失関数はsoftmaxクロスエントロピー損失を使います。\n",
        "`Classifier` が引数としてとるモデルは `__call__()` で順計算が定義されていることを想定しています。\n",
        "\n",
        "```\n",
        "model = L.Classifier(MLP(784, 10))\n",
        "```\n",
        "\n",
        "## メモ: Define-by-Run\n",
        "\n",
        "Chainerの特徴は\"Define-by-Run\"，つまり順に実行しながらネットワークを定義していきます。\n",
        "この例では関数呼び出し `__call__` の中でネットワークを順に作っています。\n",
        "\n",
        "## 課題\n",
        "\n",
        "`Chain` を継承したオブジェクトに登録されているパラメータ付関数は `namedlinks()` で呼び出すことができます。\n",
        "例えば上の例の場合 `l1`, `l2`, `l3` が呼び出されます。\n",
        "上の例を4層のニューラルネットワークに変更し`namedlinks()`の要素を表示してください。"
      ]
    },
    {
      "metadata": {
        "id": "1Ohybhhbbrny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f7056be1-7bc1-4389-949e-bcf1ce71bf2c"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "\n",
        "# make your network\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            # the size of the inputs to each layer will be inferred\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "model = L.Classifier(MLP(784, 2))\n",
        "\n",
        "# print out namedlinks\n",
        "for l in model.namedlinks():\n",
        "    print(l)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('/', <chainer.links.model.classifier.Classifier object at 0x7f7defad2710>)\n",
            "('/predictor', <__main__.MLP object at 0x7f7defad2780>)\n",
            "('/predictor/l2', <chainer.links.connection.linear.Linear object at 0x7f7defad27f0>)\n",
            "('/predictor/l3', <chainer.links.connection.linear.Linear object at 0x7f7defad2a58>)\n",
            "('/predictor/l1', <chainer.links.connection.linear.Linear object at 0x7f7defad27b8>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0iZGKqsMXFd1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNISTの学習\n",
        "\n",
        "これまで，学習/評価用データセットを作り，また学習対象のモデルを作りました。\n",
        "\n",
        "```\n",
        "train_full, test_full = chainer.datasets.get_mnist()\n",
        "train = datasets.SubDataset(train_full, 0, 1000)\n",
        "test = datasets.SubDataset(test_full, 0, 1000)\n",
        "model = L.Classifier(MLP(100, 10))\n",
        "```\n",
        "\n",
        "それでは実際に学習させてみましょう。\n",
        "今回は時間短縮のためMNISTデータセットのうち1000件のみを用いることにします。\n",
        "\n",
        "Chainerでは学習操作を抽象化するための機能が揃っています。\n",
        "これらを利用することで殆ど自分でコードを書くことなく学習させることができます。\n",
        "\n",
        "はじめにデータセット上の操作を抽象化する `Iterator` を用意します。\n",
        "`Iterator` は構築時にデータセットを引数として指定すると，そのデータセットに対する `Iterator` を返します。\n",
        "引数として，`batch_size` は，一度のアクセスでいくつ同時に読み込むか， `shuffle` はアクセスの際にランダムにアクセスするかどうかを指定します。\n",
        "\n",
        "```\n",
        "# Set up a iterator\n",
        "batchsize = 100\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
        "                                             repeat=False, shuffle=False)\n",
        "```\n",
        "\n",
        "次に，パラメータの最適化を担当する `Optimizer` を用意しします。\n",
        "ここでは複数ある `Optimizer` の中で `Adam` を使います。\n",
        "`Adam` は広い学習問題で安定して学習できる手法です。\n",
        "\n",
        "`Optimizer` はsetup()で最適化対象の `Chain` または `Link` を指定する必要があります。\n",
        "\n",
        "```                                \n",
        "# Set up an optimizer\n",
        "opt = chainer.optimizers.Adam()\n",
        "opt.setup(model)\n",
        "```\n",
        "\n",
        "次に，実際のパラメータ更新を担当する `Updater` を用意します。\n",
        "これまで用意した学習用データに対するIterator，最適化を担当する `Optimizer` ，そしてどのデバイスで\n",
        "実行するのかを指定します。`device=-1`はCPUを使うことを表します。\n",
        "\n",
        "```\n",
        "# Set up an updater\n",
        "updater = training.StandardUpdater(train_iter, opt, device=-1)\n",
        "```\n",
        "\n",
        "最後に学習ループを担当する `Trainer` を用意します。\n",
        "今回は5エポック(5回データセットを走査する)だけ学習を回すようにします。\n",
        "\n",
        "```\n",
        "# Set up a trainer\n",
        "epoch = 5\n",
        "trainer = training.Trainer(updater, (epoch, 'epoch'), out='/tmp/result')\n",
        "```\n",
        "\n",
        "`Trainer` は様々な拡張機能を使うことができます。\n",
        "\n",
        "評価データで評価をするには，次のようにします。\n",
        "\n",
        "```\n",
        "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n",
        "```\n",
        "\n",
        "学習途中の結果を表示するには，次のようにします。\n",
        "1エポックごとに、trainデータに対するaccuracyと、testデータに対するaccuracyを出力させます。\n",
        "\n",
        "```\n",
        "trainer.extend(extensions.LogReport(trigger=(1, \"epoch\")))\n",
        "trainer.extend(extensions.PrintReport(\n",
        "        ['epoch', \n",
        "         'main/accuracy', 'validation/main/accuracy']), trigger=(1, \"epoch\"))\n",
        "```\n",
        "\n",
        "これで全て用意ができました。\n",
        "trainerのrunを呼び出すことで学習できます。\n",
        "\n",
        "```\n",
        "# Run the trainer\n",
        "trainer.run()\n",
        "```\n",
        "\n",
        "最後に学習の結果を確認してみましょう。\n",
        "ランダムに選んだテストデータ一件に対する予測を出力します。\n",
        "\n",
        "```\n",
        "x, y = test[np.random.randint(len(test))]\n",
        "playground.print_mnist(x)\n",
        "pred = F.softmax(model.predictor(x.reshape(1, 784))).data\n",
        "print(\"Prediction: \", np.argmax(pred))\n",
        "print(\"Correct answer: \", y)\n",
        "```\n",
        "\n",
        "\n",
        "## 課題\n",
        "\n",
        "`Trainer` を実際に動かし学習できることを確かめてください。\n",
        "その上で例えばユニット数を変えたり，epoch（学習回数）を変えたり， `Optimizer` を `RMSprop()` などに変えたりして精度が変わることを確認してください。"
      ]
    },
    {
      "metadata": {
        "id": "I0Sdsk5rawEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a8543878-acfa-4162-876b-f2e695e4bfb2"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import datasets\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import optimizers\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "# create model\n",
        "model = L.Classifier(MLP(100, 10))\n",
        "\n",
        "# load dataset\n",
        "train_full, test_full = datasets.get_mnist()\n",
        "train = datasets.SubDataset(train_full, 0, 1000)\n",
        "test = datasets.SubDataset(test_full, 0, 1000)\n",
        "\n",
        "# Set up a iterator\n",
        "batchsize = 100\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
        "                                             repeat=False, shuffle=False)\n",
        "\n",
        "# Set up an optimizer\n",
        "\n",
        "# Set up an updater\n",
        "\n",
        "# Set up a trainer\n",
        "\n",
        "# Run the trainer\n",
        "\n",
        "# Check the result"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUOZzPs1XFjD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：機械学習\n",
        "\n",
        "ニューラルネットワークを含んだ多くの機械学習における学習タスクは最適なパラメータを探す問題です。\n",
        "\n",
        "そして，最適なパラメータは目的関数の最小化（最大化）問題を解くことで自動的に得られます。\n",
        "\n",
        "一般に何かを学習させたいという場合は次のステップからなります。\n",
        "\n",
        "1) 学習対象のモデルを定義する\n",
        "\n",
        "Chainerの場合，Function, Link, Chainを組み合わせて入力から出力を求める関数を定義することに対応します。\n",
        "\n",
        "2) 目的関数 $L(F(\\theta))$ を定義する\n",
        "\n",
        "Chainerの場合，Classifierや損失関数（例：F.soft_cross_entropyや，F.mean_squared_error）を使うことに対応します。\n",
        "\n",
        "3) 目的関数を最小化するような$\\theta$を最適化問題を解くことで得る\n",
        "\n",
        "Chainerの場合，Optimizerを使って学習させることに対応します。\n",
        "\n",
        "これらを順に紹介していきます。"
      ]
    },
    {
      "metadata": {
        "id": "kMKkCcZ7djm1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Variable\n",
        "\n",
        "それではこの章からChainerの基本についてみていきます。\n",
        "\n",
        "ChainerはDefine-by-runとよばれる思想に基づいて作られています。\n",
        "これは計算手順を書くとそれ自体がネットワークの定義になるというものです。\n",
        "\n",
        "## メモ\n",
        "\n",
        "ネットワークの定義に利用する変数はVariableとよばれるオブジェクトとして定義する必要があります。\n",
        "Variableオブジェクトとして定義すると，以降このVariableオブジェクトを含む計算手順は全て追跡され計算グラフが自動的に作られます。\n",
        "\n",
        "例として，5という値一つからなるndarrayを作り，それを元にVariableを作ってみましょう。\n",
        "\n",
        "```\n",
        "x_data = np.array([5], dtype=np.float32)\n",
        "x = Variable(x_data)\n",
        "```\n",
        "\n",
        "このVariableオブジェクトは普通の数と同じように基本的な算術演算をすることができます。\n",
        "例えば，次のように実行できます。\n",
        "\n",
        "```\n",
        "y = x ** 2 - 2 * x + 1\n",
        "```\n",
        "\n",
        "Variableオブジェクトを使った演算結果はVariableオブジェクトとなります。\n",
        "この例の場合，yもVariableオブジェクトです。\n",
        "\n",
        "Variableオブジェクトの値はdata属性で参照することができます。\n",
        "\n",
        "```\n",
        "y.data\n",
        "```\n",
        "\n",
        "また，numpyと同じ添字アクセスを備えています。\n",
        "\n",
        "```\n",
        "z_data = np.array([[2, 3, 4], [5, 6, 7]], dtype=np.float32)\n",
        "z = Variable(z_data)\n",
        "print(z[:, 1].data) # [3, 6]\n",
        "```\n",
        "\n",
        "# 課題\n",
        "\n",
        "i=0...100について，Variableオブジェクトを$x_i=2*i+1$とした上で，これらの二乗和であるVariableオブジェクト$y=\\sum_i x_i^2$を計算しその値を表示せよ"
      ]
    },
    {
      "metadata": {
        "id": "ArgS7qw0ds2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8b418631-4abf-43eb-d230-a994a0b904ac"
      },
      "cell_type": "code",
      "source": [
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x_data = np.array([5], dtype=np.float32)\n",
        "x = Variable(x_data)\n",
        "y = x ** 2 - 2 * x + 1\n",
        "print(y.data)\n",
        "\n",
        "z_data = np.array([[2, 3, 4], [5, 6, 7]], dtype=np.float32)\n",
        "z = Variable(z_data)\n",
        "print(z[:, 1].data)  # [[3], [6]]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16.]\n",
            "[3. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nW5tg_lTdjwg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：backward 後ろ向き計算\n",
        "\n",
        "Variableオブジェクトは値だけではなく，それまでの計算履歴を全て持っています。\n",
        "この計算履歴にはどの変数と変数をどのような演算で組み合わせたかという情報を全て持っています。\n",
        "このような計算履歴は計算グラフとも呼ばれます。\n",
        "\n",
        "## メモ\n",
        "\n",
        "この計算履歴はdump_graphを使って図示化することができます。使い方は[mnist example](https://github.com/pfnet/chainer/blob/master/examples/mnist/train_mnist.py)を参照してください。\n",
        "\n",
        "この計算履歴の情報を使って，最終的な値（スカラー値）に対する各変数についての勾配（gradient, grad）を計算することができます。\n",
        "Chainerではこの勾配を後ろ向き計算（逆誤差伝播法）を使って効率的に求めることができます。\n",
        "\n",
        "さきほどの演算結果であるVaribleオブジェクトyのbackward関数を呼び出すと勾配を計算し，yの計算結果に関連したVaribleのgrad属性に勾配をセットします。\n",
        "\n",
        "```\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "```\n",
        "\n",
        "値yの変数xについての勾配というのは，簡単に言えば$x$をほんの少しだけふやした時に，$y$がどの程度変わるのかという値です。\n",
        "\n",
        "例えば，$y=2x$というのは，$x$をほんの少し$d$だけ増やした時，$y$は$2d$だけ増えます。\n",
        "そのため$y=2x$という値の$x$についての勾配は（$x$によらず）2となります。\n",
        "\n",
        "$x$がベクトルやテンソルの場合には，各要素について，他の要素を固定した上で，対象の要素だけを少しだけ動かした時にどの程度変わるのかという値になります。これは数学的には偏微分とよびます。\n",
        "\n",
        "変数$x$がベクトルやテンソルの場合は勾配はその要素毎の偏微分を並べたベクトルやテンソルになります。\n",
        "このため勾配ベクトルとよんだりもします。\n",
        "\n",
        "また，勾配ベクトルは今の各変数をどの方向に動かしたら最も急激に関数が大きくなるのかという値を意味しています。\n",
        "その逆に，勾配ベクトルの逆向きは最も急激に関数の値を小さくなる方向です。\n",
        "\n",
        "backwardは基本的には損失関数の値などスカラー値に対して呼び出せます。\n",
        "複数の値に対する偏微分を並べたものはヤコビアン行列とよびますがChainerではヤコビアン行列の計算はサポートしていません。\n",
        "スカラー値でない場合，ユーザーが手動でgradを設定をすればbackwardを呼び出して計算することができます。\n",
        "\n",
        "```\n",
        "z = Variable(np.array([10, 20], dtype=np.float32))\n",
        "zz = 2 * z\n",
        "\n",
        "# これはエラー，backwardはスカラー値に対してしか基本的には呼び出せない\n",
        "zz.backward()\n",
        "\n",
        "# ユーザーがzzの勾配を設定すればbackwardを呼び出せる．これは何らかの値に対する勾配とみなせる\n",
        "zz.grad = np.array([0.1, -0.1], dtype=np.float32)\n",
        "zz.backward()\n",
        "```\n",
        "\n",
        "## 課題\n",
        "\n",
        "右の例で，実際に求めた勾配を0.01倍したものを$x$に足した上で$y$の値が実際に大きくなっていることを確かめよ。 "
      ]
    },
    {
      "metadata": {
        "id": "NFH9xmB2dihM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9c9e9e72-1a07-4b36-9e6d-ab94975984ea"
      },
      "cell_type": "code",
      "source": [
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "x_data = np.array([5], dtype=np.float32)\n",
        "x = Variable(x_data)\n",
        "y = x ** 2 - 2 * x + 1\n",
        "y.backward()\n",
        "\n",
        "z = Variable(np.array([10, 20], dtype=np.float32))\n",
        "zz = 2 * z\n",
        "zz.grad = np.array([0.1, -0.1], dtype=np.float32)\n",
        "zz.backward()\n",
        "\n",
        "print(x.grad)\n",
        "print(z.grad)\n",
        "\n",
        "# print(y.data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8.]\n",
            "[ 0.2 -0.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VamfcJ4TdkHA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Function, Link, Chain\n",
        "\n",
        "Chainer上のニューラルネットワークの定義で使う関数を表すオブジェクトには次の三つの種類があります。\n",
        "\n",
        "* Function\n",
        "　学習可能なパラメータを持たない関数\n",
        "\n",
        "* Link\n",
        "　学習可能なパラメータを持った関数\n",
        "\n",
        "* Chain（Linkを継承）\n",
        "　複数のLinkから構成される関数\n",
        "　Chain自体はLinkを継承しているので他のChainを組み合わせて構成することも可能\n",
        "\n",
        "ニューラルネットワークの定義ではこれらを組み合わせていくことで実現されます。\n",
        "\n",
        "それではこれらを順に紹介していきましょう。"
      ]
    },
    {
      "metadata": {
        "id": "qAIXuLfqeLfY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Function\n",
        "\n",
        "Functionオブジェクトは学習可能なパラメータを持たない関数です。\n",
        "但し通常の関数とは違って逆誤差伝播が計算できるように，前向き計算に加えて，後ろ向き計算ができるようになっています。\n",
        "\n",
        "また，CPUとGPUの両方における計算が定義されています。\n",
        "内部では，foward_cpu, foward_gpu, backward_cpu, backward_gpuの四種類の実装がされています。\n",
        "\n",
        "Functionを利用するには，Functionのインスタンスを作成後に関数として呼びだします。\n",
        "ニューラルネットワークで利用する多くの関数がchainer.functionsで既に実装されています。\n",
        "例えば，$\\mathrm{relu}(x) = \\max(x, 0)$で定義されるrelu関数は次のように呼び出します。\n",
        "\n",
        "```\n",
        "from chainer import functions as F\n",
        "\n",
        "x_data = np.array([5], dtype=np.float32)\n",
        "x = Variable(x_data)\n",
        "y = F.relu(x)\n",
        "```\n",
        "\n",
        "これらの関数の多くは要素毎に対する関数適用であり，計算結果のshapeは計算元と変わりませんが，\n",
        "一部の関数はshapeが変わります。\n",
        "\n",
        "\n",
        "なお，Variableは基本的な算術演算ができるという話しでしたが、それらは実際には算術演算を対応するFunction呼び出しをオーバーロードして実現されています。\n",
        "\n",
        "例えば，\n",
        "\n",
        "```\n",
        "x = Variable(x_data)\n",
        "y = Variable(y_data)\n",
        "z = x + y\n",
        "```\n",
        "\n",
        "は，内部ではそのFunctionオブジェクト\n",
        "\n",
        "```\n",
        "z = F.Add(x, y)\n",
        "```\n",
        "\n",
        "を呼び出しています。\n",
        "\n",
        "Functionは計算履歴を追跡する仕掛けが含まれています。\n",
        "Functionを適用した結果はVariableであり，backwardを呼び出して勾配を求めることができます\n",
        "\n",
        "```\n",
        "z.backward()\n",
        "```\n",
        "\n",
        "Variableが保持するデータはndarrayなので，numpyと同様の様々な配列変換（例えばreshapeやtransposeなど）を使いますがこの場合も計算結果が追跡できるようにfunctionsにある対応する関数を呼び出す必要があります。\n",
        "\n",
        "```\n",
        "z = Variable(np.array([[10, 20], [30, 40]], dtype=np.float32))\n",
        "zz = F.transpose(z)\n",
        "print(zz.data)\n",
        "```\n",
        "\n",
        "\n",
        "## メモ\n",
        "\n",
        "後ろ向き計算とは、 逆誤差伝播法で使われる出力に対する入力についての勾配，つまり$y = f(x)$の時 $\\partial y / \\partial x$の計算です。例えば$y=3x^2$の場合は後ろ向き計算は$y$の$x$についての微分，つまり$6x$が後ろ向きの結果になります。\n",
        "後ろ向き計算は効率的に計算でき，多くの場合前向き計算とほぼ同じ計算量で求めることができます。\n",
        "\n",
        "## 課題\n",
        "\n",
        "$x=[3, 4, 5]$の時，$\\exp(x)+\\sin(x)$の勾配を求めて表示せよ。"
      ]
    },
    {
      "metadata": {
        "id": "btgZbFTReT1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3151a5e5-2d65-417e-eeb1-b9307e009da0"
      },
      "cell_type": "code",
      "source": [
        "from chainer import functions as F\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x_data = np.array([5], dtype=np.float32)\n",
        "x = Variable(x_data)\n",
        "y = F.relu(x)\n",
        "\n",
        "z = Variable(np.array([[10, 20], [30, 40]], dtype=np.float32))\n",
        "zz = F.transpose(z)\n",
        "print(zz.data)\n",
        "\n",
        "# print() exp(x)+sin(x)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[10. 30.]\n",
            " [20. 40.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fydkB3FqeLtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Link\n",
        "\n",
        "ニューラルネットワークはパラメータを持った関数のパラメータを調整することで学習を実現します。\n",
        "\n",
        "Chainerではこのような学習可能なパラメータを持った関数をLinkとよびます。\n",
        "\n",
        "```\n",
        "from chainer import links as L\n",
        "```\n",
        "\n",
        "例として，ニューラルネットワークで最も広く利用される関数である[Linear](http://docs.chainer.org/en/stable/reference/links.html)を紹介しましょう。\n",
        "このLinearは総結合層，またはアフィン変換とよばれ，次の関数で表現されます。\n",
        "\n",
        "```math\n",
        "f(x; W, b) = Wx + b\n",
        "```\n",
        "\n",
        "関数において，$;$より後ろ側の変数はそれがパラメータだということを意味します。\n",
        "Linearは初期化パラメータとして（入力の次元数, 出力の次元数）をうけとります　　\n",
        "例えば入力が3次元のベクトルで出力が2次元のベクトルの場合，\n",
        "\n",
        "```\n",
        "f = L.Linear(3, 2)\n",
        "```\n",
        "\n",
        "のように定義されます。\n",
        "これは内部では3行2列の行列Wと2列のベクトルbからなります。\n",
        "\n",
        "Linkのパラメータは属性に保存されており，それらはVariableです。\n",
        "例えば，Linkの場合，Wとbの属性があります。\n",
        "\n",
        "```\n",
        "f.W\n",
        "f.b\n",
        "```\n",
        "\n",
        "Chainerの場合，Wのデフォルト値はガウシアン分布に従う乱数で初期化され，bは0に初期化されます。\n",
        "これらの初期値はオプション引数で選ぶことができます。\n",
        "また，デフォルトではバイアス項であるbがありますが，nobias=Trueを指定することでバイアスが無いLinearを作ることができます。\n",
        "\n",
        "LinkはFunctionと同様に関数として呼び出すことができます。\n",
        "\n",
        "多くのFunctionやLinkは入力として最初の次元数がバッチであるようなミニバッチ入力をうけとるように設計されています。\n",
        "例えば，先程のLinearはバッチサイズがNの時，shapeが(N, 3)であるVariableを入力とし，shapeが(N, 2)でVariableを出力します。\n",
        "\n",
        "```\n",
        "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=np.float32)\n",
        "y = f(x)\n",
        "print(y.data)\n",
        "```\n",
        "\n",
        "Linksでは次のようなメソッドがあります（全て覚える必要はありません）。\n",
        "\n",
        "* add_persistent(name, value)\n",
        "　save, load時の対象となるパラメータを追加する。\n",
        "* addgrads(link)\n",
        "　linkのgradient値を加算する。例えば分散学習時に使われる。\n",
        "* children()\n",
        "　子のlinkのgeneratorを返す。\n",
        "* cleargrads()\n",
        "　gradの値を0に初期化する。backward命令の前に呼び出す必要がある。\n",
        "* copy()\n",
        "　対象のlinkの子全てをコピーする。浅いコピーであり，パラメータのVariableはオブジェクトはコピーだが，それらのdataとgradient配列は共有される。linkの名前は初期化される。\n",
        "* copyparams(link)\n",
        "　linkからparameterをコピーする。\n",
        "* namedlinks()\n",
        "　全てのpath, linkを返す\n",
        "* namedparams()\n",
        "　全てのpath, paramを返す\n",
        "* serialize(serializer)\n",
        "　このlinkオブジェクトをserializeする\n",
        "* to_cpu()\n",
        "　パラメータとpersistent値をCPUにコピーする\n",
        "* to_gpu(device=None)\n",
        "　パラメータとpersistnt値をGPUにコピーする\n",
        "* xp\n",
        "　今CPUとGPUのどちらにいるかにしたがって，CPUであればnumpy，GPUであればcupyを返す\n",
        "\n",
        "## メモ\n",
        "\n",
        "殆どのLinkには，それと同じ名前のFunctionが存在します。\n",
        "例えばLinearもlinks.Linearとfunctions.Linearが存在します。\n",
        "前者が学習可能パラメータを属性として持ったLinkであり，後者は学習可能パラメータを引数として受け取って計算Functionです。\n",
        "links.Linearは内部でfunctions.Linearを呼び出して使っています。\n",
        "もしユーザーが自分で学習対象パラメータを管理した上で同じ関数を使いたい場合はfunctions上で定義されている関数を直接呼び出して使うことができます。\n",
        "\n",
        "## 課題\n",
        "\n",
        "入力，出力がともに(N, 3, 4)であるような[bias](http://docs.chainer.org/en/stable/reference/links.html?highlight=link#bias)を作り，それをshapeが(2, 3, 4)であるVariableに適用し，その結果を表示せよ。"
      ]
    },
    {
      "metadata": {
        "id": "HLlhfRR6dilm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c4460229-684b-4b06-ca23-497474423660"
      },
      "cell_type": "code",
      "source": [
        "from chainer import links as L\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "f = L.Linear(3, 2)\n",
        "x = np.array([[1, 2, 3],\n",
        "              [4, 5, 6],\n",
        "              [7, 8, 9],\n",
        "              [10, 11, 12]],\n",
        "             dtype=np.float32)\n",
        "y = f(x)\n",
        "print(y.data)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.6726933   0.95064974]\n",
            " [-3.5202675   1.4190176 ]\n",
            " [-5.3678417   1.8873852 ]\n",
            " [-7.2154164   2.355753  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lmsxFqO2eh1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Chain\n",
        "\n",
        "多くのニューラルネットワークは複数のLinkから構成されます。\n",
        "例えば，多層パーセプトロンは複数のLinear層からなります。\n",
        "\n",
        "Chainerでは複数のLinkをまとめて一つのオブジェクトChainとして扱うことができます。\n",
        "Chainはユーザーがネットワークを定義する際に利用されます。\n",
        "\n",
        "```\n",
        "class MyChain(Chain):\n",
        "    def __init__(self):\n",
        "        super(MyChain, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(4, 3)\n",
        "            self.l2 = L.Linear(3, 2)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self.l1(x)\n",
        "        return self.l2(h)\n",
        "```\n",
        "\n",
        "Chainを継承すると，その中に含まれる複数のLinkの管理やCPU/GPU間のデータ移動などが実現されます。\n",
        "\n",
        "ChainではLinkを登録するには，例のように`with self.init_scope()`の中で登録します。\n",
        "\n",
        "Chainの中に含まれるLinkを子Linkとよびます。\n",
        "例えばさきほどの例ではl1とl2がMyChainの子Linkです。\n",
        "なお，Chain自身もLinkを継承しています。\n",
        "そのため，あるChainを他のChainの子リンクとして使うことができます。\n",
        "\n",
        "Chainの子リンクは属性としてアクセスすることができます。\n",
        "\n",
        "```\n",
        "c = MyChain()\n",
        "print(c.l1.W.data)\n",
        "```\n",
        "\n",
        "また，Chainでは各Linkを名前付きで定義していましたが，任意個のLinkのリストを受け取るChainListを使うこともできます。\n",
        "\n",
        "```\n",
        "class MyChainList(ChainList):\n",
        "    def __init__(self):\n",
        "        super(MyChain, self).__init__(\n",
        "            L.Linear(4, 3),\n",
        "            L.Linear(3, 2),\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self[0](x)\n",
        "        return self[1](h)\n",
        "```\n",
        "\n",
        "# 課題\n",
        "\n",
        "正の整数$n$を初期化パラメータとして受け取り，$n$個のLinear(3, 3)を子Link(l1, l2, ..., ln)として含み，入力に対し$l1, l2, ..., ln$を順に適用するようなChainオブジェクトを作れ。"
      ]
    },
    {
      "metadata": {
        "id": "87XFuxADdio-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b2bf12e5-1352-41b1-97d3-a25a518f71be"
      },
      "cell_type": "code",
      "source": [
        "from chainer import Chain\n",
        "from chainer import ChainList\n",
        "from chainer import links as L\n",
        "\n",
        "\n",
        "class MyChain(Chain):\n",
        "    def __init__(self):\n",
        "        super(MyChain, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(4, 3)\n",
        "            self.l2 = L.Linear(3, 2)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self.l1(x)\n",
        "        return self.l2(h)\n",
        "\n",
        "\n",
        "class MyChainList(ChainList):\n",
        "    def __init__(self):\n",
        "        super(MyChainList, self).__init__(\n",
        "            L.Linear(4, 3),\n",
        "            L.Linear(3, 2),\n",
        "        )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h = self[0](x)\n",
        "        return self[1](h)\n",
        "\n",
        "\n",
        "c = MyChain()\n",
        "c2 = MyChainList()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TeJf9AZ-eqRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Optimization\n",
        "\n",
        "次に学習のエンジンである最適化を紹介します。\n",
        "\n",
        "最適化は，与えられた目的関数$F(x)$の最小値または極小値およびそれを達成する変数xを探す問題です。\n",
        "\n",
        "機械学習は最適化を使ってパラメータを推定，つまり学習を実現します。\n",
        "この実現のために，作ったモデルの推定結果$y'=F(x; \\theta)$と望ましい結果$y'$の差を測る損失関数$l(y', y)$を用意します。\n",
        "損失関数は推定した結果が望ましい結果であれば0，そうでなければ大きな正の値をとるような関数です。\n",
        "例えば回帰問題などでは二乗誤差$l(y', y) = (y' - y)^2$を使い，分類問題ではクロスエントロピー誤差 $-p(y) \\log{p(y')}$を使います。\n",
        "\n",
        "学習データセット${(x_i, y_i)}$が与えられた時，この学習データセットに対して損失関数の和が最小になるようなパラメータを探せば，それは多くの学習データを望ましい結果で推定していることになります。\n",
        "\n",
        "```math\n",
        "L(\\theta) = \\sum_i l(F(x_i; \\theta), y_i)\n",
        "```\n",
        "\n",
        "また，学習データに依存しないような事前知識を正則化項$R(\\theta)$として入れることもできます。\n",
        "\n",
        "```math\n",
        "L(\\theta) = R(\\theta) + \\sum_i l(F(x_i; \\theta), y_i)\n",
        "```\n",
        "\n",
        "\n",
        "これらの目的関数を最小化すれば，多くの学習データをうまく推定でき，事前知識の制約も満たすようなパラメータを求めることができます。\n",
        "\n",
        "## Memo\n",
        "\n",
        "最小値，極小値\n",
        "\n",
        "全ての $x$ について $F(x) >= F(x^*)$ を満たすような $F(x^*)$ を最小値とよび，それを達成する $x^*$ を最小解とよびます。\n",
        "一方 $x$ の近傍を $R(x)$ とした時，$x \\in R(x^*) $ について $F(x)>=F(x^*a)$ を満たすような $F(x^*)$ を極小値とよび，それを達成する $x^*$ を極小解とよびます。\n",
        "目的関数が凸関数のような性質をもっていない場合，最小解を見つけることは一般に難しく極小解を見つけることになります。\n",
        "一方でニューラルネットワークの学習における最適化の場合，殆どの極小解は最小解と殆ど変わらないほど性能が良いことが予想されています[link](https://arxiv.org/abs/1412.0233)。\n",
        "\n",
        "\n",
        "Chainerでは様々な最適化Optimizerがchainer.optimizersで実装されています。\n",
        "Optimizerを使うにはoptimizerを初期化し，次にoptimizerの最適化対象となるlinkをsetup()で設定します。\n",
        "\n",
        "```\n",
        "from chainer import optimizers\n",
        "\n",
        "model = F.Classifier(MyLink())\n",
        "optimizer = optimizers.Adam()\n",
        "optimizer.use_cleargrads()\n",
        "optimizer.setup(model)\n",
        "```\n",
        "\n",
        "この場合，modelというlinkが最適化対象になります。\n",
        "\n",
        "\n",
        "初期化の際，いくつかパラメータを選べます。\n",
        "これらのパラメータはハイパーパラメータとよばれ，上記の最適化に使うものですから，自動決定できないパラメータです。\n",
        "そのためハイパーパラメーターはユーザーが指定するか，決められた候補の中を全部試すか，別の推定手法を使う必要があります。\n",
        "\n",
        "Chainerが用意しているoptimizerの中で代表的なものは次の三つです。\n",
        "\n",
        "* SGD\n",
        "* Adam\n",
        "* RMSProp\n",
        "\n",
        "どれを使えばよいかはそれぞれ異なる最適化手法に基づいているので一概にいえませんが，安定して最適化できるのでAdam，ハイパーパラメータを選ぶが安定して精度が出やすいのはRMSPropという特徴があります。SGDは最も単純な更新則にもとづいており上記の二つの手法に比べると性能は悪くデバッグ目的以外では使う必要はありません。\n",
        "\n",
        "optimizerの使い方は三つあります。後の方がより使いやすくカスタマイズしにくい方法になります。\n",
        "\n",
        "* ユーザーがbackward()などで勾配を求めて，引数なしのupdate()を呼び出します。この場合，cleargrads()を最初に呼ぶ必要があります。\n",
        "\n",
        "```\n",
        "model.cleargrads()\n",
        "loss.backward()\n",
        "optimizer.update()\n",
        "```\n",
        "\n",
        "* 損失関数をupdate()に渡す。この場合，cleargrads()はupdate内で自動的によばれます。そのためcleargradsのよび忘れがなく簡潔に書けます。\n",
        "\n",
        "```\n",
        "def lossfun(args...):\n",
        "    ...\n",
        "    return loss\n",
        "optimizer.update(lossfun, args...)\n",
        "```\n",
        "\n",
        "* Trainerを利用する\n",
        "\n",
        "これについては後の章で紹介します。\n",
        "\n",
        "後の方法になるほど，より簡潔に書くことができます。\n",
        "一方で細かく最適化を制御したい（例えば，直接gradを操作したい）場合は最初の方法を使うことができます。\n",
        "\n",
        "## 課題\n",
        "\n",
        "SGD最適化は関数$F(x; s)$のパラメータsについての勾配が$v$の時\n",
        "\n",
        "```math\n",
        "s := s - a v\n",
        "```\n",
        "\n",
        "とすることで最適化を行います，但しa>0は学習律とよばれるハイパーパラメータです。\n",
        "\n",
        "```\n",
        "def f(x):\n",
        "    return 5.*x + 10\n",
        "\n",
        "x = np.linspace(-10, 10, num=1001)\n",
        "y = f(x) + 5.*np.random.randn()\n",
        "```\n",
        "\n",
        "で与えられるデータセット$(x, y)$について最小二乗誤差（F.mean_squared_error）を損失関数として使ってSGDで\n",
        "\n",
        "```math\n",
        "y = ax + b\n",
        "```\n",
        "\n",
        "の$a, b$（この場合$a=5.0, b=10$に近い値，但し乱数による誤差で必ずしも5.0, 10.0とはならない)を推定するプログラムを書け。"
      ]
    },
    {
      "metadata": {
        "id": "N_4L9yfBey1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f67e4139-1b06-48c7-b6c3-ecb2ff74cbbe"
      },
      "cell_type": "code",
      "source": [
        "from chainer import Chain\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import optimizers\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Linear(Chain):\n",
        "    def __init__(self):\n",
        "        super(Linear, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(1, 1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.l1(x)\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    return 5. * x + 10\n",
        "\n",
        "\n",
        "x = np.linspace(-10, 10, num=1001)\n",
        "y = f(x) + 5. * np.random.randn(1001)\n",
        "\n",
        "model = Linear()\n",
        "\n",
        "opt = optimizers.SGD()\n",
        "opt.setup(model)\n",
        "for epoch in range(100):\n",
        "    perm = np.random.permutation(len(x))\n",
        "    for i in range(len(x)):\n",
        "        x_i = Variable(np.array([[x[perm[i]]]], 'f'))\n",
        "        y_i = Variable(np.array([[y[perm[i]]]], 'f'))\n",
        "        # Write Here"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LybTGoYKeqeY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Dataset\n",
        "\n",
        "Chainerでは処理対象となるデータセットを抽象化したDatasetとよばれるオブジェクトを扱います。\n",
        "\n",
        "Datasetはデータセットに対するイテレーター，つまりデータ上を走査する機能，およびデータに対する前処理を備えます。\n",
        "\n",
        "Datasetは添字によるアクセス，および範囲指定を備えており配列と同じように扱うことができます。\n",
        "\n",
        "Iteratorは次の属性を持ちます\n",
        "\n",
        "* batchsize\n",
        "* epoch　データ上何回目の走査か\n",
        "* epoch_detail データ上何回目の走査かの浮動小数点表現，例えば，2週目とデータ半分をみている場合，2.5となる\n",
        "* is_new_epoch 直前のupdateの終了時にepochが1つ上がったかどうか\n",
        "\n",
        "## TupleDataset\n",
        "\n",
        "複数のデータセットを一つのデータセットにまとめ，各サンプルはi番目の要素がi番目のデータセットに対応するようなTupleで表現される。\n",
        "\n",
        "例えば，入力からなるデータセット$X$と，出力からなるデータセット$Y$を組み合わせて作られた$TupleDataset([X, Y])$は，$i$番目の要素は$(X[i], Y[i])$を返す。教師あり学習の場合によく使われる。\n",
        "\n",
        "## DictDataset\n",
        "\n",
        "複数のデータセットを名前付きで一つのデータセットにまとめ，各サンプルは名前付きの辞書で表現される。"
      ]
    },
    {
      "metadata": {
        "id": "LzZb7pUYe6D9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：Serializer\n",
        "\n",
        "最後に紹介する機能がシリアライザです．\n",
        "Link, Optimizer, Trainerがシリアライザをサポートしています。\n",
        "\n",
        "```\n",
        "serializers.save_npz('my.model', model)\n",
        "```\n",
        "\n",
        "これは，modelを'my.model'にNPZ（numpy + zip）形式で保存します。\n",
        "\n",
        "保存されたモデルはload_npzで読み込むことができます。\n",
        "\n",
        "```\n",
        "serializers.load_npz('my.model', model)\n",
        "```\n",
        "\n",
        "同様に，HDF5フォーマットで保存するためのsave_hdf5, load_hdf5が存在します。\n",
        "\n",
        "なお，シリアライズされるのは，parametersとpersistent valuesのみでそれ以外の属性値はシリアライズされないことに注意してください。シリアライズの対象にするには，add_persistent()を利用してください。"
      ]
    },
    {
      "metadata": {
        "id": "ukXf3tdje-3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：MNIST 例\n",
        "\n",
        "次にネットワークアーキテクチャを定義します。\n",
        "\n",
        "次の例は，3層からなるニューラルネットワークであり，中間層のユニット数がn_unitsであり，出力層のunit数からなります。\n",
        "\n",
        "この際，Linearの入力サイズがNoneとなっていますがこれは最初の実行時に入力から推論されます。\n",
        "\n",
        "```\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "```\n",
        "\n",
        "次に，損失関数を定義するClassifierを定義します。\n",
        "Classiferは精度を計算した上で損失をsoftmax_cross_entropyを使って定義します。\n",
        "\n",
        "```\n",
        "class Classifier(Chain):\n",
        "    def __init__(self, predictor):\n",
        "        super(Classifier, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.predictor = predictor\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "        y = self.predictor(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        accuracy = F.accuracy(y, t)\n",
        "        report({'loss': loss, 'accuracy': accuracy}, self)\n",
        "        return loss\n",
        "```\n",
        "\n",
        "これとほぼ同じ機能が既にchainer.links.Classifierで実装されています。\n",
        "\n",
        "```\n",
        "model = L.Classifier(MLP(784, 100, 10))\n",
        "opt = optimizers.Adam()\n",
        "opt.setup(model)\n",
        "```\n",
        "\n",
        "なお，L.Classiferは初期化時に次の三つの関数を受け取れるようになっています\n",
        "\n",
        "* predictor\n",
        "　学習対象であるLink\n",
        "* lossfun\n",
        "　誤差関数に使う関数。上記例の場合はF.softmax_cross_entropy\n",
        "* accfun\n",
        "　精度評価につかう関数。上記の場合はF.accuracy\n",
        "\n",
        "## 課題\n",
        "\n",
        "右のコードで各ミニバッチに対し，MLPで予測した結果のaccuracyを表示しなさい．"
      ]
    },
    {
      "metadata": {
        "id": "dG4cxqa5fFwT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "69854ea4-a316-4600-8cbf-f4c9b4713c65"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import Variable\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "batchsize = 100\n",
        "train, test = chainer.datasets.get_mnist()\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(\n",
        "    test, batchsize, repeat=False, shuffle=False)\n",
        "\n",
        "model = MLP(784, 10)\n",
        "opt = chainer.optimizers.Adam()\n",
        "opt.setup(model)\n",
        "\n",
        "train_num = len(train)\n",
        "for i in range(0, train_num, batchsize):\n",
        "    batch = train_iter.next()\n",
        "    x = Variable(np.asarray([s[0] for s in batch]))\n",
        "    t = Variable(np.asarray([s[1] for s in batch]))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2EKmcED2e-1K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：MNIST 例\n",
        "\n",
        "ようやく学習ができるようになりました。\n",
        "\n",
        "modelによる入力xに対する予測結果をyとします。\n",
        "そして，yとtとの間で測ったクロスエントロピー損失をlossとします。\n",
        "そして，optを更新します。\n",
        "\n",
        "この学習自体は何回も学習データを回す必要があるのでepoch_num回ループするようにします。\n",
        "```\n",
        "epoch_num = 5\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss_sum = 0\n",
        "    train_accuracy_sum = 0\n",
        "    for i in range(0, train_num, batchsize):\n",
        "        batch = train_iter.next()\n",
        "        x = np.asarray([s[0] for s in batch])\n",
        "        t = np.asarray([s[1] for s in batch])\n",
        "        y = model(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "        opt.update()\n",
        "        train_loss_sum += loss.data\n",
        "        train_accuracy_sum += F.accuracy(y, t).data\n",
        "```\n",
        "\n",
        "次に評価用データセットで性能を評価するコードです。\n",
        "この部分は学習と殆ど同じで唯一の違いはbackwardを呼ばず，modelの更新をしない部分です。\n",
        "```\n",
        "...\n",
        "test_loss_sum = 0\n",
        "test_accuracy_sum = 0\n",
        "for i in range(0, test_num, args.batchsize):\n",
        "    batch = train_iter.next()\n",
        "    x = xp.asarray([s[0] for s in batch])\n",
        "    t = xp.asarray([s[1] for s in batch])\n",
        "    y = model(x)\n",
        "    loss = F.softmax_cross_entropy(y, t)\n",
        "    test_loss_sum += loss.data.get()\n",
        "    test_accuracy_sum += F.accuracy(y, t).data.get()\n",
        "...\n",
        "```\n",
        "\n",
        "## 課題\n",
        "\n",
        "実際に実行し，精度がどのように代わっていくのかを調べよ。"
      ]
    },
    {
      "metadata": {
        "id": "V2v3Ze7VfVnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0d692d36-d5b0-4049-eb88-351d29f6a5e7"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "batchsize = 100\n",
        "train, test = chainer.datasets.get_mnist()\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(\n",
        "    test, batchsize, repeat=True, shuffle=False)\n",
        "\n",
        "model = MLP(784, 10)\n",
        "opt = chainer.optimizers.Adam()\n",
        "opt.setup(model)\n",
        "\n",
        "train_num = len(train)\n",
        "test_num = len(test)\n",
        "\n",
        "epoch_num = 10\n",
        "for epoch in range(epoch_num):\n",
        "    train_loss_sum = 0\n",
        "    train_accuracy_sum = 0\n",
        "    for i in range(0, train_num, batchsize):\n",
        "        batch = train_iter.next()\n",
        "        x = np.asarray([s[0] for s in batch])\n",
        "        t = np.asarray([s[1] for s in batch])\n",
        "        y = model(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "        opt.update()\n",
        "        train_loss_sum += loss.data\n",
        "        train_accuracy_sum += F.accuracy(y, t).data\n",
        "    print(train_loss_sum, train_accuracy_sum)\n",
        "\n",
        "    test_loss_sum = 0\n",
        "    test_accuracy_sum = 0\n",
        "    for i in range(0, test_num, batchsize):\n",
        "        batch = test_iter.next()\n",
        "        x = np.asarray([s[0] for s in batch])\n",
        "        t = np.asarray([s[1] for s in batch])\n",
        "        y = model(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        test_loss_sum += loss.data\n",
        "        test_accuracy_sum += F.accuracy(y, t).data\n",
        "\n",
        "    print(\"%5d %.5f %.5f %.5f %.5f\" %\n",
        "          (epoch,\n",
        "           train_loss_sum / train_num,\n",
        "           train_accuracy_sum / train_num * 100,\n",
        "           test_loss_sum / test_num,\n",
        "           test_accuracy_sum / test_num * 100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118.48294753395021 564.1600015684962\n",
            "    0 0.00197 0.94027 0.00096 0.97130\n",
            "45.635082385502756 585.7700055837631\n",
            "    1 0.00076 0.97628 0.00077 0.97670\n",
            "29.011558642145246 590.830006301403\n",
            "    2 0.00048 0.98472 0.00082 0.97610\n",
            "21.089173171902075 593.2300056219101\n",
            "    3 0.00035 0.98872 0.00065 0.98100\n",
            "17.14438199502183 594.2400047779083\n",
            "    4 0.00029 0.99040 0.00077 0.98040\n",
            "14.255777348545962 595.2500039339066\n",
            "    5 0.00024 0.99208 0.00067 0.98210\n",
            "13.013227524599642 595.9900035858154\n",
            "    6 0.00022 0.99332 0.00073 0.98040\n",
            "10.054194167620153 596.5800032019615\n",
            "    7 0.00017 0.99430 0.00090 0.98050\n",
            "11.207220488930034 596.7100029587746\n",
            "    8 0.00019 0.99452 0.00083 0.98060\n",
            "7.395797035227588 597.5300023555756\n",
            "    9 0.00012 0.99588 0.00086 0.98240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nTzCzYcGe-xw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chainerの基本：MNIST 例\n",
        "\n",
        "実はこれまで紹介してきた多くの処理は実はUpdater, Trainerとよばれる仕組みを使えば\n",
        "ユーザーが書く必要はありません。\n",
        "\n",
        "これまでは，内部でどのような処理をしているのかを知ってもらうためにあえて説明をしました。\n",
        "\n",
        "それではUpdater, Trainerの機能を使って学習部分を書き直していきましょう。\n",
        "\n",
        "まず，updateを担当するUpdaterを用意します\n",
        "\n",
        "```\n",
        "updater = training.StandardUpdater(train_iter, optimizer)\n",
        "```\n",
        "\n",
        "updaterには既に作成したiteratorとoptimizerを渡します。\n",
        "次に訓練を実行するTrainerをupdaterを渡して作ります。\n",
        "\n",
        "```\n",
        "trainer = training.Trainer(updater, (20, 'epoch'), out='result')\n",
        "```\n",
        "\n",
        "二つ目の引数は訓練回数を示す引数であり，単位として'epoch'か'iteration'を受けとります。\n",
        "\n",
        "例えば，(20, 'epoch')はデータ全体を20回走査するという意味ですし，('1000', 'iteration'）はミニバッチを1000回動かすという意味です。\n",
        "\n",
        "準備ができました。後はrun()をよびだし実行するだけです。\n",
        "\n",
        "```\n",
        "trainer.run()\n",
        "```\n",
        "\n",
        "先程はepochの回数だけループを回し，その中でbackwardを呼び出したりといろいろな手間がかかっていましたが\n",
        "今回はUpdater, Trainerを用紙するだけで実現できたことに注意してください。\n",
        "\n",
        "さらに，学習の際に必要な機能の多くはextentionsとよばれる拡張機能により実現することができます。\n",
        "\n",
        "例えば，次のような拡張機能がよく使われます。\n",
        "\n",
        "* Evaluatorは学習が終わった後に，test_iterで定義されるテストデータセットで評価をしてくれます\n",
        "\n",
        "```\n",
        "trainer.extend(extensions.Evaluator(test_iter, model))\n",
        "```\n",
        "\n",
        "* LogReportは報告された値をlog fileに格納してくれます。\n",
        "```\n",
        "trainer.extend(extensions.LogReport())\n",
        "```\n",
        "\n",
        "* PrintReportは指定したカラムを表示してくれます。\n",
        "```\n",
        "trainer.extend(extensions.PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy']))\n",
        "```\n",
        "\n",
        "* ProgressBarは進捗度合いをプログレスバーで表示してくれます。\n",
        "```\n",
        "trainer.extend(extensions.ProgressBar())\n",
        "```\n",
        "\n",
        "* Snapshotは定期的にモデルのスナップショットを記録し，出力ディレクトリに格納します。\n",
        "```\n",
        "trainer.extend(extensions.Snapshot((10, 'epoch')))\n",
        "```\n",
        "\n",
        "## 課題\n",
        "\n",
        "trainerを実際に動かし学習できることを確かめてください。\n",
        "その上で例えばユニット数を変えたり，収束回数を変えたり，Optimizerを変えたりして精度が変わることを\n",
        "確認してください。"
      ]
    },
    {
      "metadata": {
        "id": "NoCcOOBRdisH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d8726007-3f24-4409-a18e-78412aa33d3d"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import datasets\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "batchsize = 10\n",
        "train, test = datasets.get_mnist()\n",
        "train = datasets.SubDataset(train, 0, 100)\n",
        "test = datasets.SubDataset(test, 0, 100)\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
        "                                             repeat=False, shuffle=False)\n",
        "\n",
        "model = L.Classifier(MLP(784, 10))\n",
        "opt = chainer.optimizers.Adam()\n",
        "opt.setup(model)\n",
        "\n",
        "epoch = 2\n",
        "\n",
        "# Set up a trainer\n",
        "updater = training.StandardUpdater(train_iter, opt, device=-1)\n",
        "trainer = training.Trainer(updater, (epoch, 'epoch'), out='/tmp/result')\n",
        "\n",
        "# Evaluate the model with the test dataset for each epoch\n",
        "trainer.extend(extensions.Evaluator(test_iter, model, device=-1))\n",
        "\n",
        "# Dump a computational graph from 'loss' variable at the first iteration\n",
        "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
        "# trainer.extend(extensions.dump_graph('main/loss'))\n",
        "\n",
        "# Take a snapshot at each epoch\n",
        "# trainer.extend(extensions.snapshot(), trigger=(epoch, 'epoch'))\n",
        "\n",
        "# Write a log of evaluation statistics for each epoch\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "# Print selected entries of the log to stdout\n",
        "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
        "# \"validation\" refers to the default name of the Evaluator extension.\n",
        "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
        "# either the updater or the evaluator.\n",
        "trainer.extend(extensions.PrintReport(\n",
        "    ['epoch', 'main/loss', 'validation/main/loss',\n",
        "     'main/accuracy', 'validation/main/accuracy']))\n",
        "\n",
        "# Print a progress bar to stdout\n",
        "# trainer.extend(extensions.ProgressBar())\n",
        "\n",
        "resume = False\n",
        "if resume:\n",
        "    # Resume from a snapshot\n",
        "    chainer.serializers.load_npz(resume, trainer)\n",
        "\n",
        "# Run the training\n",
        "trainer.run()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n",
            "\u001b[J1           1.92843     1.43623               0.43           0.62                      \n",
            "\u001b[J2           0.644438    0.907035              0.91           0.71                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P01bywm2flUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 畳み込みニューラルネットワークを使った画像認識\n",
        "\n",
        "次に畳み込みニューラルネットワークと，それを使った画像認識について説明します。\n",
        "\n",
        "この章では与えられた画像に何がうつっているのかを認識し，\n",
        "その認識結果を出力するタスクを考えます。\n",
        "\n",
        "\n",
        "各画像は32*32のカラー画像で構成され，各画像には\"bear\"や，\"telephone\"など100種類の物体がうつっています。\n",
        "\n",
        "前の章のMNISTは簡単な手書き文字認識でしたが，この章では画像はMNIST同様に小さいもの背景と前景が重なっており，同じクラスでもバリエーションが非常に大きい画像の認識を扱います。\n",
        "\n",
        "\n",
        "MNISTと同様に，CIFAR-100も読み込むことができます。\n",
        "\n",
        "```\n",
        "from chainer import datasets\n",
        "train, test = datasets.get_cifar100()\n",
        "```\n",
        "\n",
        "実際のサイズを確認してみましょう\n",
        "\n",
        "```\n",
        "x, y = train[0]\n",
        "print x.shape\n",
        "# (3, 32, 32)\n",
        "```\n",
        "\n",
        "このように画像データは，(チャンネル，縦，横）\n",
        "の三次元のデータで表されます。\n",
        "\n",
        "さらにミニバッチ学習では複数のデータをまとめて学習するため，\n",
        "（データ、チャンネル，縦，横）\n",
        "の４次元の配列として扱われます。\n",
        "\n",
        "MNISTまでは，各データは1次元データ，バッチデータは次のような二次元でした。\n",
        "（データ，特徴）\n",
        "\n",
        "畳み込みニューラルネットワークではこの構造の情報を活かしてデータを変換していきます。"
      ]
    },
    {
      "metadata": {
        "id": "GEv4P0ECftb_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c63703b5-9cf6-4be5-b6e7-debe5f1eecab"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from chainer import datasets\n",
        "\n",
        "train, test = datasets.get_cifar100()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RHgTR1_WflYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CIFAR-100の学習：総結合層を使った学習\n",
        "\n",
        "はじめに，前回MNISTで使った学習器をそのまま使って学習してみましょう。\n",
        "\n",
        "この場合，画像の構造情報は捨ててしまっています。\n",
        "変更点は，MLPの入力次元数が3*32*32=3072である点のみです。\n",
        "\n",
        "その他はそのままで実行できます。\n",
        "\n",
        "実際実行してみると以下のような学習結果が得られます。"
      ]
    },
    {
      "metadata": {
        "id": "GkHyTYruf0xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3662
        },
        "outputId": "e8e5ee3f-a292-4ee7-dc77-48ef5135674a"
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer import functions as F\n",
        "from chainer import links as L\n",
        "from chainer import training\n",
        "from chainer.training import extensions\n",
        "\n",
        "\n",
        "train, test = chainer.datasets.get_cifar10()\n",
        "\n",
        "\n",
        "class MLP(chainer.Chain):\n",
        "\n",
        "    def __init__(self, n_units, n_out):\n",
        "        super(MLP, self).__init__()\n",
        "        with self.init_scope():\n",
        "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
        "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
        "            self.l3 = L.Linear(None, n_out)    # n_units -> n_out\n",
        "\n",
        "    def __call__(self, x):\n",
        "        h1 = F.relu(self.l1(x))\n",
        "        h2 = F.relu(self.l2(h1))\n",
        "        return self.l3(h2)\n",
        "\n",
        "\n",
        "batchsize = 100\n",
        "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
        "test_iter = chainer.iterators.SerialIterator(test, batchsize,\n",
        "                                             repeat=False, shuffle=False)\n",
        "\n",
        "model = L.Classifier(MLP(784, 10))\n",
        "\n",
        "gpu = 0\n",
        "if gpu >= 0:\n",
        "    chainer.cuda.get_device(gpu).use()  # Make a specified GPU current\n",
        "    model.to_gpu()  # Copy the model to the GPU\n",
        "\n",
        "opt = chainer.optimizers.Adam()\n",
        "opt.setup(model)\n",
        "\n",
        "epoch = 10\n",
        "\n",
        "# Set up a trainer\n",
        "updater = training.StandardUpdater(train_iter, opt, device=gpu)\n",
        "trainer = training.Trainer(updater, (epoch, 'epoch'), out='/tmp/result')\n",
        "\n",
        "# Evaluate the model with the test dataset for each epoch\n",
        "trainer.extend(extensions.Evaluator(test_iter, model, device=gpu))\n",
        "\n",
        "# Dump a computational graph from 'loss' variable at the first iteration\n",
        "# The \"main\" refers to the target link of the \"main\" optimizer.\n",
        "trainer.extend(extensions.dump_graph('main/loss'))\n",
        "\n",
        "# Take a snapshot at each epoch\n",
        "trainer.extend(extensions.snapshot(), trigger=(epoch, 'epoch'))\n",
        "\n",
        "# Write a log of evaluation statistics for each epoch\n",
        "trainer.extend(extensions.LogReport())\n",
        "\n",
        "# Print selected entries of the log to stdout\n",
        "# Here \"main\" refers to the target link of the \"main\" optimizer again, and\n",
        "# \"validation\" refers to the default name of the Evaluator extension.\n",
        "# Entries other than 'epoch' are reported by the Classifier link, called by\n",
        "# either the updater or the evaluator.\n",
        "trainer.extend(extensions.PrintReport(\n",
        "    ['epoch', 'main/loss', 'validation/main/loss',\n",
        "     'main/accuracy', 'validation/main/accuracy']))\n",
        "\n",
        "# Print a progress bar to stdout\n",
        "trainer.extend(extensions.ProgressBar())\n",
        "\n",
        "resume = False\n",
        "if resume:\n",
        "    # Resume from a snapshot\n",
        "    chainer.serializers.load_npz(resume, trainer)\n",
        "\n",
        "# Run the training\n",
        "trainer.run()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch       main/loss   validation/main/loss  main/accuracy  validation/main/accuracy\n",
            "\u001b[J     total [#.................................................]  2.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "       100 iter, 0 epoch / 10 epochs\n",
            "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
            "\u001b[4A\u001b[J     total [##................................................]  4.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "       200 iter, 0 epoch / 10 epochs\n",
            "    133.96 iters/sec. Estimated time to finish: 0:00:35.832355.\n",
            "\u001b[4A\u001b[J     total [###...............................................]  6.00%\n",
            "this epoch [##############################....................] 60.00%\n",
            "       300 iter, 0 epoch / 10 epochs\n",
            "    133.57 iters/sec. Estimated time to finish: 0:00:35.187506.\n",
            "\u001b[4A\u001b[J     total [####..............................................]  8.00%\n",
            "this epoch [########################################..........] 80.00%\n",
            "       400 iter, 0 epoch / 10 epochs\n",
            "    135.04 iters/sec. Estimated time to finish: 0:00:34.065181.\n",
            "\u001b[4A\u001b[J1           1.86751     1.72331               0.32702        0.3899                    \n",
            "\u001b[J     total [#####.............................................] 10.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "       500 iter, 1 epoch / 10 epochs\n",
            "    109.51 iters/sec. Estimated time to finish: 0:00:41.093250.\n",
            "\u001b[4A\u001b[J     total [######............................................] 12.00%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "       600 iter, 1 epoch / 10 epochs\n",
            "    113.81 iters/sec. Estimated time to finish: 0:00:38.660671.\n",
            "\u001b[4A\u001b[J     total [######............................................] 14.00%\n",
            "this epoch [###################...............................] 40.00%\n",
            "       700 iter, 1 epoch / 10 epochs\n",
            "    117.06 iters/sec. Estimated time to finish: 0:00:36.733751.\n",
            "\u001b[4A\u001b[J     total [########..........................................] 16.00%\n",
            "this epoch [##############################....................] 60.00%\n",
            "       800 iter, 1 epoch / 10 epochs\n",
            "    119.48 iters/sec. Estimated time to finish: 0:00:35.151070.\n",
            "\u001b[4A\u001b[J     total [#########.........................................] 18.00%\n",
            "this epoch [########################################..........] 80.00%\n",
            "       900 iter, 1 epoch / 10 epochs\n",
            "    121.37 iters/sec. Estimated time to finish: 0:00:33.779836.\n",
            "\u001b[4A\u001b[J2           1.6625      1.6003                0.40814        0.4284                    \n",
            "\u001b[J     total [##########........................................] 20.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      1000 iter, 2 epoch / 10 epochs\n",
            "    117.16 iters/sec. Estimated time to finish: 0:00:34.140057.\n",
            "\u001b[4A\u001b[J     total [###########.......................................] 22.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      1100 iter, 2 epoch / 10 epochs\n",
            "    118.76 iters/sec. Estimated time to finish: 0:00:32.838396.\n",
            "\u001b[4A\u001b[J     total [############......................................] 24.00%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      1200 iter, 2 epoch / 10 epochs\n",
            "    120.17 iters/sec. Estimated time to finish: 0:00:31.622608.\n",
            "\u001b[4A\u001b[J     total [#############.....................................] 26.00%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      1300 iter, 2 epoch / 10 epochs\n",
            "    121.36 iters/sec. Estimated time to finish: 0:00:30.487019.\n",
            "\u001b[4A\u001b[J     total [#############.....................................] 28.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      1400 iter, 2 epoch / 10 epochs\n",
            "    122.36 iters/sec. Estimated time to finish: 0:00:29.422548.\n",
            "\u001b[4A\u001b[J3           1.56947     1.52632               0.4349         0.4599                    \n",
            "\u001b[J     total [###############...................................] 30.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      1500 iter, 3 epoch / 10 epochs\n",
            "    119.39 iters/sec. Estimated time to finish: 0:00:29.314550.\n",
            "\u001b[4A\u001b[J     total [################..................................] 32.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      1600 iter, 3 epoch / 10 epochs\n",
            "    120.28 iters/sec. Estimated time to finish: 0:00:28.266612.\n",
            "\u001b[4A\u001b[J     total [#################.................................] 34.00%\n",
            "this epoch [###################...............................] 40.00%\n",
            "      1700 iter, 3 epoch / 10 epochs\n",
            "    121.14 iters/sec. Estimated time to finish: 0:00:27.240201.\n",
            "\u001b[4A\u001b[J     total [##################................................] 36.00%\n",
            "this epoch [##############################....................] 60.00%\n",
            "      1800 iter, 3 epoch / 10 epochs\n",
            "    121.49 iters/sec. Estimated time to finish: 0:00:26.338972.\n",
            "\u001b[4A\u001b[J     total [###################...............................] 38.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      1900 iter, 3 epoch / 10 epochs\n",
            "    121.78 iters/sec. Estimated time to finish: 0:00:25.455997.\n",
            "\u001b[4A\u001b[J4           1.50219     1.52119               0.46292        0.4575                    \n",
            "\u001b[J     total [####################..............................] 40.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      2000 iter, 4 epoch / 10 epochs\n",
            "    118.94 iters/sec. Estimated time to finish: 0:00:25.223759.\n",
            "\u001b[4A\u001b[J     total [#####################.............................] 42.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      2100 iter, 4 epoch / 10 epochs\n",
            "    118.77 iters/sec. Estimated time to finish: 0:00:24.416543.\n",
            "\u001b[4A\u001b[J     total [######################............................] 44.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      2200 iter, 4 epoch / 10 epochs\n",
            "    118.64 iters/sec. Estimated time to finish: 0:00:23.600429.\n",
            "\u001b[4A\u001b[J     total [#######################...........................] 46.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      2300 iter, 4 epoch / 10 epochs\n",
            "    118.38 iters/sec. Estimated time to finish: 0:00:22.807444.\n",
            "\u001b[4A\u001b[J     total [########################..........................] 48.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      2400 iter, 4 epoch / 10 epochs\n",
            "    118.13 iters/sec. Estimated time to finish: 0:00:22.009411.\n",
            "\u001b[4A\u001b[J5           1.46151     1.49907               0.47792        0.4684                    \n",
            "\u001b[J     total [#########################.........................] 50.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      2500 iter, 5 epoch / 10 epochs\n",
            "     115.9 iters/sec. Estimated time to finish: 0:00:21.570576.\n",
            "\u001b[4A\u001b[J     total [##########################........................] 52.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      2600 iter, 5 epoch / 10 epochs\n",
            "    116.56 iters/sec. Estimated time to finish: 0:00:20.589870.\n",
            "\u001b[4A"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[J     total [###########################.......................] 54.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      2700 iter, 5 epoch / 10 epochs\n",
            "    117.06 iters/sec. Estimated time to finish: 0:00:19.647299.\n",
            "\u001b[4A\u001b[J     total [###########################.......................] 56.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      2800 iter, 5 epoch / 10 epochs\n",
            "    117.23 iters/sec. Estimated time to finish: 0:00:18.765953.\n",
            "\u001b[4A\u001b[J     total [############################......................] 58.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      2900 iter, 5 epoch / 10 epochs\n",
            "    117.38 iters/sec. Estimated time to finish: 0:00:17.890416.\n",
            "\u001b[4A\u001b[J6           1.4298      1.4593                0.49066        0.4812                    \n",
            "\u001b[J     total [##############################....................] 60.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      3000 iter, 6 epoch / 10 epochs\n",
            "    115.55 iters/sec. Estimated time to finish: 0:00:17.308196.\n",
            "\u001b[4A\u001b[J     total [###############################...................] 62.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      3100 iter, 6 epoch / 10 epochs\n",
            "    116.11 iters/sec. Estimated time to finish: 0:00:16.363256.\n",
            "\u001b[4A\u001b[J     total [################################..................] 64.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      3200 iter, 6 epoch / 10 epochs\n",
            "    116.61 iters/sec. Estimated time to finish: 0:00:15.436320.\n",
            "\u001b[4A\u001b[J     total [################################..................] 66.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      3300 iter, 6 epoch / 10 epochs\n",
            "    117.08 iters/sec. Estimated time to finish: 0:00:14.519646.\n",
            "\u001b[4A\u001b[J     total [##################################................] 68.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      3400 iter, 6 epoch / 10 epochs\n",
            "    117.54 iters/sec. Estimated time to finish: 0:00:13.612463.\n",
            "\u001b[4A\u001b[J7           1.39452     1.43076               0.5047         0.4846                    \n",
            "\u001b[J     total [###################################...............] 70.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      3500 iter, 7 epoch / 10 epochs\n",
            "     116.5 iters/sec. Estimated time to finish: 0:00:12.875773.\n",
            "\u001b[4A\u001b[J     total [####################################..............] 72.00%\n",
            "this epoch [##########........................................] 20.00%\n",
            "      3600 iter, 7 epoch / 10 epochs\n",
            "    116.94 iters/sec. Estimated time to finish: 0:00:11.971571.\n",
            "\u001b[4A\u001b[J     total [#####################################.............] 74.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      3700 iter, 7 epoch / 10 epochs\n",
            "     117.4 iters/sec. Estimated time to finish: 0:00:11.072964.\n",
            "\u001b[4A\u001b[J     total [######################################............] 76.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      3800 iter, 7 epoch / 10 epochs\n",
            "    117.83 iters/sec. Estimated time to finish: 0:00:10.183806.\n",
            "\u001b[4A\u001b[J     total [#######################################...........] 78.00%\n",
            "this epoch [#######################################...........] 80.00%\n",
            "      3900 iter, 7 epoch / 10 epochs\n",
            "    118.25 iters/sec. Estimated time to finish: 0:00:09.302458.\n",
            "\u001b[4A\u001b[J8           1.36568     1.41404               0.5145         0.4976                    \n",
            "\u001b[J     total [########################################..........] 80.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      4000 iter, 8 epoch / 10 epochs\n",
            "    117.38 iters/sec. Estimated time to finish: 0:00:08.519689.\n",
            "\u001b[4A\u001b[J     total [#########################################.........] 82.00%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      4100 iter, 8 epoch / 10 epochs\n",
            "    117.77 iters/sec. Estimated time to finish: 0:00:07.641709.\n",
            "\u001b[4A\u001b[J     total [##########################################........] 84.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      4200 iter, 8 epoch / 10 epochs\n",
            "    118.16 iters/sec. Estimated time to finish: 0:00:06.770335.\n",
            "\u001b[4A\u001b[J     total [###########################################.......] 86.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      4300 iter, 8 epoch / 10 epochs\n",
            "    118.53 iters/sec. Estimated time to finish: 0:00:05.905531.\n",
            "\u001b[4A\u001b[J     total [############################################......] 88.00%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      4400 iter, 8 epoch / 10 epochs\n",
            "    118.89 iters/sec. Estimated time to finish: 0:00:05.046716.\n",
            "\u001b[4A\u001b[J9           1.33369     1.40198               0.5229         0.502                     \n",
            "\u001b[J     total [#############################################.....] 90.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      4500 iter, 9 epoch / 10 epochs\n",
            "     118.1 iters/sec. Estimated time to finish: 0:00:04.233600.\n",
            "\u001b[4A\u001b[J     total [##############################################....] 92.00%\n",
            "this epoch [#########.........................................] 20.00%\n",
            "      4600 iter, 9 epoch / 10 epochs\n",
            "    118.44 iters/sec. Estimated time to finish: 0:00:03.377266.\n",
            "\u001b[4A\u001b[J     total [###############################################...] 94.00%\n",
            "this epoch [####################..............................] 40.00%\n",
            "      4700 iter, 9 epoch / 10 epochs\n",
            "    118.77 iters/sec. Estimated time to finish: 0:00:02.525950.\n",
            "\u001b[4A\u001b[J     total [################################################..] 96.00%\n",
            "this epoch [#############################.....................] 60.00%\n",
            "      4800 iter, 9 epoch / 10 epochs\n",
            "    119.08 iters/sec. Estimated time to finish: 0:00:01.679523.\n",
            "\u001b[4A\u001b[J     total [#################################################.] 98.00%\n",
            "this epoch [########################################..........] 80.00%\n",
            "      4900 iter, 9 epoch / 10 epochs\n",
            "    119.38 iters/sec. Estimated time to finish: 0:00:00.837667.\n",
            "\u001b[4A\u001b[J10          1.30951     1.4127                0.53274        0.5052                    \n",
            "\u001b[J     total [##################################################] 100.00%\n",
            "this epoch [..................................................]  0.00%\n",
            "      5000 iter, 10 epoch / 10 epochs\n",
            "    118.66 iters/sec. Estimated time to finish: 0:00:00.\n",
            "\u001b[4A\u001b[J"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VNgKTJrXZ7a6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}